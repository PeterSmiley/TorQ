{
    "docs": [
        {
            "location": "/", 
            "text": "The TorQ framework created by \nAquaQ Analytics\n forms the basis of a production kdb+ system by implementing some core functionality and utilities on top of kdb+, allowing developers to concentrate on the application business logic. It incorporates as many best practices as possible, with particular focus on performance, process management, diagnostic information, maintainability and extensibility. Wherever possible, we have tried to avoid re-inventing the wheel and instead have used contributed code from \ncode.kx.com\n (either directly or modified). This framework will be suitable for those looking to create a new kdb+ system from scratch or those looking to add additional functionality to their existing kdb+ systems.\n\n\nThe easiest way to get a production capture started is to download and install one of the \nStarter Packs\n. We also have a \nGoogle Group for questions/discussions\n.\n\n\nFor recent updates to TorQ please check out our \nblog\n.\n\n\nWe've highlighted some key TorQ features on our blog posts page.\n\n\nFor email support contact", 
            "title": "Home"
        }, 
        {
            "location": "/Overview/", 
            "text": "Overview\n\n\n\n\nWhat is kdb+?\n\n\nkdb+ is the market leading timeseries database from Kx Systems. kdb+\nis used predominently in the Financial Services sector to capture,\nprocess and analyse billions of records on a daily basis, with Kx\ncounting almost all of the top tier investment banks as customers. kdb+\nincorporates a programming language, q, which is known for its\nperformance and expressive power. Given the unsurpassed data management\nand analytical capabilities of kdb+, the applicability of kdb+\ntechnology extends beyond the financial domain into any sector where\nrapid pre-built or adhoc analysis of large datasets is required. Other\nsectors which have good use cases for kdb+ include utilities,\npharmaceuticals, telecoms, manufacturing, retail and any sector\nutilising telemetry or sensor data.\n\n\n\n\nWhat is AquaQ TorQ?\n\n\nAquaQ TorQ is a framework which forms the basis of a production kdb+\nsystem by implementing some core functionality and utilities on top of\nkdb+, allowing developers to concentrate on the application business\nlogic. We have incorporated as many best practices as possible, with\nparticular focus on performance, process management, diagnostic\ninformation, maintainability and extensibility. We have kept the code as\nreadable as possible using descriptive comments, error messages,\nfunction names and variable names. Wherever possible, we have tried to\navoid re-inventing the wheel and instead have used contributed code from\ncode.kx.com (either directly or modified). All code sections taken from\ncode.kx.com are referenced in this document.\n\n\nAquaQ TorQ can be extended or modified as required. We have chosen some\ndefault behaviour, but it can all be overridden. The features of AquaQ\nTorQ are:\n\n\n\n\n\n\nProcess Management: Each process is given a type and name. By\n    default these are used to determine the code base it loads, the\n    configuration loaded, log file naming and how it reports itself to\n    discovery services. Whenever possible we have tried to ensure that\n    all default behaviour can be overridden at the process type level,\n    and further at the process name level.\n\n\n\n\n\n\nCode Management: Processes can optionally load common or process\n    type/name specific code bases. All code loading is error trapped.\n\n\n\n\n\n\nConfiguration Management: Configuration scripts can be loaded as\n    standard and with specific process type/name configuration\n    overriding default values. Configuration scripts are loaded in a\n    specific order; default, then process type specific, then process\n    name specific. Values loaded last will override values loaded\n    previously.\n\n\n\n\n\n\nUsage Logging: All process usage is logged to a single text log file\n    and periodically rolled. Logging includes opening/closing of\n    connections, synchronous and asynchronous queries and functions\n    executed on the timer. Logged values include the request, the\n    details of where it came from, the time it was received, the time it\n    took, memory usage before and after, the size of the result set, the\n    status and any error message.\n\n\n\n\n\n\nIncoming and Outgoing Connection Management: Incoming (client) and\n    outgoing (server) connections are stored and their usage monitored\n    through query counts and total data size counts. Connections are\n    stored and retrieved and can be set to automatically be re-opened as\n    required. The password used for outgoing connections can be\n    overridden at default, process type and process name level.\n\n\n\n\n\n\nAccess Controls: Basic access controls are provided, and could be\n    extended. These apply restrictions on the IP addresses of remote\n    connections, the users who can access the process, and the functions\n    that each user can execute. A similar hierarchical approach is used\n    for access control management as for configuration management.\n\n\n\n\n\n\nTimer Extensions: Mechanism to allow multiple functions to be added\n    to the timer either on a repeating or one-off basis. Multiple\n    re-scheduling algorithms supplied for repeating timers.\n\n\n\n\n\n\nStandard Out/Error Logging: Functions to print formatted messages to\n    standard out and error. Hooks are provided to extend these as\n    required, e.g. publication to centralised logging database. Standard\n    out and error are redirected to appropriately named, timestamped and\n    aliased log files, which are periodically rolled.\n\n\n\n\n\n\nError Handling: Different failure options are supplied in case code\n    fails to load; either exit upon failure, stop at the point of\n    failure or trap and continue.\n\n\n\n\n\n\nVisualisation: Utilities to ease GUI development using websockets\n    and HTML5.\n\n\n\n\n\n\nDocumentation and Development Tools: Functionality to document the\n    system is built into AquaQ TorQ, and can be accessed directly from\n    every q session. Developers can extend the documentation as they add\n    new functions. Functionality for searching for functions and\n    variables by name and definition is provided, and for ordering\n    variables by memory usage. The standard help.q from code.kx is also\n    included.\n\n\n\n\n\n\nUtilities: We intend to build out and add utilities as we find them\n    to be suitably useful and generic. So far we have:\n\n\n\n\n\n\nCaching: allows a result set cache to be declared and result\n    sets to be stored and retrieved from the cache. Suitable for\n    functions which may be run multiple times with the same\n    parameters, with the underlying data not changing in a short\n    time frame;\n\n\n\n\n\n\nTimezone Handling: derived from code.kx, allows conversion\n    between timestamps in different timezones;\n\n\n\n\n\n\nEmail: an library to send emails;\n\n\n\n\n\n\nAsync Messaging: allows easy use of advanced async messaging\n    methods such as deferred synchronous communication and async\n    communication using postback functions;\n\n\n\n\n\n\nHeartbeating: each process can be set to publish heartbeats, and\n    subscribe to and manage heartbeats from other processes in the\n    environment;\n\n\n\n\n\n\nData Loading: utility wrapper around .Q.fsn to read a data file\n    from disk, manipulate it and write it out in chunks;\n\n\n\n\n\n\nSubscriptions: allow processes to dynamically detect and\n    subscribe to datasources;\n\n\n\n\n\n\nTickerplant Log File Recovery: recover as many messages as\n    possible from corrupt log files;\n\n\n\n\n\n\nDatabase Writing: utility functions for writing to, sorting and\n    parting on disk databases;\n\n\n\n\n\n\nCompression: allows compression of a database. This can be\n    performed using a set of parameters for the entire database, but\n    also gives the flexibilty of compressing user-specified tables\n    and/or columns of those tables with different parameters if\n    required, and also offers decompression.\n\n\n\n\n\n\n\n\n\n\nAquaQ TorQ will wrap easily around kdb+tick and therefore around any\ntickerplant, RDB, HDB or real time processing application. We currently\nhave several customised processes of our own:\n\n\n\n\n\n\nDiscovery Service: Every process has a type, name and set of\n    available attributes, which are used by other processes to connect\n    to it. The Discovery Service is a central point that can be used to\n    find other available processes. Client processes can subscribe to\n    updates from the discovery service as new processes become\n    available- the discovery service will notify its subscribers, which\n    can then use the supplied hook to implement required behavior e.g.\n    connect to the newly available process;\n\n\n\n\n\n\nGateway: A fully synchronous and asynchronous gateway is provided.\n\n\nThe gateway will connect to a defined list of process types (can be\nhomogenous or heterogeneous processes) and will route queries across\nthem according to the priority of received requests. The routing\nalgorithms can be easily modified e.g. give priority to user X, or\nonly route queries to processes which exist in the same data centre\nor geographical region to avoid the WAN (this would entail using the\nprocess attributes). The gateway can either return the result to the\nclient back down the same handle, or it can wrap it in a callback\nfunction to be invoked on the client;\n\n\n\n\n\n\nReal Time Database (RDB): A customized version of the kdb+tick RDB,\n    to allow dynamic tickerplant subscriptions, reloading of multiple\n    HDBs using authenticated connections, and customized end-of-day save\n    downs. The RDB, WDB and tickerplant log replay share a common code\n    base to ensure that a save-down modification to a table is applied\n    across each of these processes.\n\n\n\n\n\n\nWrite Database (WDB): The job of a WDB is to write data to disk\n    rather than to serve client queries. WDBs usually write data out\n    periodically throughout the day, and are useful when there is too\n    much data to fit into memory and/or the end of day save operation\n    needs to be speeded up. The concept is based on\n    \nw.q\n\n\n\n\n\n\nTickerplant Log Replay: A process for replaying tickerplant log\n    files to create on-disk data sets. Extended features are provided\n    for only replaying subsets of log files (by message number and/or\n    table name), replaying in chunks, invoking bespoke final behaviour\n    etc.;\n\n\n\n\n\n\nReporter: The Reporter Process runs defined reports (q queries or\n    parameterized functions) against specific database or gateways on a\n    schedule. The results are retrieved and processed. Processing can be\n    user defined, or can be a standard operation such as writing the\n    data to disk, or emailing the results to a list of recipients. This\n    can be useful for running system checks or generating management\n    reports.\n\n\n\n\n\n\nHousekeeping: A process to undertake housekeeping tasks\n    periodically, such as compressing and removing files that are no\n    longer used. Housekeeping looks up a file of instructions and\n    performs maintenance tasks on directories accordingly. Features\n    allow selective file deletion and zipping according to file age,\n    including a search string parameter and the ability to exclude items\n    from the search. The process can be scheduled, or run immediately\n    from the command line and can be extended as required to incorporate\n    more tasks.\n\n\n\n\n\n\nFile Alerter: A process to periodically scan a set of directories\n    and execute a function based on the availability of a file. This is\n    useful where files may arrive to the system during the day and must\n    be acted upon (e.g. files are uploaded to a shared directory by\n    users/clients). The functions to execute are defined by the user and\n    the whole process is driven by a csv file detailing the file to\n    search for, the function to execute and, optionally, a directory to\n    move the file to after it has been processed.\n\n\n\n\n\n\nMonitor: A basic monitoring process which uses the Discovery Service\n    to locate the other processes within the system, listens for\n    heartbeats, and subscribes for log messages. This should be extended\n    as required but provides a basic central point for system health\n    checks;\n\n\n\n\n\n\nKill: A process used to kill other processes, optionally using the\n    Discovery Service to locate them.\n\n\n\n\n\n\n\n\nA Large Scale Data Processing Platform\n\n\nOne of the key drivers behind TorQ development has been to ensure all\nthe tools necessary to build a large scale data processing platform are\navailable. \nkdb+tick\n\nprovides the basic building blocks, and a standard set-up usually looks\nsomething like this:\n\n\n\n\nHowever, in reality it is usually more complicated. A larger scale\narchitecture serving large numbers of client queries and receiving data\nfrom multiple sources may look like this:\n\n\n\n\nA common practice is to use a gateway (section gateway) to\nmanage client queries across back-end processes. The gateway can load\nbalance across processes and make failures transparent to the client. If\nthe clients access the gateway with asynchronous calls, then the gateway\ncan serve many requests at once and additionally implement client\nqueuing algorithms.\n\n\nOther common production features include:\n\n\n\n\n\n\nA modified version of the RDB (section sec:rdb) which does\n    different operations at end-of-day, reloads multiple HDB processes\n    etc.\n\n\n\n\n\n\nA Write Database (section [sec:wdb]) which receives data from the\n    tickerplant and periodically writes it to disk. WDBs are used when\n    there is too much data in a day to fit into memory and/or to speed\n    up the end-of-day rollover job\n\n\n\n\n\n\nProcesses that load data from other sources either into the HDB\n    directly or to the RDB potentially via the tickerplant (section\n    [sec:dataloader]). The data may be dropped in specific locations\n    which have to be monitored (section [sec:filealerter])\n\n\n\n\n\n\nA Reporting Engine (section [sec:reporter]) to run periodic\n    reports and do something with the result (e.g. generate an xls file\n    from the database and email it to senior management). Reporting\n    engines can also be used to run periodic checks of the system\n\n\n\n\n\n\nA Discovery Service (section [sec:discovery]) to allow processes\n    to locate each other, and to allow processes to dynamically register\n    availability and push notifications around the system.\n\n\n\n\n\n\nBasic Monitoring (section [sec:monitor]) of process availability\n\n\n\n\n\n\nHousekeeping (section [sec:housekeeping]) to ensure log files are\n    tidied up, tickerplant log files are compressed/moved in a timely\n    fashion etc.\n\n\n\n\n\n\n\n\nDo I Really Have to Read This Whole Document?\n\n\nHopefully not. The core of AquaQ TorQ is a script called torq.q and we\nhave tried to make it as descriptive as possible, so perhaps that will\nsuffice. The first place to look will be in the config files, the main\none being \\$KDBCONFIG/settings/default.q. This should contain a lot of\ninformation on what can be modified. In addition:\n\n\n\n\n\n\nWe have added a load of usage information:\n\n\naquaq$ q torq.q -usage\nKDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems\n\nGeneral:\n This script should form the basis of a production kdb+ environment.\n It can be sourced from other files if required, or used as a launch script before loading other files/directories using either -load or -loaddir flags \n... etc ...\n\n\n\nIf sourcing from another script there are hooks to modify and extend\nthe usage information as required.\n\n\n\n\n\n\nWe have some pretty extensive logging:\n\n\naquaq$ q torq.q -p 9999 -debug\nKDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems\n\n2013.11.05D12:22:42.597500000|aquaq|torq.q_3139_9999|INF|init|trap mode (initialisation errors will be caught and thrown, rather than causing an exit) is set to 0\n2013.11.05D12:22:42.597545000|aquaq|torq.q_3139_9999|INF|init|stop mode (initialisation errors cause the process loading to stop) is set to 0\n2013.11.05D12:22:42.597810000|aquaq|torq.q_3139_9999|INF|init|attempting to read required process parameters proctype,procname from file /torqhome/config/process.csv\n2013.11.05D12:22:42.598081000|aquaq|torq.q_3139_9999|INF|init|read in process parameters of proctype=hdb; procname=hdb1\n2013.11.05D12:22:42.598950000|aquaq|hdb1|INF|fileload|config file /torqhome/config/default.q found\n... etc ...\n\n\n\n\n\n\n\nWe have added functionality to find functions or variables defined\n    in the session, and also to search function definitions.\n\n\nq).api.f`max                                                                                                                                                                                   \nname                | vartype   namespace public descrip              ..\n--------------------| ------------------------------------------------..\nmaxs                | function  .q        1      \"\"                   ..\nmmax                | function  .q        1      \"\"                   ..\n.clients.MAXIDLE    | variable  .clients  0      \"\"                   ..\n.access.MAXSIZE     | variable  .access   0      \"\"                   ..\n.cache.maxsize      | variable  .cache    1      \"The maximum size in ..\n.cache.maxindividual| variable  .cache    1      \"The maximum size in ..\nmax                 | primitive           1      \"\"                   ..\n\nq)first 0!.api.p`.api                                                                                                                                                                          \nname     | `.api.f\nvartype  | `function\nnamespace| `.api\npublic   | 1b\ndescrip  | \"Find a function/variable/table/view in the current process\"\nparams   | \"[string:search string]\"\nreturn   | \"table of matching elements\"\n\nq).api.p`.api                                                                                                                                                                                  \nname        | vartype  namespace public descrip                       ..\n------------| --------------------------------------------------------..\n.api.f      | function .api      1      \"Find a function/variable/tabl..\n.api.p      | function .api      1      \"Find a public function/variab..\n.api.u      | function .api      1      \"Find a non-standard q public ..\n.api.s      | function .api      1      \"Search all function definitio..\n.api.find   | function .api      1      \"Generic method for finding fu..\n.api.search | function .api      1      \"Generic method for searching ..\n.api.add    | function .api      1      \"Add a function to the api des..\n.api.fullapi| function .api      1      \"Return the full function api ..\n\n\n\n\n\n\n\nWe have incorporated help.q.\n\n\nq)help`                                                                                                                                                                                        \nadverb    | adverbs/operators\nattributes| data attributes\ncmdline   | command line parameters\ndata      | data types\ndefine    | assign, define, control and debug\ndotz      | .z locale contents\nerrors    | error messages\nsave      | save/load tables\nsyscmd    | system commands\ntemporal  | temporal - date \n time casts\nverbs     | verbs/functions\n\n\n\n\n\n\n\nWe have separated and commented all of our config:\n\n\naquaq$ head config/default.q \n/- Default configuration - loaded by all processes\n\n/- Process initialisation\n\\d .proc\nloadcommoncode:1b   /- whether to load the common code defined at\n                        /- ${KDBCODE}/common\nloadprocesscode:0b  /- whether to load the process specific code defined at \n                        /- ${KDBCODE}/{process type} \nloadnamecode:0b     /- whether to load the name specific code defined at \n                    /- ${KDBCODE}/{name of process}\nloadhandlers:1b     /- whether to load the message handler code defined at \n                        /- ${KDBCODE}/handlers\nlogroll:1b      /- whether to roll the std out/err logs daily\n... etc ...\n\n\n\n\n\n\n\n\n\nOperating System and kdb+ Version\n\n\nAquaQ TorQ has been built and tested on the linux and OSX operating\nsystems though as far as we are aware there is nothing that would make\nthis incompatible with Solaris or Windows. It has also been tested with\nkdb+ 3.1 and 2.8. Please report any incompatibilities with other kdb+\nversions or operating systems.\n\n\n\n\nLicense\n\n\nThis code is released under the MIT license.", 
            "title": "About"
        }, 
        {
            "location": "/Overview/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/Overview/#what-is-kdb", 
            "text": "kdb+ is the market leading timeseries database from Kx Systems. kdb+\nis used predominently in the Financial Services sector to capture,\nprocess and analyse billions of records on a daily basis, with Kx\ncounting almost all of the top tier investment banks as customers. kdb+\nincorporates a programming language, q, which is known for its\nperformance and expressive power. Given the unsurpassed data management\nand analytical capabilities of kdb+, the applicability of kdb+\ntechnology extends beyond the financial domain into any sector where\nrapid pre-built or adhoc analysis of large datasets is required. Other\nsectors which have good use cases for kdb+ include utilities,\npharmaceuticals, telecoms, manufacturing, retail and any sector\nutilising telemetry or sensor data.", 
            "title": "What is kdb+?"
        }, 
        {
            "location": "/Overview/#what-is-aquaq-torq", 
            "text": "AquaQ TorQ is a framework which forms the basis of a production kdb+\nsystem by implementing some core functionality and utilities on top of\nkdb+, allowing developers to concentrate on the application business\nlogic. We have incorporated as many best practices as possible, with\nparticular focus on performance, process management, diagnostic\ninformation, maintainability and extensibility. We have kept the code as\nreadable as possible using descriptive comments, error messages,\nfunction names and variable names. Wherever possible, we have tried to\navoid re-inventing the wheel and instead have used contributed code from\ncode.kx.com (either directly or modified). All code sections taken from\ncode.kx.com are referenced in this document.  AquaQ TorQ can be extended or modified as required. We have chosen some\ndefault behaviour, but it can all be overridden. The features of AquaQ\nTorQ are:    Process Management: Each process is given a type and name. By\n    default these are used to determine the code base it loads, the\n    configuration loaded, log file naming and how it reports itself to\n    discovery services. Whenever possible we have tried to ensure that\n    all default behaviour can be overridden at the process type level,\n    and further at the process name level.    Code Management: Processes can optionally load common or process\n    type/name specific code bases. All code loading is error trapped.    Configuration Management: Configuration scripts can be loaded as\n    standard and with specific process type/name configuration\n    overriding default values. Configuration scripts are loaded in a\n    specific order; default, then process type specific, then process\n    name specific. Values loaded last will override values loaded\n    previously.    Usage Logging: All process usage is logged to a single text log file\n    and periodically rolled. Logging includes opening/closing of\n    connections, synchronous and asynchronous queries and functions\n    executed on the timer. Logged values include the request, the\n    details of where it came from, the time it was received, the time it\n    took, memory usage before and after, the size of the result set, the\n    status and any error message.    Incoming and Outgoing Connection Management: Incoming (client) and\n    outgoing (server) connections are stored and their usage monitored\n    through query counts and total data size counts. Connections are\n    stored and retrieved and can be set to automatically be re-opened as\n    required. The password used for outgoing connections can be\n    overridden at default, process type and process name level.    Access Controls: Basic access controls are provided, and could be\n    extended. These apply restrictions on the IP addresses of remote\n    connections, the users who can access the process, and the functions\n    that each user can execute. A similar hierarchical approach is used\n    for access control management as for configuration management.    Timer Extensions: Mechanism to allow multiple functions to be added\n    to the timer either on a repeating or one-off basis. Multiple\n    re-scheduling algorithms supplied for repeating timers.    Standard Out/Error Logging: Functions to print formatted messages to\n    standard out and error. Hooks are provided to extend these as\n    required, e.g. publication to centralised logging database. Standard\n    out and error are redirected to appropriately named, timestamped and\n    aliased log files, which are periodically rolled.    Error Handling: Different failure options are supplied in case code\n    fails to load; either exit upon failure, stop at the point of\n    failure or trap and continue.    Visualisation: Utilities to ease GUI development using websockets\n    and HTML5.    Documentation and Development Tools: Functionality to document the\n    system is built into AquaQ TorQ, and can be accessed directly from\n    every q session. Developers can extend the documentation as they add\n    new functions. Functionality for searching for functions and\n    variables by name and definition is provided, and for ordering\n    variables by memory usage. The standard help.q from code.kx is also\n    included.    Utilities: We intend to build out and add utilities as we find them\n    to be suitably useful and generic. So far we have:    Caching: allows a result set cache to be declared and result\n    sets to be stored and retrieved from the cache. Suitable for\n    functions which may be run multiple times with the same\n    parameters, with the underlying data not changing in a short\n    time frame;    Timezone Handling: derived from code.kx, allows conversion\n    between timestamps in different timezones;    Email: an library to send emails;    Async Messaging: allows easy use of advanced async messaging\n    methods such as deferred synchronous communication and async\n    communication using postback functions;    Heartbeating: each process can be set to publish heartbeats, and\n    subscribe to and manage heartbeats from other processes in the\n    environment;    Data Loading: utility wrapper around .Q.fsn to read a data file\n    from disk, manipulate it and write it out in chunks;    Subscriptions: allow processes to dynamically detect and\n    subscribe to datasources;    Tickerplant Log File Recovery: recover as many messages as\n    possible from corrupt log files;    Database Writing: utility functions for writing to, sorting and\n    parting on disk databases;    Compression: allows compression of a database. This can be\n    performed using a set of parameters for the entire database, but\n    also gives the flexibilty of compressing user-specified tables\n    and/or columns of those tables with different parameters if\n    required, and also offers decompression.      AquaQ TorQ will wrap easily around kdb+tick and therefore around any\ntickerplant, RDB, HDB or real time processing application. We currently\nhave several customised processes of our own:    Discovery Service: Every process has a type, name and set of\n    available attributes, which are used by other processes to connect\n    to it. The Discovery Service is a central point that can be used to\n    find other available processes. Client processes can subscribe to\n    updates from the discovery service as new processes become\n    available- the discovery service will notify its subscribers, which\n    can then use the supplied hook to implement required behavior e.g.\n    connect to the newly available process;    Gateway: A fully synchronous and asynchronous gateway is provided.  The gateway will connect to a defined list of process types (can be\nhomogenous or heterogeneous processes) and will route queries across\nthem according to the priority of received requests. The routing\nalgorithms can be easily modified e.g. give priority to user X, or\nonly route queries to processes which exist in the same data centre\nor geographical region to avoid the WAN (this would entail using the\nprocess attributes). The gateway can either return the result to the\nclient back down the same handle, or it can wrap it in a callback\nfunction to be invoked on the client;    Real Time Database (RDB): A customized version of the kdb+tick RDB,\n    to allow dynamic tickerplant subscriptions, reloading of multiple\n    HDBs using authenticated connections, and customized end-of-day save\n    downs. The RDB, WDB and tickerplant log replay share a common code\n    base to ensure that a save-down modification to a table is applied\n    across each of these processes.    Write Database (WDB): The job of a WDB is to write data to disk\n    rather than to serve client queries. WDBs usually write data out\n    periodically throughout the day, and are useful when there is too\n    much data to fit into memory and/or the end of day save operation\n    needs to be speeded up. The concept is based on\n     w.q    Tickerplant Log Replay: A process for replaying tickerplant log\n    files to create on-disk data sets. Extended features are provided\n    for only replaying subsets of log files (by message number and/or\n    table name), replaying in chunks, invoking bespoke final behaviour\n    etc.;    Reporter: The Reporter Process runs defined reports (q queries or\n    parameterized functions) against specific database or gateways on a\n    schedule. The results are retrieved and processed. Processing can be\n    user defined, or can be a standard operation such as writing the\n    data to disk, or emailing the results to a list of recipients. This\n    can be useful for running system checks or generating management\n    reports.    Housekeeping: A process to undertake housekeeping tasks\n    periodically, such as compressing and removing files that are no\n    longer used. Housekeeping looks up a file of instructions and\n    performs maintenance tasks on directories accordingly. Features\n    allow selective file deletion and zipping according to file age,\n    including a search string parameter and the ability to exclude items\n    from the search. The process can be scheduled, or run immediately\n    from the command line and can be extended as required to incorporate\n    more tasks.    File Alerter: A process to periodically scan a set of directories\n    and execute a function based on the availability of a file. This is\n    useful where files may arrive to the system during the day and must\n    be acted upon (e.g. files are uploaded to a shared directory by\n    users/clients). The functions to execute are defined by the user and\n    the whole process is driven by a csv file detailing the file to\n    search for, the function to execute and, optionally, a directory to\n    move the file to after it has been processed.    Monitor: A basic monitoring process which uses the Discovery Service\n    to locate the other processes within the system, listens for\n    heartbeats, and subscribes for log messages. This should be extended\n    as required but provides a basic central point for system health\n    checks;    Kill: A process used to kill other processes, optionally using the\n    Discovery Service to locate them.", 
            "title": "What is AquaQ TorQ?"
        }, 
        {
            "location": "/Overview/#a-large-scale-data-processing-platform", 
            "text": "One of the key drivers behind TorQ development has been to ensure all\nthe tools necessary to build a large scale data processing platform are\navailable.  kdb+tick \nprovides the basic building blocks, and a standard set-up usually looks\nsomething like this:   However, in reality it is usually more complicated. A larger scale\narchitecture serving large numbers of client queries and receiving data\nfrom multiple sources may look like this:   A common practice is to use a gateway (section gateway) to\nmanage client queries across back-end processes. The gateway can load\nbalance across processes and make failures transparent to the client. If\nthe clients access the gateway with asynchronous calls, then the gateway\ncan serve many requests at once and additionally implement client\nqueuing algorithms.  Other common production features include:    A modified version of the RDB (section sec:rdb) which does\n    different operations at end-of-day, reloads multiple HDB processes\n    etc.    A Write Database (section [sec:wdb]) which receives data from the\n    tickerplant and periodically writes it to disk. WDBs are used when\n    there is too much data in a day to fit into memory and/or to speed\n    up the end-of-day rollover job    Processes that load data from other sources either into the HDB\n    directly or to the RDB potentially via the tickerplant (section\n    [sec:dataloader]). The data may be dropped in specific locations\n    which have to be monitored (section [sec:filealerter])    A Reporting Engine (section [sec:reporter]) to run periodic\n    reports and do something with the result (e.g. generate an xls file\n    from the database and email it to senior management). Reporting\n    engines can also be used to run periodic checks of the system    A Discovery Service (section [sec:discovery]) to allow processes\n    to locate each other, and to allow processes to dynamically register\n    availability and push notifications around the system.    Basic Monitoring (section [sec:monitor]) of process availability    Housekeeping (section [sec:housekeeping]) to ensure log files are\n    tidied up, tickerplant log files are compressed/moved in a timely\n    fashion etc.", 
            "title": "A Large Scale Data Processing Platform"
        }, 
        {
            "location": "/Overview/#do-i-really-have-to-read-this-whole-document", 
            "text": "Hopefully not. The core of AquaQ TorQ is a script called torq.q and we\nhave tried to make it as descriptive as possible, so perhaps that will\nsuffice. The first place to look will be in the config files, the main\none being \\$KDBCONFIG/settings/default.q. This should contain a lot of\ninformation on what can be modified. In addition:    We have added a load of usage information:  aquaq$ q torq.q -usage\nKDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems\n\nGeneral:\n This script should form the basis of a production kdb+ environment.\n It can be sourced from other files if required, or used as a launch script before loading other files/directories using either -load or -loaddir flags \n... etc ...  If sourcing from another script there are hooks to modify and extend\nthe usage information as required.    We have some pretty extensive logging:  aquaq$ q torq.q -p 9999 -debug\nKDB+ 3.1 2013.10.08 Copyright (C) 1993-2013 Kx Systems\n\n2013.11.05D12:22:42.597500000|aquaq|torq.q_3139_9999|INF|init|trap mode (initialisation errors will be caught and thrown, rather than causing an exit) is set to 0\n2013.11.05D12:22:42.597545000|aquaq|torq.q_3139_9999|INF|init|stop mode (initialisation errors cause the process loading to stop) is set to 0\n2013.11.05D12:22:42.597810000|aquaq|torq.q_3139_9999|INF|init|attempting to read required process parameters proctype,procname from file /torqhome/config/process.csv\n2013.11.05D12:22:42.598081000|aquaq|torq.q_3139_9999|INF|init|read in process parameters of proctype=hdb; procname=hdb1\n2013.11.05D12:22:42.598950000|aquaq|hdb1|INF|fileload|config file /torqhome/config/default.q found\n... etc ...    We have added functionality to find functions or variables defined\n    in the session, and also to search function definitions.  q).api.f`max                                                                                                                                                                                   \nname                | vartype   namespace public descrip              ..\n--------------------| ------------------------------------------------..\nmaxs                | function  .q        1      \"\"                   ..\nmmax                | function  .q        1      \"\"                   ..\n.clients.MAXIDLE    | variable  .clients  0      \"\"                   ..\n.access.MAXSIZE     | variable  .access   0      \"\"                   ..\n.cache.maxsize      | variable  .cache    1      \"The maximum size in ..\n.cache.maxindividual| variable  .cache    1      \"The maximum size in ..\nmax                 | primitive           1      \"\"                   ..\n\nq)first 0!.api.p`.api                                                                                                                                                                          \nname     | `.api.f\nvartype  | `function\nnamespace| `.api\npublic   | 1b\ndescrip  | \"Find a function/variable/table/view in the current process\"\nparams   | \"[string:search string]\"\nreturn   | \"table of matching elements\"\n\nq).api.p`.api                                                                                                                                                                                  \nname        | vartype  namespace public descrip                       ..\n------------| --------------------------------------------------------..\n.api.f      | function .api      1      \"Find a function/variable/tabl..\n.api.p      | function .api      1      \"Find a public function/variab..\n.api.u      | function .api      1      \"Find a non-standard q public ..\n.api.s      | function .api      1      \"Search all function definitio..\n.api.find   | function .api      1      \"Generic method for finding fu..\n.api.search | function .api      1      \"Generic method for searching ..\n.api.add    | function .api      1      \"Add a function to the api des..\n.api.fullapi| function .api      1      \"Return the full function api ..    We have incorporated help.q.  q)help`                                                                                                                                                                                        \nadverb    | adverbs/operators\nattributes| data attributes\ncmdline   | command line parameters\ndata      | data types\ndefine    | assign, define, control and debug\ndotz      | .z locale contents\nerrors    | error messages\nsave      | save/load tables\nsyscmd    | system commands\ntemporal  | temporal - date   time casts\nverbs     | verbs/functions    We have separated and commented all of our config:  aquaq$ head config/default.q \n/- Default configuration - loaded by all processes\n\n/- Process initialisation\n\\d .proc\nloadcommoncode:1b   /- whether to load the common code defined at\n                        /- ${KDBCODE}/common\nloadprocesscode:0b  /- whether to load the process specific code defined at \n                        /- ${KDBCODE}/{process type} \nloadnamecode:0b     /- whether to load the name specific code defined at \n                    /- ${KDBCODE}/{name of process}\nloadhandlers:1b     /- whether to load the message handler code defined at \n                        /- ${KDBCODE}/handlers\nlogroll:1b      /- whether to roll the std out/err logs daily\n... etc ...", 
            "title": "Do I Really Have to Read This Whole Document?"
        }, 
        {
            "location": "/Overview/#operating-system-and-kdb-version", 
            "text": "AquaQ TorQ has been built and tested on the linux and OSX operating\nsystems though as far as we are aware there is nothing that would make\nthis incompatible with Solaris or Windows. It has also been tested with\nkdb+ 3.1 and 2.8. Please report any incompatibilities with other kdb+\nversions or operating systems.", 
            "title": "Operating System and kdb+ Version"
        }, 
        {
            "location": "/Overview/#license", 
            "text": "This code is released under the MIT license.", 
            "title": "License"
        }, 
        {
            "location": "/gettingstarted/", 
            "text": "Getting Started\n\n\nkdb+ is very customisable. Customisations are contained in q scripts (.q\nfiles), which define functions and variables which modify the behaviour\nof a process. Every q process can load a single q script, or a directory\ncontaining q scripts and/or q data files. Hooks are provided to enable\nthe programmer to apply a custom function to each entry point of the\nprocess (.z.p*), to be invoked on the timer (.z.ts) or when a variable\nin the top level namespace is amended (.z.vs). By default none of these\nhooks are implemented.\n\n\nWe provide a codebase and a single main script, torq.q. torq.q is\nessentially a wrapper for bespoke functionality which can load other\nscripts/directories, or can be sourced from other scripts. Whenever\npossible, torq.q should be invoked directly and used to load other\nscripts as required. torq.q will:\n\n\n\n\n\n\nensure the environment is set up correctly;\n\n\n\n\n\n\ndefine some common utility functions (such as logging);\n\n\n\n\n\n\nexecute process management tasks, such as discovering the name and\n    type of the process, and re-directing output to log files;\n\n\n\n\n\n\nload configuration;\n\n\n\n\n\n\nload the shared code based;\n\n\n\n\n\n\nset up the message handlers;\n\n\n\n\n\n\nload any required bespoke scripts.\n\n\n\n\n\n\nThe behavior of torq.q is modified by both command line parameters and\nconfiguration. We have tried to keep as much as possible in\nconfiguration files, but if the parameter either has a global effect on\nthe process or if it is required to be known before the configuration is\nread, then it is a command line parameter.\n\n\n\n\nUsing torq.q\n\n\ntorq.q can be invoked directly from the command line and be set to\nsource a specified file or directory. torq.q requires the 5 environment\nvariables to be set (see section\u00a0envvar). If using a unix\nenvironment, this can be done with the setenv.sh script. To start a\nprocess in the foreground without having to modify any other files (e.g.\nprocess.csv) you need to specify the type and name of the process as\nparameters. An example is below.\n\n\n$ . setenv.sh\n$ q torq.q -debug -proctype testproc -procname test1\n\n\n\nTo load a file, do:\n\n\n$ q torq.q -load myfile.q -debug -proctype testproc -procname test1\n\n\n\nIt can also be sourced from another script. If this is the case, some of\nthe variables can be overridden, and the usage information can be\nmodified or extended. Any variable that has a definition like below can\nbe overridden from the loading script.\n\n\nmyvar:@[value;`myvar;1 2 3]\n\n\n\nThe available command line parameters are:\n\n\n\n\n\n\n\n\nCmd Line Param\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-procname x -proctype y\n\n\nThe process name and process type\n\n\n\n\n\n\n-procfile x\n\n\nThe name of the file to get the process information from\n\n\n\n\n\n\n-load x [y..z]\n\n\nThe files or database directory to load\n\n\n\n\n\n\n-loaddir x [y..z]\n\n\nLoad all .q, .k files in specified directories\n\n\n\n\n\n\n-localtime\n\n\nSets processes running in local time rather than GMT for log messages, timer calls etc. The change is backwards compatible; without -localtime flag the process will print logs etc. in GMT but can also have a different .z.P\n\n\n\n\n\n\n-trap\n\n\nAny errors encountered during initialization when loading external files will be caught and logged, processing will continue\n\n\n\n\n\n\n-stop\n\n\nStop loading the file if an error is encountered but do not exit\n\n\n\n\n\n\n-noredirect\n\n\nDo not redirect std out/std err to a file (useful for debugging)\n\n\n\n\n\n\n-noredirectalias\n\n\nDo not create an alias for the log files (aliases drop any suffix e.g. timestamp suffix)\n\n\n\n\n\n\n-noconfig\n\n\nDo not load configuration\n\n\n\n\n\n\n-nopi\n\n\nReset the definition of .z.pi to the initial value (useful for debugging)\n\n\n\n\n\n\n-debug\n\n\nEquivalent to [-nopi -noredirect]\n\n\n\n\n\n\n-usage\n\n\nPrint usage info and exit\n\n\n\n\n\n\n\n\nIn addition any process variable in a namespace (.*.*) can be\noverridden from the command line. Any value supplied on the command line\nwill take priority over any other predefined value (.e.g. in a\nconfiguration or wrapper). Variable names should be supplied with full\nqualification e.g. -.servers.HOPENTIMEOUT 5000.\n\n\n\n\nEnvironment Variables\n\n\nFive environment variables are required:\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKDBCONFIG\n\n\nThe base configuration directory\n\n\n\n\n\n\nKDBCODE\n\n\nThe base code directory\n\n\n\n\n\n\nKDBLOGS\n\n\nWhere standard out/error and usage logs are written\n\n\n\n\n\n\nKDBHTML\n\n\nContains HTML files\n\n\n\n\n\n\nKDBLIB\n\n\nContains supporting library files\n\n\n\n\n\n\n\n\ntorq.q will check for these and exit if they are not set. If torq.q is\nbeing sourced from another script, the required environment variables\ncan be extended by setting .proc.envvars before loading torq.q.\n\n\n\n\nProcess Identification\n\n\nAt the crux of AquaQ TorQ is how processes identify themselves. This is\ndefined by two variables - .proc.proctype and .proc.procname which are\nthe type and name of the process respectively. These two values\ndetermine the code base and configuration loaded, and how they are\nconnected to by other processes. If both of these are not defined, the\nTorQ will attempt to use the port number a process was started on to\ndetermine the code base and configuration loaded.\n\n\nThe most important of these is the proctype. It is up to the user to\ndefine at what level to specify a process type. For example, in a\nproduction environment it would be valid to specify processes of type\n\u201chdb\u201d (historic database) and \u201crdb\u201d (real time database). It would also\nbe valid to segregate a little more granularly based on approximate\nfunctionality, for example \u201chdbEMEA\u201d and \u201chdbAmericas\u201d. The actual\nfunctionality of a process can be defined more specifically, but this\nwill be discussed later. The procname value is used solely for\nidentification purposes. A process can determine its type and name in a\nnumber of ways:\n\n\n\n\n\n\nFrom the process file in the default location of\n    $KDBCONFIG/process.csv;\n\n\n\n\n\n\nFrom the process file defined using the command line parameter\n    -procfile;\n\n\n\n\n\n\nFrom the port number it is started on, by referring to the process\n    file for further process details;\n\n\n\n\n\n\nUsing the command line parameters -proctype and -procname;\n\n\n\n\n\n\nBy defining .proc.proctype and .proc.procname in a script which\n    loads torq.q.\n\n\n\n\n\n\nFor options 4 and 5, both parameters must be defined using that method\nor neither will be used (the values will be read from the process file).\n\n\nFor option 3, TorQ will check the process file for any entries where the\nport matches the port number it has been started on, and deduce it\u2019s\nproctype and procname based on this port number and the corresponding\nhostname entry.\n\n\nThe process file has format as below.\n\n\naquaq$ cat config/process.csv \nhost,port,proctype,procname\naquaq,9997,rdb,rdb_europe_1\naquaq,9998,hdb,hdb_europe_1\naquaq,9999,hdb,hdb_europa_2\n\n\n\nThe process will read the file and try to identify itself based on the\nhost and port it is started on. The host can either be the value\nreturned by .z.h, or the ip address of the server. If the process can\nnot automatically identify itself it will exit, unless proctype and\nprocname were both passed in as command line parameters. If both of\nthese parameters are passed in then default configuration settings will\nbe used.\n\n\n\n\nLogging\n\n\nBy default, each process will redirect output to a standard out log and\na standard error log, and create aliases for them. These will be rolled\nat midnight on a daily basis. They are all written to the $KDBLOGS\ndirectory. The log files created are:\n\n\n\n\n\n\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nout_[procname]_[date].log\n\n\nTimestamped out log\n\n\n\n\n\n\nerr_[procname]_[date].log\n\n\nTimestamped error log\n\n\n\n\n\n\nout_[procname].log\n\n\nAlias to current log log\n\n\n\n\n\n\nerr_[procname].log\n\n\nAlias to current error log\n\n\n\n\n\n\n\n\nThe date suffix can be overridden by modifying the .proc.logtimestamp\nfunction and sourcing torq.q from another script. This could, for\nexample, change the suffixing to a full timestamp.\n\n\n\n\nConfiguration Loading\n\n\nDefault Configuration Loading\n\n\nDefault process configuration is contained in q scripts, and stored in\nthe $KDBCONFIG /settings directory. Each process tries to load all the\nconfiguration it can find and will attempt to load three configuration\nfiles in the below order:-\n\n\n\n\n\n\ndefault.q: default configuration loaded by all processes. In a\n    standard installation this should contain the superset of\n    customisable configuration, including comments;\n\n\n\n\n\n\n[proctype].q: configuration for a specific process type;\n\n\n\n\n\n\n[procname].q: configuration for a specific named process.\n\n\n\n\n\n\nThe only one which should always be present is default.q. Each of the\nother scripts can contain a subset of the configuration variables, which\nwill override anything loaded previously.\n\n\nApplication Configuration Loading\n\n\nApplication specific configuration can be stored in a user defined\ndirectory and made visible to TorQ by setting the $KDBAPPCONFIG\nenvironment variable. If $KDBAPPCONFIG is set, then TorQ will search\nthe $KDBAPPCONFIG/settings directory and load all configuration it can\nfind. Application configuration will be loaded after all default\nconfiguration in the following order:-\n\n\n\n\n\n\ndefault.q: Application default configuration loaded by all\n    processes.\n\n\n\n\n\n\n[[proctype]]{}.q: Application specific configuration for a\n    specific process type.\n\n\n\n\n\n\n[[procname]]{}.q: Appliction specific configuration for a specific\n    named process.\n\n\n\n\n\n\nAll loaded configuration will override anything loaded previously. None\nof the above scripts are required to be present and can contain a subset\nof the default configuration variables from the default configuration\ndirectory.\n\n\nAll configuration is loaded before code.\n\n\n\n\nCode Loading\n\n\nCode is loaded from the $KDBCODE directory. There is also a common\ncodebase, a codebase for each process type, and a code base for each\nprocess name, contained in the following directories and loaded in this\norder:\n\n\n\n\n\n\n$KDBCODE/common: shared codebase loaded by all processes;\n\n\n\n\n\n\n$KDBCODE/[proctype]: code for a specific process type;\n\n\n\n\n\n\n$KDBCODE/[procname]: code for a specific process name;\n\n\n\n\n\n\nFor any directory loaded, the load order can be specified by adding\norder.txt to the directory. order.txt dictates the order that files in\nthe directory are loaded. If a file is not in order.txt, it will still\nbe loaded but after all the files listed in order.txt have been loaded.\n\n\nIn addition to loading code form $KDBCODE, application specific code can be \nsaved in a user defined directory with the same structure as above, and made\nvisible to TorQ by setting the $KDBAPPCODE environment variable.\n\n\nIf this environment variable is set, TorQ will load codebase in the following order.\n\n\n\n\n\n\n$KDBCODE/common: shared codebase loaded by all processes;\n\n\n\n\n\n\n$KDBAPPCODE/common: application specific code shared by all processes;\n\n\n\n\n\n\n$KDBCODE/[proctype]: code for a specific process type;\n\n\n\n\n\n\n$KDBAPPCODE/[proctype]: application specific code for a specific process type;\n\n\n\n\n\n\n$KDBCODE/[procname]: code for a specific process name;\n\n\n\n\n\n\n$KDBAPPCODE/[procname]: application specific code for a specific process name;\n\n\n\n\n\n\nAdditional directories can be loaded using the -loaddir command line\nparameter.\n\n\n\n\nInitialization Errors\n\n\nInitialization errors can be handled in different ways. The default\naction is any initialization error causes the process to exit. This is\nto enable fail-fast type conditions, where it is better for a process to\nfail entirely and immediately than to start up in an indeterminate\nstate. This can be overridden with the -trap or -stop command line\nparameters. With -trap, the process will catch the error, log it, and\ncontinue. This is useful if, for example, the error is encountered\nloading a file of stored procedures which may not be invoked and can be\nreloaded later. With -stop the process will halt at the point of the\nerror but will not exit. Both -stop and -trap are useful for debugging.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/gettingstarted/#getting-started", 
            "text": "kdb+ is very customisable. Customisations are contained in q scripts (.q\nfiles), which define functions and variables which modify the behaviour\nof a process. Every q process can load a single q script, or a directory\ncontaining q scripts and/or q data files. Hooks are provided to enable\nthe programmer to apply a custom function to each entry point of the\nprocess (.z.p*), to be invoked on the timer (.z.ts) or when a variable\nin the top level namespace is amended (.z.vs). By default none of these\nhooks are implemented.  We provide a codebase and a single main script, torq.q. torq.q is\nessentially a wrapper for bespoke functionality which can load other\nscripts/directories, or can be sourced from other scripts. Whenever\npossible, torq.q should be invoked directly and used to load other\nscripts as required. torq.q will:    ensure the environment is set up correctly;    define some common utility functions (such as logging);    execute process management tasks, such as discovering the name and\n    type of the process, and re-directing output to log files;    load configuration;    load the shared code based;    set up the message handlers;    load any required bespoke scripts.    The behavior of torq.q is modified by both command line parameters and\nconfiguration. We have tried to keep as much as possible in\nconfiguration files, but if the parameter either has a global effect on\nthe process or if it is required to be known before the configuration is\nread, then it is a command line parameter.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/gettingstarted/#using-torqq", 
            "text": "torq.q can be invoked directly from the command line and be set to\nsource a specified file or directory. torq.q requires the 5 environment\nvariables to be set (see section\u00a0envvar). If using a unix\nenvironment, this can be done with the setenv.sh script. To start a\nprocess in the foreground without having to modify any other files (e.g.\nprocess.csv) you need to specify the type and name of the process as\nparameters. An example is below.  $ . setenv.sh\n$ q torq.q -debug -proctype testproc -procname test1  To load a file, do:  $ q torq.q -load myfile.q -debug -proctype testproc -procname test1  It can also be sourced from another script. If this is the case, some of\nthe variables can be overridden, and the usage information can be\nmodified or extended. Any variable that has a definition like below can\nbe overridden from the loading script.  myvar:@[value;`myvar;1 2 3]  The available command line parameters are:     Cmd Line Param  Description      -procname x -proctype y  The process name and process type    -procfile x  The name of the file to get the process information from    -load x [y..z]  The files or database directory to load    -loaddir x [y..z]  Load all .q, .k files in specified directories    -localtime  Sets processes running in local time rather than GMT for log messages, timer calls etc. The change is backwards compatible; without -localtime flag the process will print logs etc. in GMT but can also have a different .z.P    -trap  Any errors encountered during initialization when loading external files will be caught and logged, processing will continue    -stop  Stop loading the file if an error is encountered but do not exit    -noredirect  Do not redirect std out/std err to a file (useful for debugging)    -noredirectalias  Do not create an alias for the log files (aliases drop any suffix e.g. timestamp suffix)    -noconfig  Do not load configuration    -nopi  Reset the definition of .z.pi to the initial value (useful for debugging)    -debug  Equivalent to [-nopi -noredirect]    -usage  Print usage info and exit     In addition any process variable in a namespace (.*.*) can be\noverridden from the command line. Any value supplied on the command line\nwill take priority over any other predefined value (.e.g. in a\nconfiguration or wrapper). Variable names should be supplied with full\nqualification e.g. -.servers.HOPENTIMEOUT 5000.", 
            "title": "Using torq.q"
        }, 
        {
            "location": "/gettingstarted/#environment-variables", 
            "text": "Five environment variables are required:     Environment Variable  Description      KDBCONFIG  The base configuration directory    KDBCODE  The base code directory    KDBLOGS  Where standard out/error and usage logs are written    KDBHTML  Contains HTML files    KDBLIB  Contains supporting library files     torq.q will check for these and exit if they are not set. If torq.q is\nbeing sourced from another script, the required environment variables\ncan be extended by setting .proc.envvars before loading torq.q.", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/gettingstarted/#process-identification", 
            "text": "At the crux of AquaQ TorQ is how processes identify themselves. This is\ndefined by two variables - .proc.proctype and .proc.procname which are\nthe type and name of the process respectively. These two values\ndetermine the code base and configuration loaded, and how they are\nconnected to by other processes. If both of these are not defined, the\nTorQ will attempt to use the port number a process was started on to\ndetermine the code base and configuration loaded.  The most important of these is the proctype. It is up to the user to\ndefine at what level to specify a process type. For example, in a\nproduction environment it would be valid to specify processes of type\n\u201chdb\u201d (historic database) and \u201crdb\u201d (real time database). It would also\nbe valid to segregate a little more granularly based on approximate\nfunctionality, for example \u201chdbEMEA\u201d and \u201chdbAmericas\u201d. The actual\nfunctionality of a process can be defined more specifically, but this\nwill be discussed later. The procname value is used solely for\nidentification purposes. A process can determine its type and name in a\nnumber of ways:    From the process file in the default location of\n    $KDBCONFIG/process.csv;    From the process file defined using the command line parameter\n    -procfile;    From the port number it is started on, by referring to the process\n    file for further process details;    Using the command line parameters -proctype and -procname;    By defining .proc.proctype and .proc.procname in a script which\n    loads torq.q.    For options 4 and 5, both parameters must be defined using that method\nor neither will be used (the values will be read from the process file).  For option 3, TorQ will check the process file for any entries where the\nport matches the port number it has been started on, and deduce it\u2019s\nproctype and procname based on this port number and the corresponding\nhostname entry.  The process file has format as below.  aquaq$ cat config/process.csv \nhost,port,proctype,procname\naquaq,9997,rdb,rdb_europe_1\naquaq,9998,hdb,hdb_europe_1\naquaq,9999,hdb,hdb_europa_2  The process will read the file and try to identify itself based on the\nhost and port it is started on. The host can either be the value\nreturned by .z.h, or the ip address of the server. If the process can\nnot automatically identify itself it will exit, unless proctype and\nprocname were both passed in as command line parameters. If both of\nthese parameters are passed in then default configuration settings will\nbe used.", 
            "title": "Process Identification"
        }, 
        {
            "location": "/gettingstarted/#logging", 
            "text": "By default, each process will redirect output to a standard out log and\na standard error log, and create aliases for them. These will be rolled\nat midnight on a daily basis. They are all written to the $KDBLOGS\ndirectory. The log files created are:     Log File  Description      out_[procname]_[date].log  Timestamped out log    err_[procname]_[date].log  Timestamped error log    out_[procname].log  Alias to current log log    err_[procname].log  Alias to current error log     The date suffix can be overridden by modifying the .proc.logtimestamp\nfunction and sourcing torq.q from another script. This could, for\nexample, change the suffixing to a full timestamp.", 
            "title": "Logging"
        }, 
        {
            "location": "/gettingstarted/#configuration-loading", 
            "text": "", 
            "title": "Configuration Loading"
        }, 
        {
            "location": "/gettingstarted/#default-configuration-loading", 
            "text": "Default process configuration is contained in q scripts, and stored in\nthe $KDBCONFIG /settings directory. Each process tries to load all the\nconfiguration it can find and will attempt to load three configuration\nfiles in the below order:-    default.q: default configuration loaded by all processes. In a\n    standard installation this should contain the superset of\n    customisable configuration, including comments;    [proctype].q: configuration for a specific process type;    [procname].q: configuration for a specific named process.    The only one which should always be present is default.q. Each of the\nother scripts can contain a subset of the configuration variables, which\nwill override anything loaded previously.", 
            "title": "Default Configuration Loading"
        }, 
        {
            "location": "/gettingstarted/#application-configuration-loading", 
            "text": "Application specific configuration can be stored in a user defined\ndirectory and made visible to TorQ by setting the $KDBAPPCONFIG\nenvironment variable. If $KDBAPPCONFIG is set, then TorQ will search\nthe $KDBAPPCONFIG/settings directory and load all configuration it can\nfind. Application configuration will be loaded after all default\nconfiguration in the following order:-    default.q: Application default configuration loaded by all\n    processes.    [[proctype]]{}.q: Application specific configuration for a\n    specific process type.    [[procname]]{}.q: Appliction specific configuration for a specific\n    named process.    All loaded configuration will override anything loaded previously. None\nof the above scripts are required to be present and can contain a subset\nof the default configuration variables from the default configuration\ndirectory.  All configuration is loaded before code.", 
            "title": "Application Configuration Loading"
        }, 
        {
            "location": "/gettingstarted/#code-loading", 
            "text": "Code is loaded from the $KDBCODE directory. There is also a common\ncodebase, a codebase for each process type, and a code base for each\nprocess name, contained in the following directories and loaded in this\norder:    $KDBCODE/common: shared codebase loaded by all processes;    $KDBCODE/[proctype]: code for a specific process type;    $KDBCODE/[procname]: code for a specific process name;    For any directory loaded, the load order can be specified by adding\norder.txt to the directory. order.txt dictates the order that files in\nthe directory are loaded. If a file is not in order.txt, it will still\nbe loaded but after all the files listed in order.txt have been loaded.  In addition to loading code form $KDBCODE, application specific code can be \nsaved in a user defined directory with the same structure as above, and made\nvisible to TorQ by setting the $KDBAPPCODE environment variable.  If this environment variable is set, TorQ will load codebase in the following order.    $KDBCODE/common: shared codebase loaded by all processes;    $KDBAPPCODE/common: application specific code shared by all processes;    $KDBCODE/[proctype]: code for a specific process type;    $KDBAPPCODE/[proctype]: application specific code for a specific process type;    $KDBCODE/[procname]: code for a specific process name;    $KDBAPPCODE/[procname]: application specific code for a specific process name;    Additional directories can be loaded using the -loaddir command line\nparameter.", 
            "title": "Code Loading"
        }, 
        {
            "location": "/gettingstarted/#initialization-errors", 
            "text": "Initialization errors can be handled in different ways. The default\naction is any initialization error causes the process to exit. This is\nto enable fail-fast type conditions, where it is better for a process to\nfail entirely and immediately than to start up in an indeterminate\nstate. This can be overridden with the -trap or -stop command line\nparameters. With -trap, the process will catch the error, log it, and\ncontinue. This is useful if, for example, the error is encountered\nloading a file of stored procedures which may not be invoked and can be\nreloaded later. With -stop the process will halt at the point of the\nerror but will not exit. Both -stop and -trap are useful for debugging.", 
            "title": "Initialization Errors"
        }, 
        {
            "location": "/utilities/", 
            "text": "Utilities\n\n\nWe have provided several utility scripts, which either implement\ndeveloper aids or standard operations which are useful across processes.\n\n\n\n\napi.q\n\n\nThis provides a mechanism for documenting and publishing\nfunction/variable/table or view definitions within the kdb+ process. It\nprovides a search facility both by name and definition (in the case of\nfunctions). There is also a function for returning the approximate\nmemory usage of each variable in the process in descending order.\n\n\nDefinitions are added using the .api.add function. A variable can be\nmarked as public or private, and given a description, parameter list and\nreturn type. The search functions will return all the values found which\nmatch the pattern irrespective of them having a pre-defined definition.\n\n\nWhether a value is public or private is defined in the definitions\ntable. If not found then by default all values are private, except those\nwhich live in the .q or top level namespace.\n\n\n.api.f is used to find a function, variable, table or view based on a\ncase-insensitive pattern search. If a symbol parameter is supplied, a\nwildcard search of *[suppliedvalue]* is done. If a string is\nsupplied, the value is used as is, meaning other non-wildcard regex\npattern matching can be done.\n\n\n\n    q).api.f`max                                                                                                                                                                                                                    \n    name                | vartype   namespace public descrip             ..\n    --------------------| -----------------------------------------------..\n    maxs                | function  .q        1      \n                  ..\n    mmax                | function  .q        1      \n                  ..\n    .clients.MAXIDLE    | variable  .clients  0      \n                  ..\n    .access.MAXSIZE     | variable  .access   0      \n                  ..\n    .cache.maxsize      | variable  .cache    1      \nThe maximum size in..\n    .cache.maxindividual| variable  .cache    1      \nThe maximum size in..\n    max                 | primitive           1      \n                  ..\n    q).api.f\nmax*\n                                                                                                                                                                                                                  \n    name| vartype   namespace public descrip params return\n    ----| ------------------------------------------------\n    maxs| function  .q        1      \n      \n     \n    \n    max | primitive           1      \n      \n     \n    \n\n\n\n\n\n.api.p is the same as .api.f, but only returns public functions. .api.u\nis as .api.p, but only includes user defined values i.e. it excludes q\nprimitives and values found in the .q, .Q, .h and .o namespaces.\n.api.find is a more general version of .api.f which can be used to do\ncase sensitive searches.\n\n\n.api.s is used to search function definitions for specific values.\n\n\nq).api.s\"*max*\"                                                                                                                                                                                                                 \nfunction            definition                                       ..\n---------------------------------------------------------------------..\n.Q.w                \"k){`used`heap`peak`wmax`mmap`mphy`syms`symw!(.\\\"..\n.clients.cleanup    \"{if[count w0:exec w from`.clients.clients where ..\n.access.validsize   \"{[x;y;z] $[superuser .z.u;x;MAXSIZE\ns:-22!x;x;'\\..\n.servers.getservers \"{[nameortype;lookups;req;autoopen;onlyone]\\n r:$..\n.cache.add          \"{[function;id;status]\\n \\n res:value function;\\n..\n\n\n\n.api.m is used to return the approximate memory usage of variables and\nviews in the process, retrieved using -22!. Views will be re-evaluated\nif required. Use .api.mem[0b] if you do not want to evaluate and\nreturn views.\n\n\nq).api.m[]                                                                                                                                                                                                                      \nvariable          size    sizeMB\n--------------------------------\n.tz.t             1587359 2     \n.help.TXT         15409   0     \n.api.detail       10678   0     \n.proc.usage       3610    0     \n.proc.configusage 1029    0     \n..\n\n\n\n.api.whereami[lambda] can be used to retrieve the name of a function\ngiven its definition. This can be useful in debugging.\n\n\nq)g:{x+y}                                                                                                                                                                                                                                                                     \nq)f:{20 + g[x;10]}                                                                                                                                                                                                                                                            \nq)f[10]                                                                                                                                                                                                                                                                       \n40\nq)f[`a]                                                                                                                                                                                                                                                                       \n{x+y}\n`type\n+\n`a\n10\nq)).api.whereami[.z.s]                                                                                                                                                                                                                                                        \n`..g\n\n\n\napidetails.q\n\n\nThis file in both the common and the handler directories is used to add to the api using the functions defined in api.q\n\n\n\n\ntimer.q\n\n\nkdb+ provides a single timer function, .z.ts which is triggered with the\nfrequency specified by -t. We have provided an extension to allow\nmultiple functions to be added to the timer and fired when required. The\nbasic concept is that timer functions are registered in a table, with\n.z.ts periodically checking the table and running whichever functions\nare required. This is not a suitable mechanism where very high frequency\ntimers are required (e.g. sub 500ms).\n\n\nThere are two ways a function can be added to a timer- either as a\nrepeating timer, or to fire at a specific time. When a repeating timer\nis specified, there are three options as to how the timer can be\nrescheduled. Assuming that a timer function with period P is scheduled\nto fire at time T0, actually fires at time T1 and finishes at time T2,\nthen\n\n\n\n\n\n\nmode 0 will reschedule for T0+P;\n\n\n\n\n\n\nmode 1 will reschedule for T1+P;\n\n\n\n\n\n\nmode 2 will reschedule for T2+P.\n\n\n\n\n\n\nBoth mode 0 and mode 1 have the potential for causing the timer to back\nup if the finish time T2 is after the next schedule time. See\n.api.p\u201c.timer.*\u201dfor more details.\n\n\n\n\nasync.q\n\n\nkdb+ processes can communicate with each using either synchronous or\nasynchronous calls. Synchronous calls expect a response and so the\nserver must process the request when it is received to generate the\nresult and return it to the waiting client. Asynchronous calls do not\nexpect a response so allow for greater flexibility. The effect of\nsynchronous calls can be replicated with asynchronous calls in one of\ntwo ways (further details in section\u00a0gateway):\n\n\n\n\n\n\ndeferred synchronous: the client sends an async request, then blocks\n    on the handle waiting for the result. This allows the server more\n    flexibility as to how and when the query is processed;\n\n\n\n\n\n\nasynchronous postback: the client sends an async request which is\n      wrapped in a function to be posted back to the client when the\n      result is ready. This allows the server flexibility as to how and\n      when the query is processed, and allows the client to continue\n      processing while the server is generating the result.\n\n\n\n\n\n\nThe code for both of these can get a little tricky, largely due to the\namount of error trapping required. We have provided two functions to\nallow these methods to be used more easily. .async.deferred takes a list\nof handles and a query, and will return a two item list of\n(success;results).\n\n\nq).async.deferred[3 5;({system\"sleep 1\";system\"p\"};())]                                                                                                                                                                                     \n1    1   \n9995 9996\nq).async.deferred[3 5;({x+y};1;2)]                                                                                                                                                                                                          \n1 1\n3 3\nq).async.deferred[3 5;({x+y};1;`a)]                                                                                                                                                                                                         \n0                         0                        \n\"error: server fail:type\" \"error: server fail:type\"\nq).async.deferred[3 5 87;({system\"sleep 1\";system\"p\"};())]                                                                                                                                                                                  \n1     1     0                                       \n9995i 9996i \"error: comm fail: failed to send query\"\n\n\n\n.async.postback takes a list of handles, a query, and the name or lambda\nof the postback function to return the result to. It will immediately\nreturn a success vector, and the results will be posted back to the\nclient when ready.\n\n\nq).async.postback[3 5;({system\"sleep 1\";system\"p\"};());`showresult]                                                                                                                                                                         \n11b\nq)                                                                                                                                                                                                                                          \nq)9995i\n9996i\n\nq).async.postback[3 5;({x+y};1;2);`showresult]                                                                                                                                                                                              \n11b\nq)3\n3\n\nq).async.postback[3 5;({x+y};1;`a);`showresult]                                                                                                                                                                                             \n11b\nq)\"error: server fail:type\"\n\"error: server fail:type\"\n\nq).async.postback[3 5;({x+y};1;`a);showresult]                                                                                                                                                                                              \n11b\nq)\"error: server fail:type\"\n\"error: server fail:type\"\n\nq).async.postback[3 5 87;({x+y};1;2);showresult]                                                                                                                                                                                            \n110b\nq)3\n3\n\n\n\nFor more details, see .api.p\u201c.async.*\u201d.\n\n\n\n\ncache.q\n\n\ncache.q provides a mechanism for storing function results in a cache and\nreturning them from the cache if they are available and non stale. This\ncan greatly boost performance for frequently run queries.\n\n\nThe result set cache resides in memory and as such takes up space. It is\nup to the programmer to determine which functions are suitable for\ncaching. Likely candidates are those where some or all of the following\nconditions hold:\n\n\n\n\n\n\nthe function is run multiple times with the same parameters (perhaps\n    different clients all want the same result set);\n\n\n\n\n\n\nthe result set changes infrequently or the clients can accept\n      slightly out-of-date values;\n\n\n\n\n\n\nthe result set is not too large and/or is relatively expensive to\n      produce. For example, it does not make sense to cache raw data\n      extracts.\n\n\n\n\n\n\nThe cache has a maximum size and a minimum size for any individual\nresult set, both of which are defined in the configuration file. Size\nchecks are done with -22! which will give an approximation (but\nunderestimate) of the result set size. In the worst case the estimate\ncould be half the size of the actual size.\n\n\nIf a new result set is to be cached, the size is checked. Assuming it\ndoes not exceed the maximum individual size then it is placed in the\ncache. If the new cache size would exceed the maximum allowed space,\nother result sets are evicted from the cache. The current eviction\npolicy is to remove the least recently accessed result sets until the\nrequired space is freed. The cache performance is tracked in a table.\nCache adds, hits, fails, reruns and evictions are monitored.\n\n\nThe main function to use the cache is .cache.execute[function;\nstaletime]. If the function has been executed within the last\nstaletime, then the result is returned from the cache. Otherwise the\nfunction is executed and placed in the cache.\n\n\nThe function is run and the result placed in the cache:\n\n\nq)\\t r:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01]                                                                                                                                                                     \n2023\nq)r                                                                                                                                                                                                                             \n3\n\n\n\nThe second time round, the result set is returned immediately from the\ncache as we are within the staletime value:\n\n\nq)\\t r1:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01]                                                                                                                                                                    \n0\nq)r1                                                                                                                                                                                                                            \n3\n\n\n\nIf the time since the last execution is greater than the required stale\ntime, the function is re-run, the cached result is updated, and the\nresult returned:\n\n\nq)\\t r2:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:00]                                                                                                                                                                    \n2008\nq)r2                                                                                                                                                                                                                            \n3\n\n\n\nThe cache performance is tracked:\n\n\nq).cache.getperf[]                                                                                                                                                                                                              \ntime                          id status function                  \n------------------------------------------------------------------\n2013.11.06D12:41:53.103508000 2  add    {system\"sleep 2\"; x+y} 1 2\n2013.11.06D12:42:01.647731000 2  hit    {system\"sleep 2\"; x+y} 1 2\n2013.11.06D12:42:53.930404000 2  rerun  {system\"sleep 2\"; x+y} 1 2\n\n\n\nSee .api.p.cache.*for more details.\n\n\n\n\nemail.q\n\n\nA library file is provided to allow TorQ processes to send emails using\nan SMTP server. This is a wrapper around the standard libcurl library.\nThe library file is currently available for Windows (32 bit), Linux (32\nand 64 bit) and OSX (32 and 64 bit). The associated q script contains\ntwo main methods for creating a connection and sending emails. The email\nlibrary requires a modification to the path to find the required libs -\nsee the top of email.q for details.\n\n\nThe main connection method .email.connect takes a single dictionary\nparameter and returns 0i for success and -1i for failure.\n\n\n\n\n\n\n\n\nParameter\n\n\nReq\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nurl\n\n\nY\n\n\nsymbol\n\n\nURL of mail server e.g. smtp://mail.example.com\n\n\n\n\n\n\nuser\n\n\nY\n\n\nsymbol\n\n\nUsername of user to login as\n\n\n\n\n\n\npassword\n\n\nY\n\n\nsymbol\n\n\nPassword for user\n\n\n\n\n\n\nusessl\n\n\nN\n\n\nboolean\n\n\nConnect using SSL/TLS, defaults to false\n\n\n\n\n\n\nfrom\n\n\nN\n\n\nsymbol\n\n\nEmail from field, defaults to torq@aquaq.co.uk\n\n\n\n\n\n\ndebug\n\n\nN\n\n\ninteger\n\n\nDebug level. 0=no output, 1=normal output, 2=verbose output. Default is 1\n\n\n\n\n\n\n\n\nAn example is:\n\n\nq).email.connect[`url`user`password`from`usessl`debug!(`$\"smtp://mail.example.com:80\";`$\"torquser@aquaq.co.uk\";`hello;`$\"torquser@aquaq.co.uk\";0b;1i)]\n02 Jan 2015 11:45:19   emailConnect: url is set to smtp://mail.example.com:80\n02 Jan 2015 11:45:19   emailConnect: user is set to torquser@aquaq.co.uk\n02 Jan 2015 11:45:19   emailConnect: password is set\n02 Jan 2015 11:45:19   emailConnect: from is set torquser@aquaq.co.uk\n02 Jan 2015 11:45:19   emailConnect: trying to connect\n02 Jan 2015 11:45:19   emailConnect: connected, socket is 5\n0i\n\n\n\nThe email sending function .email.send takes a single dictionary\nparameter containing the details of the email to send. A connection must\nbe established before an email can be sent. The send function returns an\ninteger of the email length on success, or -1 on failure.\n\n\n\n\n\n\n\n\nParameter\n\n\nReq\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nto\n\n\nY\n\n\nsymbol (list)\n\n\naddresses to send to\n\n\n\n\n\n\nsubject\n\n\nY\n\n\nchar list\n\n\nemail subject\n\n\n\n\n\n\nbody\n\n\nY\n\n\nlist of char lists\n\n\nemail body\n\n\n\n\n\n\ncc\n\n\nN\n\n\nsymbol (list)\n\n\ncc list\n\n\n\n\n\n\nbodyType\n\n\nN\n\n\nsymbol\n\n\ntype of email body. Can be `text or `html. Default is `text\n\n\n\n\n\n\ndebug\n\n\nN\n\n\ninteger\n\n\nDebug level. 0=no output, 1=normal output,2=verbose output. Default is 1\n\n\n\n\n\n\n\n\nAn example is:\n\n\nq).email.send[`to`subject`body`debug!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\");1i)]\n02 Jan 2015 12:39:29   sending email with subject: test email\n02 Jan 2015 12:39:29   email size in bytes is 16682\n02 Jan 2015 12:39:30   emailSend: email sent\n16682i\n\n\n\nNote that if emails are sent infrequently the library must re-establish\nthe connection to the mail server (this will be done automatically after\nthe initial connection). In some circumstances it may be better to batch\nemails together to send, or to offload email sending to separate\nprocesses as communication with the SMTP server can take a little time.\n\n\nTwo further functions are available, .email.connectdefault and\n.email.senddefault. These are as above but will use the default\nconfiguration defined within the configuration files as the relevant\nparameters passed to the methods. In addition, .email.senddefault will\nautomatically establish a connection.\n\n\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\"))]\n2015.01.02D12:43:34.646336000|aquaq||discovery1|INF|email|sending email\n2015.01.02D12:43:35.743887000|aquaq||discovery1|INF|email|connection to mail server successful\n2015.01.02D12:43:37.250427000|aquaq|discovery1|INF|email|email sent\n16673i\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\"))]\n2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email\n2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent\n16675i\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\");`\"$/home/ashortt/example.txt\")]\n2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email\n2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent\n47338i\n\n\n\n.email.test will attempt to establish a connection to the default\nconfigured email server and send a test email to the specified address.\ndebug should be set to 2i (verbose) to extract the full information.\n\n\nq).email.debug:2i\nq).email.test `$\"test@aquaq.co.uk\"\n...\n\n\n\nAdditionally functions are available within the email library. See\n.api.p.email.*for more details.\n\n\nEmails with SSL certificates from Windows\n\n\nIf you wish to send emails via an account which requires authentication\nfrom Windows (e.g. Hotmail, Gmail) then you have to do a few extra steps\nas usessl must be true and Windows does not usually find the correct\ncertificate. The steps are:\n\n\n\n\n\n\ndownload\n    \nthis\n\n    and save it to your PC\n\n\n\n\n\n\nset\n\n\n  CURLOPT_CAINFO=c:/path/to/cabundle_file/ca-bundle.crt\n\n\n\n\n\n\n\nMore information is available\n\nhere\n\nand \nhere\n\n\n\n\ntimezone.q\n\n\nA slightly customised version of the timezone conversion functionality\nfrom code.kx. It loads a table of timezone information from\n$KDBCONFIG. See .api.p.tz.*for more details.\n\n\n\n\ncompress.q\n\n\ncompress.q applies compression to any kdb+ database, handles all\npartition types including date, month, year, int, and can deal with top\nlevel splayed tables. It will also decompress files as required. Once\nthe compression/decompression is complete, summary statistics are\nreturned, with detailed statistics for each compressed or decompressed\nfile held in a table.\n\n\nThe utility is driven by the configuration specified within a csv file.\nDefault parameters can be given, and these can be used to compress all\nfiles within the database. However, the compress.q utility also provides\nthe flexibility to compress different tables with different compression\nparameters, and different columns within tables using different\nparameters. A function is provided which will return a table showing\neach file in the database to be compressed, and how, before the\ncompression is performed.\n\n\nCompression is performed using the -19! operator, which takes 3\nparameters; the compression algorithm to use (0 - none, 1 - kdb+ IPC, 2\n- gzip), the compression blocksize as a power of 2 (between 12 and 19),\n  and the level of compression to apply (from 0 - 9, applicable only for\n  gzip). (For further information on -19! and the parameters used, see\n  code.kx.com.)\n\n\nThe compressionconfig.csv file should have the following format:\n\n\ntable,minage,column,calgo,cblocksize,clevel\ndefault,20,default,2,17,6\ntrades,20,default,1,17,0\nquotes,20,asize,2,17,7\nquotes,20,bsize,2,17,7\n\n\n\nThis file can be placed in the config folder, or a path to the file\ngiven at run time.\n\n\nThe compression utility compresses all tables and columns present in the\nHDB but not specified in the driver file according the default\nparameters. In effect, to compress an entire HDB using the same\ncompression parameters, a single row with name default would suffice. To\nspecify that a particular table should be compressed in a certain\ndifferent manner, it should be listed in the table. If default is given\nas the column for this table, then all of the columns of that table will\nbe compressed accordingly. To specify the compression parameters for\nparticular columns, these should be listed individually. For example,\nthe file above will compress trades tables 20 days old or more with an\nalgorithm of 1, and a blocksize of 17. The asize and bsize columns of\nany quotes tables older than 20 days old will be compressed using\nalgorithm 2, blocksize 17 and level 7. All other files present will be\ncompressed according to the default, using an algorithm 2, blocksize 17\nand compression level 6. To leave files uncompressed, you must specify\nthem explicitly in the table with a calgo of 0. If the file is already\ncompressed, note that an algorithm of 0 will decompress the file.\n\n\nThis utility should be used with caution. Before running the compression\nit is recommended to run the function .cmp.showcomp, which takes three\nparameters - the path to the database, the path to the csv file, and the\nmaximum age of the files to be compressed:\n\n\n.cmp.showcomp[`:/full/path/to/HDB;.cmp.inputcsv;maxage]   \n        /- for using the csv file in the config folder\n.cmp.showcomp[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage]    \n        /- to specify a file\n\n\n\nThis function produces a table of the files to be compressed, the\nparameters with which they will be compressed, and the current size of\nthe file. Note that the current size column is calculated using hcount;\non a file which is already compressed this returns the uncompressed\nlength, i.e. this cannot be used as a signal as to whether the file is\ncompressed already.\n\n\nfullpath                        column table  partition  age calgo cblocksize clevel compressage currentsize\n-------------------------------------------------------------------------------------\n:/home/hdb/2013.11.05/depth/asize1 asize1 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/asize2 asize2 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/asize3 asize3 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/ask1   ask1   depth  2013.11.05 146 0     17         8      1           1575904\n....\n\n\n\nTo then run the compression function, use .cmp.compressmaxage with the\nsame parameters as .cmp.showcomp (hdb path, csv path, maximum age of\nfiles):\n\n\n.cmp.compressmaxage[`:/full/path/to/HDB;.cmp.inputcsv;maxage]   \n        /- for using the csv file in the config folder\n.cmp.compressmaxage[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage]    \n        /- to specify a file\n\n\n\nTo run compression on all files in the database disregarding the maximum\nage of the files (i.e. from minage as specified in the configuration\nfile to infinitely old), then use:\n\n\n.cmp.docompression[`:/full/path/to/HDB;.cmp.inputcsv]   \n        /- for using the csv file in the config folder\n.cmp.docompression[`:/full/path/to/HDB;`:/full/path/to/csvfile]    \n        /- to specify a file\n\n\n\nLogs are produced for each file which is compressed or decompressed.\nOnce the utility is complete, the statistics of the compression are also\nlogged. This includes the memory savings in MB from compression, the\nadditional memory usage in MB for decompression, the total compression\nratio, and the total decompression ratio:\n\n\n|comp1|INF|compression|Memory savings from compression: 34.48MB. Total compression ratio: 2.51.\n|comp1|INF|compression|Additional memory used from de-compression: 0.00MB. Total de-compression ratio: .\n|comp1|INF|compression|Check .cmp.statstab for info on each file.\n\n\n\nA table with the compressed and decompressed length for each individual\nfile, in descending order of compression ratio, is also produced. This\ncan be found in .cmp.statstab:\n\n\nfile                    algo compressedLength uncompressedLength compressionratio\n-----------------------------------------------------------------------------------\n:/hdb/2014.03.05/depth/asize1 2    89057            772600             8.675343\n:/hdb/2014.01.06/depth/asize1 2    114930           995532             8.662073\n:/hdb/2014.03.05/depth/bsize1 2    89210            772600             8.660464\n:/hdb/2014.03.12/depth/bsize1 2    84416            730928             8.658643\n:/hdb/2014.01.06/depth/bsize1 2    115067           995532             8.651759\n.....\n\n\n\nA note for windows users - windows supports compression only with a\ncompression blocksize of 16 or more.\n\n\n\n\ndataloader.q\n\n\nThis script contains some utility functions to assist in loading data\nfrom delimited files (e.g. comma separated, tab delimited). It is a more\ngeneric version of \nthe data loader example on\ncode.kx\n.\nThe supplied functions allow data to be read in configurable size chunks\nand written out to the database. When all the data is written, the\non-disk data is re-sorted and the attributes are applied. The main\nfunction is .loader.loadalldata which takes two parameters- a dictionary\nof loading parameters and a directory containing the files to read. The\ndictionary should/can have the following fields:\n\n\n\n\n\n\n\n\nParameter\n\n\nReq\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nheaders\n\n\nY\n\n\nsymbol list\n\n\nNames of the header columns in the file\n\n\n\n\n\n\ntypes\n\n\nY\n\n\nchar list\n\n\nData types to read from the file\n\n\n\n\n\n\nseparator\n\n\nY\n\n\nchar[list]\n\n\nDelimiting character. Enlist it if first line of file is header data\n\n\n\n\n\n\ntablename\n\n\nY\n\n\nsymbol\n\n\nName of table to write data to\n\n\n\n\n\n\ndbdir\n\n\nY\n\n\nsymbol\n\n\nDirectory to write data to\n\n\n\n\n\n\npartitiontype\n\n\nN\n\n\nsymbol\n\n\nPartitioning to use. Must be one of `date`month`year`int. Default is `date\n\n\n\n\n\n\npartitioncol\n\n\nN\n\n\nsymbol\n\n\nColumn to use to extract partition information.Default is `time\n\n\n\n\n\n\ndataprocessfunc\n\n\nN\n\n\nfunction\n\n\nDiadic function to process data after it has been read in. First argument is load parameters dictionary, second argument is data which has been read in. Default is {[x;y] y}\n\n\n\n\n\n\nchunksize\n\n\nN\n\n\nint\n\n\nData size in bytes to read in one chunk. Default is 100 MB\n\n\n\n\n\n\ncompression\n\n\nN\n\n\nint list\n\n\nCompression parameters to use e.g. 17 2 6. Default is empty list for no compression\n\n\n\n\n\n\ngc\n\n\nN\n\n\nboolean\n\n\nWhether to run garbage collection at appropriate points. Default is 0b (false)\n\n\n\n\n\n\n\n\nExample usage:\n\n\n.loader.loadallfiles[`headers`types`separator`tablename`dbdir!(`sym`time`price`volume;\"SP  FI\";\",\";`trade;`:hdb); `:TDC/toload]\n.loader.loadallfiles[`headers`types`separator`tablename`dbdir`dataprocessfunc`chunksize`partitiontype`partitioncol`compression`gc!(`sym`time`price`volume;\"SP  FI\";enlist\",\";`tradesummary;`:hdb;{[p;t] select sum size, max price by date:time.date from t};`int$500*2 xexp 20;`month;`date;16 1 0;1b); `:TDC/toload]\n\n\n\n\n\nsubscriptions.q\n\n\nThe subscription utilities allow multiple subscriptions to different\ndata sources to be managed and maintained. Automatic resubscriptions in\nthe event of failure are possible, along as specifying whether the\nprocess will get the schema and replay the log file from the remote\nsource (e.g. in the case of tickerplant subscriptions).\n\n\n.sub.getsubscriptionhandles is used to get a table of processes to\nsubscribe to. The following can be used to return a table of all\nconnected processes of type tickerplant:\n\n\n.sub.getsubscriptionhandles[`tickerplant;`;()!()]\n\n\n\n.sub.subscribe is used to subscribe to a process for the supplied list\nof tables and instruments. For example, to subscribe to instruments A, B\nand C for the quote table from all tickerplants:\n\n\n.sub.subscribe[`trthquote;`A`B;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;`;()!()]\n\n\n\nThe subscription method uses backtick for \u201call\u201d (which is the same as\nkdb+tick). To subscribe to all tables, all instruments, from all\ntickerplants:\n\n\n.sub.subscribe[`;`;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;`;()!()]\n\n\n\nSee .api.p\u201c.sub.*\u201d for more details.\n\n\n\n\npubsub.q\n\n\npubsub.q is essentially a placeholder script to allow publish and\nsubscribe functionality to be implemented. Licenced kdb+tick users can\nuse the publish and subscribe functionality implemented in u.[k|q]. If\nu.[k|q] is placed in the common code directory and loaded before\npubsub.q (make sure u.[k|q] is listed before pubsub.q in order.txt)\nthen publish and subscribe will be implemented. You can also build out\nthis file to add your own publish and subscribe routines as required.\n\n\n\n\nkafka.q\n\n\nkafka.q provides q language bindings for Apache Kafka, a 'distributed streaming\nplatform', a real time messaging system with persistent storage in message logs.\n\n\nThe core functionality of Kafka \u2013 pub/sub messaging with persisted logs, will be\nfamiliar to most readers as the functionality offered by the kdb+ tick\ntickerplant. The tickerplant log allows the real time database and other\nconsumers to replay a day\u2019s events to recover state. An application architecture\nbuilt around Kafka could dispense with a tickerplant component, and have RDBs\nand other real time clients query Kafka on startup for offsets, and play back\nthe data they need. While not suitable for very low latency access to streaming\ndata, it would carry some advantages for very high throughput applications,\nparticularly those in the cloud:\n\n\n\n\nKafka\u2019s distributed nature should allow it to scale more transparently than\nsplitting tickerplants by instrument universe or message type\n\n\nReplaying from offsets is the same interface as live pub/sub and doesn\u2019t require\nfilesystem access to the tickerplant log, so RDB\u2019s and other consumer could be\non a different server\n\n\n\n\nBy default, the Kafka bindings will be loaded into all TorQ processes running on\nl64 systems (the only platform currently supported). An example of usage is\nshown here (this assumes a local running instance of kafka - instructions for\nthis are available on the \nkafkaq\n github \nrepo):\n\n\nq).kafka.initconsumer[`localhost:9092;()]\nq).kafka.initproducer[`localhost:9092;()]\nq)kupd / print default definition for incoming data - ignore key, print message\nas ascii\n{[k;x] -1 `char$x;}\nq).kafka.subscribe[`test;0] / subscribe to topic test, partition 0\nq)pub:{.kafka.publish[`test;0;`;`byte$x]} / define pub to publish text input to topic\ntest on partition 0 with no key defined\nq)pub\nhello world\n\nq)hello world\n\n\n\n\n\nLimitations of the current implementation:\n\n\n\n\nOnly l64 supported\n\n\nSingle consumer thread subscribed to one topic at a time\n\n\n\n\n\n\ntplogutils.q\n\n\ntplogutils.q contains functions for recovering tickerplant log files.\nUnder certain circumstances the tickerplant log file can become corrupt\nby having an invalid sequence of bytes written to it. A log file can be\nrecovered using a simple recovery method. However, this will only\nrecover messages up to the first invalid message. The recovery functions\ndefined in tplogutils.q allow all valid messages to be recovered from\nthe tickerplant log file.\n\n\n\n\nmonitoringchecks.q\n\n\nmonitoringchecks.q implements a set of standard, basic monitoring\nchecks. They include checks to ensure:\n\n\n\n\n\n\ntable sizes are increasing during live capture\n\n\n\n\n\n\nthe HDB data saves down correctly\n\n\n\n\n\n\nthe allocated memory of a process does not increase past a certain\n      size\n\n\n\n\n\n\nthe size of the symbol list in memory doesn\u2019t grow to big\n\n\n\n\n\n\nthe process does not have too much on its pending subscriber queue\n\n\n\n\n\n\nThese checks are intended to be run by the reporter process on a\nschedule, and any alerts emailed to an appropriate recipient list.\n\n\n\n\nheartbeat.q\n\n\nheartbeat.q implements heartbeating, and relies on both timer.q and\npubsub.q. A table called heartbeat will be published periodically,\nallowing downstream processes to detect the availability of upstream\ncomponents. The heartbeat table contains a heartbeat time and counter.\nThe heartbeat script contains functions to handle and process heartbeats\nand manage upstream process failures. See .api.p.hb.*for details.\n\n\nrmvr.q\n\n\nThis file contains a function which can be used to convert environment variable paths into a full path from the root directory.\n\n\nos.q\n\n\nA file with various q functions to perform system operations. This will detect your operating system and will perform the correct commands depending on what you are using.\n\n\nThis is a modification of a script developed by Simon Garland.\n\n\n\n\ndbwriteutils.q\n\n\nThis contains a set of utility functions for writing data to historic\ndatabases.\n\n\nSorting and Attributes\n\n\nThe sort utilities allow the sort order and attributes of tables to be\nglobally defined. This helps to manage the code base when the data can\npotentially be written from multiple locations (e.g. written from the\nRDB, loaded from flat file, replayed from the tickerplant log). The\nconfiguration is defined in a csv which defaults to $KDBCONFG/sort.csv.\nThe default setup is that every table is sorted by sym and time, with a\np attribute on sym (this is the standard kdb+ tick configuration).\n\n\naquaq$ tail config/sort.csv \ntabname,att,column,sort\ndefault,p,sym,1\ndefault,,time,1\n\n\n\nAs an example, assume we have an optiontrade table which we want to be\ndifferent from the standard set up. We would like the table to be sorted\nby optionticker and then time, with a p attribute on optionticker. We\nalso have a column called underlyingticker which we can put an attribute\non as it is derived from optionticker (so there is an element of\nde-normalisation present in the table). We also have an exchange field\nwhich we would like to put a g attribute on. All other tables we want to\nbe sorted and parted in the standard way. The configuration file would\nlook like this (sort order is derived from the order within the file\ncombined with the sort flag being set to true):\n\n\naquaq$ tail config/sort.csv                \ntabname,att,column,sort\ndefault,p,sym,1\ndefault,,time,1\noptiontrade,p,optionticker,1\noptiontrade,,exchtime,1\noptiontrade,p,underlyingticker,0\noptiontrade,g,exchange,0\n\n\n\nTo invoke the sort utilities, supply a list of (tablename; partitions)\ne.g.\n\n\nq).sort.sorttab(`trthtrade;`:hdb/2014.11.20/trthtrade`:hdb/2014.11.20/trthtrade)\n2014.12.03D09:56:19.214006000|aquaq|test|INF|sort|sorting the trthtrade table\n2014.12.03D09:56:19.214045000|aquaq|test|INF|sorttab|No sort parameters have been specified for : trthtrade. Using default parameters\n2014.12.03D09:56:19.214057000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.19/trthtrade/ by these columns : sym, time\n2014.12.03D09:56:19.219716000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.19/trthtrade/\n2014.12.03D09:56:19.220846000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.20/trthtrade/ by these columns : sym, time\n2014.12.03D09:56:19.226008000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.20/trthtrade/\n2014.12.03D09:56:19.226636000|aquaq|test|INF|sort|finished sorting the trthtrade table\n\n\n\nA different sort configuration file can be loaded with\n\n\n.sort.getsortcsv[`:file]\n\n\n\nGarbage Collection\n\n\nThe garbage collection utility prints some debug information before and\nafter the garbage collection.\n\n\nq).gc.run[]                                                                                                                                                      \n2014.12.03D10:22:51.688435000|aquaq|test|INF|garbagecollect|Starting garbage collect. mem stats: used=2 MB; heap=1984 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB\n2014.12.03D10:22:53.920656000|aquaq|test|INF|garbagecollect|Garbage collection returned 1472MB. mem stats: used=2 MB; heap=512 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB\n\n\n\nTable Manipulation\n\n\nThe table manipulation utilities allow table manipulation routines to be\ndefined in a single place. This is useful when data can be written from\nmutliple different processes e.g. RDB, WDB, or tickerplant log replay.\nInstead of having to create a separate definition of customised\nmanipulation in each process, it can be done in a single location and\ninvokved in each process.\n\n\n\n\nhelp.q\n\n\nThe standard help.q from code.kx provides help utilities in the console.\nThis should be kept up to date with\n[\ncode.kx\n].\n\n\nq)help`                                                                                                                                                                                                                         \nadverb    | adverbs/operators\nattributes| data attributes\ncmdline   | command line parameters\ndata      | data types\ndefine    | assign, define, control and debug\ndotz      | .z locale contents\nerrors    | error messages\nsave      | save/load tables\nsyscmd    | system commands\ntemporal  | temporal - date \n time casts\nverbs     | verbs/functions\n\n\n\n\n\nhtml.q\n\n\nAn HTML utility has been added to accompany the HTML5 front end for the\nMonitoring process. It includes functions to format dates, tables to csv\nto configure the HTML file to work on the correct process. It is\naccessible from the \n.html\n namespace.\n\n\n\n\neodtime.q\n\n\nThis script provides functionality for managing timezones. TorQ can be \nconfigured to timestamp data in a specific timezone, while also being\nconfigured to perform the end of day rollover in another timezone, at a\nconfigurable time.\n\n\nThese options are handled by three settings:\n\n\n\n\n\n\n\n\nSetting\n\n\nReq\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n.eodtime.rolltimeoffset\n\n\nY\n\n\ntimespan\n\n\nOffset from default midnight roll time\n\n\n\n\n\n\n.eodtime.rolltimezone\n\n\nY\n\n\nsymbol\n\n\nTime zone in which to rollover\n\n\n\n\n\n\n.eodtime.datatimezone\n\n\nY\n\n\nsymbol\n\n\nTime zone in which to timestamp data in TP\n\n\n\n\n\n\n\n\nThe default configuration sets both timezones to GMT and has the rollover\nperformed at midnight.\n\n\nA table containing the valid timezones is loaded into TorQ processes as .tz.t\n\n\nAn example configuration where data is stamped in GMT, but the rollover\noccurs at 5PM New York time would be:\n\n\n.eodtime.rolltimeoffset:-0D07:00:00.000000000; // 5 PM i.e. 7 hours before midnight\n.eodtime.rolltimezone:`$\"America/New_YorK\";    // roll in NYC time\n.eodtime.datatimezone:`$\"GMT\";                 // timestamp in GMT\n\n\n\nNote that the rolltimeoffset can be negative - this will cause the rollover to happen \n\"yesterday\", meaning that at the rolltime, the trading date will become the day \nafter\n\nthe calendar date. Where this is positive, the rollover occurs \"today\" and so the trading\ndate will become the current calendar date.\n\n\nsubscribercutoff.q\n\n\nThis script is used to provide functionality for cutting off any slow subscribers on any\nTorQ processes. The script will periodically check (time between checks set in .subcut.checkfreq.\nDefault is 1 minute) the byte size of the queue for all the handles on the process to see if\nthey have exceeded a set cut-off point (set in the variable .subcut.maxsize) and will only\ncut-off the handle if it exceeds this limit a set number of times in a row (default is 3\nand set in the .subcut.breachlimit variable). This gives clients a chance to tidy up their\nbehavior and will avoid cutting off clients if they happened to have a spike just before the\ncheck was performed. The .subcut.state variable is used to keep track of the handles and the \nnumber of times they have exceeded the size limit in a row. \n\n\nTo enable this functionality the .subcut.enabled flag must be set to true and \nthe timer.q script must be loaded on the desired processes. By default the chained \ntickerplant is the only processes with the functionality enabled. \n\n\ndatareplay.q\n\n\nThe datareplay utility provides functionality for generating tickerplant function calls from historcial\ndata which can be executed by subscriber functions. This can be used to test a known data-set against a \nsubscriber for testing or debugging purposes.\n\n\nIt can load this data from the current TorQ session, or from a remote hdb if given its connection handle.\n\n\nIt can also chunk the data by time increments (as if the tickerplant was in batch mode), and can also generate\ncalls to a custom timer function for the same time increments (defaults to .z.ts).\n\n\nThe functions provided by this utility are made available in the .datareplay namespace.\n\n\nThe utility is mainly used via the tabesToDataStreamFunction, which accepts a dictionary parameter with the following\nfields:\n\n\n\n\n\n\n\n\nKey\n\n\nExample Value\n\n\nDescription\n\n\nRequired\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\ntabs\n\n\n`trade`quote or `trade\n\n\nList of tables to include\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\nsts\n\n\n2014.04.04D07:00:00.000\n\n\nStart timestamp for data\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\nets\n\n\n2014.04.04D16:30:00.000\n\n\nEnd of timestamp for data\n\n\nYes\n\n\nN/A\n\n\n\n\n\n\nsyms\n\n\n`AAPL`IBM\n\n\nList of symbols to include\n\n\nNo\n\n\nAll syms\n\n\n\n\n\n\nwhere\n\n\n,(=;`src;,`L)\n\n\nCustom where clause in functional form\n\n\nNo\n\n\nnone\n\n\n\n\n\n\ntimer\n\n\n1b\n\n\nGenerate timer function flag\n\n\nNo\n\n\n0b\n\n\n\n\n\n\nh\n\n\n5i\n\n\nHandle to hdb process\n\n\nNo\n\n\n0i (self)\n\n\n\n\n\n\ninterval\n\n\n0D00:00:01.00\n\n\nTime interval used to chunk data, bucketed by timestamp if no time interval set\n\n\nNo\n\n\nNone\n\n\n\n\n\n\ntc\n\n\n`data_time\n\n\nName of time column to cut on\n\n\nNo\n\n\n`time\n\n\n\n\n\n\ntimerfunc\n\n\n.z.ts\n\n\nTimer function to use if `timer parameter is set\n\n\nNo\n\n\n.z.ts\n\n\n\n\n\n\n\n\nWhen the timer flag is set, the utility will interleave timer function calls in the message column at intervals based on the interval parameter, or every 10 seconds if interval is not set. This is useful if testing requires a call to a function at a set time, to generate a VWAP every 10 minutes for example. The function the timer messages call is based on the timerfunc parameter, or .z.ts if this parameter is not set.\n\n\nIf the interval is set the messages will be aggregated into chunks based on the interval value, if no interval is specified, the data will be bucketed by timestamp (one message chunk per distinct timestamp per table).\n\n\nIf no connection handle is specified (h parameter), the utility will retrieve the data from the process the utility is running on, using handle 0.\n\n\nThe where parameter allows for the use of a custom where clause when extracting data, which can be useful when the dataset is large and only certain data is required, for example if only data where \nsrc=`L\n is required. The where clause(s) are required to be in functional form, for example \nenlist (=;`src;,`L)\n or \n((=;`src;enlist `L);(\n;`size;100))\n (note, that if only one custom where clause is included it is required to be enlisted).\n\n\nIt is possible to get the functional form of a where clause by running parse on a mock select string like below:\n\n\nq)parse \"select from t where src=`L,size\n100\"\n?\n`t\n,((=;`src;,`L);(\n;`size;100))\n0b\n()\n\n\n\nThe where clause is then the 3rd item returned in the parse tree.\n\n\nExamples:\n\n\nExtract all data between sts and ets from the trades table in the current process.\n\n\nq)input\ntabs| `trades\nsts | 2014.04.21D07:00:00.000000000\nets | 2014.05.02D17:00:00.000000000\nq).datareplay.tablesToDataStream input\ntime                          msg                                            ..\n-----------------------------------------------------------------------------..\n2014.04.21D08:00:23.478000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n2014.04.21D08:00:49.511000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n2014.04.21D08:01:45.623000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n2014.04.21D08:02:41.346000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n..\nq)first .datareplay.tablesToDataStream input\ntime| 2014.04.21D08:00:23.478000000\nmsg | (`upd;`trades;`sym`time`src`price`size!(`YHOO;2014.04.21D08:00:23.47800..\n\n\n\nExtract all data between sts and ets from the trades table from a remote hdb handle=3i.\n\n\nq)input\ntabs| `trades\nsts | 2014.04.21D07:00:00.000000000\nets | 2014.05.02D17:00:00.000000000\nh   | 3i\nq).datareplay.tablesToDataStream input\ntime                          msg                                            ..\n-----------------------------------------------------------------------------..\n2014.04.21D08:00:07.769000000 `upd `trades `sym`time`src`price`size!(`IBM;201..\n2014.04.21D08:00:13.250000000 `upd `trades `sym`time`src`price`size!(`NOK;201..\n2014.04.21D08:00:19.070000000 `upd `trades `sym`time`src`price`size!(`MSFT;20..\n2014.04.21D08:00:23.678000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n..\nq)first .datareplay.tablesToDataStream input\ntime| 2014.04.21D08:00:07.769000000\nmsg | (`upd;`trades;`sym`time`src`price`size!(`IBM;2014.04.21D08:00:07.769000..\n\n\n\nSame as above but including quote table and with interval of 10 minutes:\n\n\nq)input\ntabs    | `quotes`trades\nsts     | 2014.04.21D07:00:00.000000000\nets     | 2014.05.02D17:00:00.000000000\nh       | 3i\ninterval| 0D00:10:00.000000000\nq).datareplay.tablesToDataStream input\ntime                          msg                                            ..\n-----------------------------------------------------------------------------..\n2014.04.21D08:09:47.600000000 `upd `trades +`sym`time`src`price`size!(`YHOO`A..\n2014.04.21D08:09:55.210000000 `upd `quotes +`sym`time`src`bid`ask`bsize`asize..\n2014.04.21D08:19:39.467000000 `upd `trades +`sym`time`src`price`size!(`CSCO`N..\n2014.04.21D08:19:49.068000000 `upd `quotes +`sym`time`src`bid`ask`bsize`asize..\n..\nq)first .datareplay.tablesToDataStream input\ntime| 2014.04.21D08:09:47.600000000\nmsg | (`upd;`trades;+`sym`time`src`price`size!(`YHOO`AAPL`MSFT`NOK`DELL`YHOO`..\n\n\n\nAll messages from trades where \nsrc=`L\n bucketed in 10 minute intervals interleaved with calls to the function \n`vwap\n.\n\n\nq)input\ntabs     | `trades\nh        | 3i\nsts      | 2014.04.21D08:00:00.000000000\nets      | 2014.05.02D17:00:00.000000000\nwhere    | ,(=;`src;,`L)\ntimer    | 1b\ntimerfunc| `vwap\ninterval | 0D00:10:00.000000000\nq).datareplay.tablesToDataStream input\ntime                          msg                                            ..\n-----------------------------------------------------------------------------..\n2014.04.21D08:00:00.000000000 (`vwap;2014.04.21D08:00:00.000000000)          ..\n2014.04.21D08:09:46.258000000 (`upd;`trades;+`sym`time`src`price`size!(`AAPL`..\n2014.04.21D08:10:00.000000000 (`vwap;2014.04.21D08:10:00.000000000)          ..\n2014.04.21D08:18:17.188000000 (`upd;`trades;+`sym`time`src`price`size!(`AAPL`..\n..\n\n\n\nModified u.q\n\n\nStarting in kdb+ v3.4, the new broadcast feature has some performance\nbenefits. It works by serialising a message once before sending it\nasynchronously to a list of subscribers whereas the previous method\nwould serialise it separately for each subscriber. To take advantage of\nthis, we\u2019ve modified u.q. This can be turned off by setting .u.broadcast\nto false. It is enabled by default, but will only override default\npublishing if the kdb+ version being used is 3.4 or after.\n\n\n\n\nFull API\n\n\nThe full public api can be found by running\n\n\nq).api.u`                                                                                                                                                                                                                       \nname             | vartype  namespace public descrip                 ..\n-----------------| --------------------------------------------------..\n.proc.createlog  | function .proc     1      \"Create the standard out..\n.proc.rolllogauto| function .proc     1      \"Roll the standard out/e..\n.proc.loadf      | function .proc     1      \"Load the specified file..\n.proc.loaddir    | function .proc     1      \"Load all the .q and .k ..\n.lg.o            | function .lg       1      \"Log to standard out\"   ..\n..\n\n\n\nCombined with the commented configuration file, this should give a good\noverview of the functionality available. A description of the individual\nnamespaces is below- run .api.u namespace*to list the functions.\n\n\n\n\n\n\n\n\nNamespace\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n.proc\n\n\nProcess API\n\n\n\n\n\n\n.lg\n\n\nStandard out/error logging API\n\n\n\n\n\n\n.err\n\n\nError throwing API\n\n\n\n\n\n\n.usage\n\n\nUsage logging API\n\n\n\n\n\n\n.access\n\n\nPermissions API\n\n\n\n\n\n\n.clients\n\n\nClient tracking API\n\n\n\n\n\n\n.servers\n\n\nServer tracking API\n\n\n\n\n\n\n.async\n\n\nAsync communication API\n\n\n\n\n\n\n.timer\n\n\nTimer API\n\n\n\n\n\n\n.cache\n\n\nCaching API\n\n\n\n\n\n\n.tz\n\n\nTimezone conversions API\n\n\n\n\n\n\n.checks\n\n\nMonitoring API\n\n\n\n\n\n\n.cmp\n\n\nCompression API\n\n\n\n\n\n\n.ps\n\n\nPublish and Subscribe API\n\n\n\n\n\n\n.hb\n\n\nHeartbeating API\n\n\n\n\n\n\n.loader\n\n\nData Loader API\n\n\n\n\n\n\n.sort\n\n\nData sorting and attribute setting API\n\n\n\n\n\n\n.sub\n\n\nSubscription API\n\n\n\n\n\n\n.gc\n\n\nGarbage Collection API\n\n\n\n\n\n\n.tplog\n\n\nTickerplant Log Replay API\n\n\n\n\n\n\n.api\n\n\nAPI management API\n\n\n\n\n\n\n\n\nAPI Table\n\n\n\n\n\n\n\n\nname\n\n\nvartype\n\n\nnamespace\n\n\ndescrip\n\n\nparams\n\n\nreturn\n\n\n\n\n\n\n\n\n\n\n.proc.createlog\n\n\nfunction\n\n\n.proc\n\n\nCreate the standard out and standard err log files. Redirect to them\n\n\n[string: log directory; string: name of the log file;mixed: timestamp suffix for the file (can be null); boolean: suppress the generation of an alias link]\n\n\nnull\n\n\n\n\n\n\n.proc.rolllogauto\n\n\nfunction\n\n\n.proc\n\n\nRoll the standard out/err log files\n\n\n[]\n\n\nnull\n\n\n\n\n\n\n.proc.loadf\n\n\nfunction\n\n\n.proc\n\n\nLoad the specified file\n\n\n[string: filename]\n\n\nnull\n\n\n\n\n\n\n.proc.loaddir\n\n\nfunction\n\n\n.proc\n\n\nLoad all the .q and .k files in the specified directory. If order.txt is found in the directory, use the ordering found in that file\n\n\n[string: name of directory]\n\n\nnull\n\n\n\n\n\n\n.proc.getattributes\n\n\nfunction\n\n\n.proc\n\n\nCalled by external processes to retrieve the attributes (advertised functionality) of this process\n\n\n[]\n\n\ndictionary of attributes\n\n\n\n\n\n\n.proc.override\n\n\nfunction\n\n\n.proc\n\n\nOverride configuration varibles with command line parameters.  For example, if you set -.servers.HOPENTIMEOUT 5000 on the command line and call this function, then the command line value will be used\n\n\n[]\n\n\nnull\n\n\n\n\n\n\n.proc.overrideconfig\n\n\nfunction\n\n\n.proc\n\n\nOverride configuration varibles with values in supplied parameter dictionary. Generic version of .proc.override\n\n\n[dictionary: command line parameters.  .proc.params should be used]\n\n\nnull\n\n\n\n\n\n\n.lg.o\n\n\nfunction\n\n\n.lg\n\n\nLog to standard out\n\n\n[symbol: id of log message; string: message]\n\n\nnull\n\n\n\n\n\n\n.lg.e\n\n\nfunction\n\n\n.lg\n\n\nLog to standard err\n\n\n[symbol: id of log message; string: message]\n\n\nnull\n\n\n\n\n\n\n.lg.l\n\n\nfunction\n\n\n.lg\n\n\nLog to either standard error or standard out, depending on the log level\n\n\n[symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters, used in the logging extension function]\n\n\nnull\n\n\n\n\n\n\n.lg.err\n\n\nfunction\n\n\n.lg\n\n\nLog to standard err\n\n\n[symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters, used in the logging extension function]\n\n\nnull\n\n\n\n\n\n\n.lg.ext\n\n\nfunction\n\n\n.lg\n\n\nExtra function invoked in standard logging function .lg.l.  Can be used to do more with the log message, e.g. publish externally\n\n\n[symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters]\n\n\nnull\n\n\n\n\n\n\n.err.ex\n\n\nfunction\n\n\n.err\n\n\nLog to standard err, exit\n\n\n[symbol: id of log message; string: message; int: exit code]\n\n\nnull\n\n\n\n\n\n\n.err.usage\n\n\nfunction\n\n\n.err\n\n\nThrow a usage error and exit\n\n\n[]\n\n\nnull\n\n\n\n\n\n\n.err.param\n\n\nfunction\n\n\n.err\n\n\nCheck a dictionary for a set of required parameters. Print an error and exit if not all required are supplied\n\n\n[dict: parameters; symbol list: the required param values]\n\n\nnull\n\n\n\n\n\n\n.err.env\n\n\nfunction\n\n\n.err\n\n\nCheck if a list of required environment variables are set.  If not, print an error and exit\n\n\n[symbol list: list of required environment variables]\n\n\nnull\n\n\n\n\n\n\n.usage.rolllogauto\n\n\nfunction\n\n\n.usage\n\n\nRoll the .usage txt files\n\n\n[]\n\n\nnull\n\n\n\n\n\n\n.usage.readlog\n\n\nfunction\n\n\n.usage\n\n\nRead and return a usage log file as a table\n\n\n[string: name of log file]\n\n\nnull\n\n\n\n\n\n\n.usage.logtodisk\n\n\nvariable\n\n\n.usage\n\n\nwhether to log to disk\n\n\n\n\n\n\n\n\n\n\n.usage.logtomemory\n\n\nvariable\n\n\n.usage\n\n\nwhether to log to .usage.usage\n\n\n\n\n\n\n\n\n\n\n.usage.ignore\n\n\nvariable\n\n\n.usage\n\n\nwhether to check the ignore list for functions to ignore\n\n\n\n\n\n\n\n\n\n\n.usage.ignorelist\n\n\nvariable\n\n\n.usage\n\n\nthe list of functions to ignore\n\n\n\n\n\n\n\n\n\n\n.usage.logroll\n\n\nvariable\n\n\n.usage\n\n\nwhether to automatically roll the log file\n\n\n\n\n\n\n\n\n\n\n.usage.usage\n\n\ntable\n\n\n.usage\n\n\nlog of messages through the message handlers\n\n\n\n\n\n\n\n\n\n\n.clients.clients\n\n\ntable\n\n\n.clients\n\n\ntable containing client handles and session values\n\n\n\n\n\n\n\n\n\n\n.sub.getsubscriptionhandles\n\n\nfunction\n\n\n.sub\n\n\nConnect to a list of processes of a specified type\n\n\n[symbol: process type to match; symbol: process name to match; dictionary:attributes of process]\n\n\ntable of process names, types and the handle connected on\n\n\n\n\n\n\n.sub.subscribe\n\n\nfunction\n\n\n.sub\n\n\nSubscribe to a table or list of tables and specified instruments\n\n\n[symbol (list):table names; symbol (list): instruments; boolean: whether to set the schema from the server; boolean: wether to replay the logfile; dictionary: procname,proctype,handle\n\n\n\n\n\n\n\n\n.pm.adduser\n\n\nfunction\n\n\n.pm\n\n\nAdds a user to be permissioned as well as setting their password and the method used to hash it.\n\n\n[symbol: the username; symbol: method used to authenticate; symbol: method used to hash the password; string: password, hashed using the proper method]\n\n\nnull\n\n\n\n\n\n\n.pm.addgroup\n\n\nfunction\n\n\n.pm\n\n\nAdd a group which will have access to certain tables and variables\n\n\n[symbol: the name of the group; string: a description of the group]\n\n\nnull\n\n\n\n\n\n\n.pm.addrole\n\n\nfunction\n\n\n.pm\n\n\nAdd a role which will have access to certain functions\n\n\n[symbol: the name of the role; string: a description of the role]\n\n\nnull\n\n\n\n\n\n\n.pm.addtogroup\n\n\nfunction\n\n\n.pm\n\n\nAdd a user to a group, giving them access to all of its variables\n\n\n[symbol: the name of the user to add; symbol: group the user is to be added to]\n\n\nnull\n\n\n\n\n\n\n.pm.assignrole\n\n\nfunction\n\n\n.pm\n\n\nAssign a user a role, giving them access to all of its functions\n\n\n[symbol: the name of the user to add; symbol: role the user is to be assigned to]\n\n\nnull\n\n\n\n\n\n\n.pm.grantaccess\n\n\nfunction\n\n\n.pm\n\n\nGive a group access to a variable\n\n\n[symbol: the name of the variable the group should get access to; symbol: group that is to be given this access; symbol: the type of access that should be given, eg. read, write]\n\n\nnull\n\n\n\n\n\n\n.pm.grantfunction\n\n\nfunction\n\n\n.pm\n\n\nGive a role access to a function\n\n\nsymbol: name of the function to be added; symbol: role that is to be given this access; TO CLARIFY\n\n\nnull\n\n\n\n\n\n\n.pm.createvirtualtable\n\n\nfunction\n\n\n.pm\n\n\nCreate a virtual table that a group might be able to access instead of the full table\n\n\n[symbol: new name of the table; symbol: name of the actual table t add; TO CLARIFY]\n\n\nnull\n\n\n\n\n\n\n.pm.cloneuser\n\n\nfunction\n\n\n.pm\n\n\nAdd a new user that is identical to another user\n\n\n[symbol: name of the new user; symbol: name of the user to be cloned; string: password of the new user]\n\n\nnull\n\n\n\n\n\n\n.access.addsuperuser\n\n\nfunction\n\n\n.access\n\n\nAdd a super user\n\n\n[symbol: user]\n\n\nnull\n\n\n\n\n\n\n.access.addpoweruser\n\n\nfunction\n\n\n.access\n\n\nAdd a power user\n\n\n[symbol: user]\n\n\nnull\n\n\n\n\n\n\n.access.adddefaultuser\n\n\nfunction\n\n\n.access\n\n\nAdd a default user\n\n\n[symbol: user]\n\n\nnull\n\n\n\n\n\n\n.access.readpermissions\n\n\nfunction\n\n\n.access\n\n\nRead the permissions from a directory\n\n\n[string: directory containing the permissions files]\n\n\nnull\n\n\n\n\n\n\n.access.USERS\n\n\ntable\n\n\n.access\n\n\nTable of users and their types\n\n\n\n\n\n\n\n\n\n\n.servers.opencon\n\n\nfunction\n\n\n.servers\n\n\nopen a connection to a process using the default timeout. If no user:pass supplied, the default one will be added if set\n\n\n[symbol: the host:port[:user:pass]]\n\n\nint: the process handle, null if the connection failed\n\n\n\n\n\n\n.servers.addh\n\n\nfunction\n\n\n.servers\n\n\nopen a connection to a server, store the connection details\n\n\n[symbol: the host:port:user:pass connection symbol]\n\n\nint: the server handle\n\n\n\n\n\n\n.servers.addw\n\n\nfunction\n\n\n.servers\n\n\nadd the connection details of a process behind the handle\n\n\n[int: server handle]\n\n\nnull\n\n\n\n\n\n\n.servers.addnthawc\n\n\nfunction\n\n\n.servers\n\n\nadd the details of a connection to the table\n\n\n[symbol: process name; symbol: process type; hpup: host:port:user:pass connection symbol; dict: attributes of the process; int: handle to the process;boolean: whether to check the handle is valid on insert\n\n\nint: the handle of the process\n\n\n\n\n\n\n.servers.getservers\n\n\nfunction\n\n\n.servers\n\n\nget a table of servers which match the given criteria\n\n\n[symbol: pick the server based on the name value or the type value.  Can be either `procname`proctype; symbol(list): lookup values. ` for any; dict: requirements dictionary; boolean: whether to automatically open dead connections for the specified lookup values; boolean: if only one of each of the specified lookup values is required (means dead connections aren't opened if there is one available)]\n\n\ntable: processes details and requirements matches\n\n\n\n\n\n\n.servers.gethandlebytype\n\n\nfunction\n\n\n.servers\n\n\nget a server handle for the supplied type\n\n\n[symbol: process type; symbol: selection criteria. One of `roundrobin`any`last]\n\n\nint: handle of server\n\n\n\n\n\n\n.servers.gethpbytype\n\n\nfunction\n\n\n.servers\n\n\nget a server hpup connection symbol for the supplied type\n\n\n[symbol: process type; symbol: selection criteria. One of `roundrobin`any`last]\n\n\nsymbol: h:p:u:p connection symbol of server\n\n\n\n\n\n\n.servers.startup\n\n\nfunction\n\n\n.servers\n\n\ninitialise all the connections.  Must processes should call this during initialisation\n\n\n[]\n\n\nnull\n\n\n\n\n\n\n.servers.refreshattributes\n\n\nfunction\n\n\n.servers\n\n\nrefresh the attributes registered with the discovery service.  Should be called whenever they change e.g. end of day for an HDB\n\n\n[]\n\n\nnull\n\n\n\n\n\n\n.servers.SERVERS\n\n\ntable\n\n\n.servers\n\n\ntable containing server handles and session values\n\n\n\n\n\n\n\n\n\n\n.timer.repeat\n\n\nfunction\n\n\n.timer\n\n\nAdd a repeating timer with default next schedule\n\n\n[timestamp: start time; timestamp: end time; timespan: period; mixedlist: (function and argument list); string: description string]\n\n\nnull\n\n\n\n\n\n\n.timer.once\n\n\nfunction\n\n\n.timer\n\n\nAdd a one-off timer to fire at a specific time\n\n\n[timestamp: execute time; mixedlist: (function and argument list); string: description string]\n\n\nnull\n\n\n\n\n\n\n.timer.remove\n\n\nfunction\n\n\n.timer\n\n\nDelete a row from the timer schedule\n\n\n[int: timer id to delete]\n\n\nnull\n\n\n\n\n\n\n.timer.removefunc\n\n\nfunction\n\n\n.timer\n\n\nDelete a specific function from the timer schedule\n\n\n[mixedlist: (function and argument list)]\n\n\nnull\n\n\n\n\n\n\n.timer.rep\n\n\nfunction\n\n\n.timer\n\n\nAdd a repeating timer - more flexibility than .timer.repeat\n\n\n[timestamp: execute time; mixedlist: (function and argument list); short: scheduling algorithm for next timer; string: description string; boolean: whether to check if this new function is already present on the schedule]\n\n\nnull\n\n\n\n\n\n\n.timer.one\n\n\nfunction\n\n\n.timer\n\n\nAdd a one-off timer to fire at a specific time - more flexibility than .timer.once\n\n\n[timestamp: execute time; mixedlist: (function and argument list); string: description string; boolean: whether to check if this new function is already present on the schedule]\n\n\nnull\n\n\n\n\n\n\n.timer.timer\n\n\ntable\n\n\n.timer\n\n\nThe table containing the timer information\n\n\n\n\n\n\n\n\n\n\n.cache.execute\n\n\nfunction\n\n\n.cache\n\n\nCheck the cache for a valid result set, return the results if found, execute the function, cache it and return if not\n\n\n[mixed: function or string to execute;timespan: maximum allowable age of cache item if found in cache]\n\n\nmixed: result of function\n\n\n\n\n\n\n.cache.getperf\n\n\nfunction\n\n\n.cache\n\n\nReturn the performance statistics of the cache\n\n\n[]\n\n\ntable: cache performance\n\n\n\n\n\n\n.cache.maxsize\n\n\nvariable\n\n\n.cache\n\n\nThe maximum size in MB of the cache. This is evaluated using -22!, so may be incorrect due to power of 2 memory allocation.  To be conservative and ensure it isn't exceeded, set max size to half of the actual max size that you want\n\n\n\n\n\n\n\n\n\n\n.cache.maxindividual\n\n\nvariable\n\n\n.cache\n\n\nThe maximum size in MB of an individual item in the cache. This is evaluated using -22!, so may be incorrect due to power of 2 memory allocation.  To be conservative and ensure it isn't exceeded, set max size to half of the actual max size that you want\n\n\n\n\n\n\n\n\n\n\n.tz.dg\n\n\nfunction\n\n\n.tz\n\n\ndefault from GMT. Convert a timestamp from GMT to the default timezone\n\n\n[timestamp (list): timestamps to convert]\n\n\ntimestamp atom or list\n\n\n\n\n\n\n.tz.lg\n\n\nfunction\n\n\n.tz\n\n\nlocal from GMT. Convert a timestamp from GMT to the specified local timezone\n\n\n[symbol (list): timezone ids;timestamp (list): timestamps to convert]\n\n\ntimestamp atom or list\n\n\n\n\n\n\n.tz.gd\n\n\nfunction\n\n\n.tz\n\n\nGMT from default. Convert a timestamp from the default timezone to GMT\n\n\n[timestamp (list): timestamps to convert]\n\n\ntimestamp atom or list\n\n\n\n\n\n\n.tz.gl\n\n\nfunction\n\n\n.tz\n\n\nGMT from local. Convert a timestamp from the specified local timezone to GMT\n\n\n[symbol (list): timezone ids; timestamp (list): timestamps to convert]\n\n\ntimestamp atom or list\n\n\n\n\n\n\n.tz.ttz\n\n\nfunction\n\n\n.tz\n\n\nConvert a timestamp from a specified timezone to a specified destination timezone\n\n\n[symbol (list): destination timezone ids; symbol (list): source timezone ids; timestamp (list): timestamps to convert]\n\n\ntimestamp atom or list\n\n\n\n\n\n\n.tz.default\n\n\nvariable\n\n\n.tz\n\n\nDefault timezone\n\n\n\n\n\n\n\n\n\n\n.tz.t\n\n\ntable\n\n\n.tz\n\n\nTable of timestamp information\n\n\n\n\n\n\n\n\n\n\n.email.connectdefault\n\n\nfunction\n\n\n.email\n\n\nconnect to the default mail server specified in configuration\n\n\n[]\n\n\n\n\n\n\n\n\n.email.senddefault\n\n\nfunction\n\n\n.email\n\n\nconnect to email server if not connected. Send email using default settings\n\n\n[dictionary of email parameters. Required dictionary keys are to (symbol (list) of email address to send to), subject (character list), body (list of character arrays).  Optional parameters are cc (symbol(list) of addresses to cc), bodyType (can be `html, default is `text), attachment (symbol (list) of files to attach), image (symbol of image to append to bottom of email. `none is no image), debug (int flag for debug level of connection library. 0i=no info, 1i=normal. 2i=verbose)]\n\n\nsize in bytes of sent email. -1 if failure\n\n\n\n\n\n\n.email.test\n\n\nfunction\n\n\n.email\n\n\nsend a test email\n\n\n[symbol(list):email address to send test email to]\n\n\nsize in bytes of sent email. -1 if failure\n\n\n\n\n\n\n.hb.addprocs\n\n\nfunction\n\n\n.hb\n\n\nAdd a set of process types and names to the heartbeat table to actively monitor for heartbeats.  Processes will be automatically added and monitored when the heartbeats are subscribed to, but this is to allow for the case where a process might already be dead and so can't be subscribed to\n\n\n[symbol(list): process types; symbol(list): process names]\n\n\n\n\n\n\n\n\n.hb.processwarning\n\n\nfunction\n\n\n.hb\n\n\nCallback invoked if any process goes into a warning state.  Default implementation is to do nothing - modify as required\n\n\n[table: processes currently in warning state]\n\n\n\n\n\n\n\n\n.hb.processerror\n\n\nfunction\n\n\n.hb\n\n\nCallback invoked if any process goes into an error state. Default implementation is to do nothing - modify as required\n\n\n[table: processes currently in error state]\n\n\n\n\n\n\n\n\n.hb.storeheartbeat\n\n\nfunction\n\n\n.hb\n\n\nStore a heartbeat update.  This function should be added to you update callback when a heartbeat is received\n\n\n[table: the heartbeat table data to store]\n\n\n\n\n\n\n\n\n.hb.warningperiod\n\n\nfunction\n\n\n.hb\n\n\nReturn the warning period for a particular process type.  Default is to return warningtolerance * publishinterval. Can be overridden as required\n\n\n[symbollist: the process types to return the warning period for]\n\n\ntimespan list of warning period\n\n\n\n\n\n\n.hb.errorperiod\n\n\nfunction\n\n\n.hb\n\n\nReturn the error period for a particular process type.  Default is to return errortolerance * publishinterval. Can be overridden as required\n\n\n[symbollist: the process types to return the error period for]\n\n\ntimespan list of error period\n\n\n\n\n\n\n.rdb.moveandclear\n\n\nfunction\n\n\n.rdb\n\n\nMove a variable (table) from one namespace to another, deleting its contents.  Useful during the end-of-day roll down for tables you do not want to save to the HDB\n\n\n[symbol: the namespace to move the table from; symbol:the namespace to move the variable to; symbol: the name of the variable]\n\n\nnull\n\n\n\n\n\n\n.api.f\n\n\nfunction\n\n\n.api\n\n\nFind a function/variable/table/view in the current process\n\n\n[string:search string]\n\n\ntable of matching elements\n\n\n\n\n\n\n.api.p\n\n\nfunction\n\n\n.api\n\n\nFind a public function/variable/table/view in the current process\n\n\n[string:search string]\n\n\ntable of matching public elements\n\n\n\n\n\n\n.api.u\n\n\nfunction\n\n\n.api\n\n\nFind a non-standard q public function/variable/table/view in the current process.  This excludes the .q, .Q, .h, .o namespaces\n\n\n[string:search string]\n\n\ntable of matching public elements\n\n\n\n\n\n\n.api.s\n\n\nfunction\n\n\n.api\n\n\nSearch all function definitions for a specific string\n\n\n[string: search string]\n\n\ntable of matching functions and definitions\n\n\n\n\n\n\n.api.find\n\n\nfunction\n\n\n.api\n\n\nGeneric method for finding functions/variables/tables/views. f,p and u are based on this\n\n\n[string: search string; boolean (list): public flags to include; boolean: whether the search is context senstive\n\n\ntable of matching elements\n\n\n\n\n\n\n.api.search\n\n\nfunction\n\n\n.api\n\n\nGeneric method for searching all function definitions for a specific string. s is based on this\n\n\n[string: search string; boolean: whether the search is context senstive\n\n\ntable of matching functions and definitions\n\n\n\n\n\n\n.api.add\n\n\nfunction\n\n\n.api\n\n\nAdd a function to the api description table\n\n\n[symbol:the name of the function; boolean:whether it should be called externally; string:the description; dict or string:the parameters for the function;string: what the function returns]\n\n\nnull\n\n\n\n\n\n\n.api.fullapi\n\n\nfunction\n\n\n.api\n\n\nReturn the full function api table\n\n\n[]\n\n\napi table\n\n\n\n\n\n\n.api.m\n\n\nfunction\n\n\n.api\n\n\nReturn the ordered approximate memory usage of each variable and view in the process. Views will be re-evaluated if required\n\n\n[]\n\n\nmemory usage table\n\n\n\n\n\n\n.api.mem\n\n\nfunction\n\n\n.api\n\n\nReturn the ordered approximate memory usage of each variable and view in the process. Views are only returned if view flag is set to true. Views will be re-evaluated if required\n\n\n[boolean:return views]\n\n\nmemory usage table\n\n\n\n\n\n\n.api.whereami\n\n\nfunction\n\n\n.api\n\n\nGet the name of a supplied function definition. Can be used in the debugger e.g. .api.whereami[.z.s]\n\n\nfunction definition\n\n\nsymbol: the name of the current function\n\n\n\n\n\n\n.ps.publish\n\n\nfunction\n\n\n.ps\n\n\nPublish a table of data\n\n\n[symbol: name of table; table: table of data]\n\n\n\n\n\n\n\n\n.ps.subscribe\n\n\nfunction\n\n\n.ps\n\n\nSubscribe to a table and list of instruments\n\n\n[symbol(list): table name. ` for all; symbol(list): symbols to subscribe to. ` for all]\n\n\nmixed type list of table names and schemas\n\n\n\n\n\n\n.ps.initialise\n\n\nfunction\n\n\n.ps\n\n\nInitialise the pubsub routines.  Any tables that exist in the top level can be published\n\n\n[]\n\n\n\n\n\n\n\n\n.async.deferred\n\n\nfunction\n\n\n.async\n\n\nUse async messaging to simulate sync communication\n\n\n[int(list): handles to query; query]\n\n\n(boolean list:success status; result list)\n\n\n\n\n\n\n.async.postback\n\n\nfunction\n\n\n.async\n\n\nSend an async message to a process and the results will be posted back within the postback function call\n\n\n[int(list): handles to query; query; postback function]\n\n\nboolean list: successful send status\n\n\n\n\n\n\n.cmp.showcomp\n\n\nfunction\n\n\n.cmp\n\n\nShow which files will be compressed and how; driven from csv file\n\n\n[`:/path/to/database; `:/path/to/configcsv; maxagefilestocompress]\n\n\ntable of files to be compressed\n\n\n\n\n\n\n.cmp.compressmaxage\n\n\nfunction\n\n\n.cmp\n\n\nRun compression on files using parameters specified in configuration csv file, and specifying the maximum age of files to compress\n\n\n[`:/path/to/database; `:/path/to/configcsv; maxagefilestocompress]\n\n\n\n\n\n\n\n\n.cmp.docompression\n\n\nfunction\n\n\n.cmp\n\n\nRun compression on files using parameters specified in configuration csv file\n\n\n[`:/path/to/database; `:/path/to/configcsv]\n\n\n\n\n\n\n\n\n.loader.loadallfiles\n\n\nfunction\n\n\n.loader\n\n\nGeneric loader function to read a directory of files in chunks and write them out to disk\n\n\n[dictionary of load parameters. Should have keys of headers (symbol list), types (character list), separator (character), tablename (symbol), dbdir (symbol).  Optional params of dataprocessfunc (diadic function), datecol (name of column to extract date from: symbol), chunksize (amount of data to read at once:int), compression (compression parameters to use e.g. 16 1 0:int list), gc (boolean flag of whether to run garbage collection:boolean); directory containing files to load (symbol)]\n\n\n\n\n\n\n\n\n.sort.sorttab\n\n\nfunction\n\n\n.sort\n\n\nSort and set the attributes for a table and set of partitions based on a configuration file (default is $KDBCONFIG/sort.csv)\n\n\n[2 item list of (tablename e.g. `trade; partitions to sort and apply attributes to e.g. `:/hdb/2000.01.01/trade`:hdb/2000.01.02/trade)]\n\n\n\n\n\n\n\n\n.sort.getsortcsv\n\n\nfunction\n\n\n.sort\n\n\nRead in the sort csv from the specified location\n\n\n[symbol: the location of the file e.g. `:config/sort.csv]\n\n\n\n\n\n\n\n\n.gc.run\n\n\nfunction\n\n\n.gc\n\n\nRun garbage collection, print debug info before and after\n\n\n\n\n\n\n\n\n\n\n.mem.objsize\n\n\nfunction\n\n\n.mem\n\n\nReturns the calculated memory size in bytes used by an object.  It may take a little bit of time for objects with lots of nested structures (e.g. lots of nested columns)\n\n\n[q object]\n\n\nsize of the object in bytes\n\n\n\n\n\n\n.tplog.check\n\n\nfunction\n\n\n.tplog\n\n\nChecks if tickerplant log can be replayed.  If it can or can replay the first X messages, then returns the log handle, else it will read log as byte stream and create a good log and then return the good log handle\n\n\n[logfile (symbol), handle to the log file to check; lastmsgtoreplay (long), the index of the last message to be replayed from log ]\n\n\nhandle to log file, will be either the input log handle or handle to repaired log, depends on whether the log was corrupt", 
            "title": "Utilities"
        }, 
        {
            "location": "/utilities/#utilities", 
            "text": "We have provided several utility scripts, which either implement\ndeveloper aids or standard operations which are useful across processes.", 
            "title": "Utilities"
        }, 
        {
            "location": "/utilities/#apiq", 
            "text": "This provides a mechanism for documenting and publishing\nfunction/variable/table or view definitions within the kdb+ process. It\nprovides a search facility both by name and definition (in the case of\nfunctions). There is also a function for returning the approximate\nmemory usage of each variable in the process in descending order.  Definitions are added using the .api.add function. A variable can be\nmarked as public or private, and given a description, parameter list and\nreturn type. The search functions will return all the values found which\nmatch the pattern irrespective of them having a pre-defined definition.  Whether a value is public or private is defined in the definitions\ntable. If not found then by default all values are private, except those\nwhich live in the .q or top level namespace.  .api.f is used to find a function, variable, table or view based on a\ncase-insensitive pattern search. If a symbol parameter is supplied, a\nwildcard search of *[suppliedvalue]* is done. If a string is\nsupplied, the value is used as is, meaning other non-wildcard regex\npattern matching can be done.  \n    q).api.f`max                                                                                                                                                                                                                    \n    name                | vartype   namespace public descrip             ..\n    --------------------| -----------------------------------------------..\n    maxs                | function  .q        1                         ..\n    mmax                | function  .q        1                         ..\n    .clients.MAXIDLE    | variable  .clients  0                         ..\n    .access.MAXSIZE     | variable  .access   0                         ..\n    .cache.maxsize      | variable  .cache    1       The maximum size in..\n    .cache.maxindividual| variable  .cache    1       The maximum size in..\n    max                 | primitive           1                         ..\n    q).api.f max*                                                                                                                                                                                                                   \n    name| vartype   namespace public descrip params return\n    ----| ------------------------------------------------\n    maxs| function  .q        1                        \n    max | primitive           1                          .api.p is the same as .api.f, but only returns public functions. .api.u\nis as .api.p, but only includes user defined values i.e. it excludes q\nprimitives and values found in the .q, .Q, .h and .o namespaces.\n.api.find is a more general version of .api.f which can be used to do\ncase sensitive searches.  .api.s is used to search function definitions for specific values.  q).api.s\"*max*\"                                                                                                                                                                                                                 \nfunction            definition                                       ..\n---------------------------------------------------------------------..\n.Q.w                \"k){`used`heap`peak`wmax`mmap`mphy`syms`symw!(.\\\"..\n.clients.cleanup    \"{if[count w0:exec w from`.clients.clients where ..\n.access.validsize   \"{[x;y;z] $[superuser .z.u;x;MAXSIZE s:-22!x;x;'\\..\n.servers.getservers \"{[nameortype;lookups;req;autoopen;onlyone]\\n r:$..\n.cache.add          \"{[function;id;status]\\n \\n res:value function;\\n..  .api.m is used to return the approximate memory usage of variables and\nviews in the process, retrieved using -22!. Views will be re-evaluated\nif required. Use .api.mem[0b] if you do not want to evaluate and\nreturn views.  q).api.m[]                                                                                                                                                                                                                      \nvariable          size    sizeMB\n--------------------------------\n.tz.t             1587359 2     \n.help.TXT         15409   0     \n.api.detail       10678   0     \n.proc.usage       3610    0     \n.proc.configusage 1029    0     \n..  .api.whereami[lambda] can be used to retrieve the name of a function\ngiven its definition. This can be useful in debugging.  q)g:{x+y}                                                                                                                                                                                                                                                                     \nq)f:{20 + g[x;10]}                                                                                                                                                                                                                                                            \nq)f[10]                                                                                                                                                                                                                                                                       \n40\nq)f[`a]                                                                                                                                                                                                                                                                       \n{x+y}\n`type\n+\n`a\n10\nq)).api.whereami[.z.s]                                                                                                                                                                                                                                                        \n`..g", 
            "title": "api.q"
        }, 
        {
            "location": "/utilities/#apidetailsq", 
            "text": "This file in both the common and the handler directories is used to add to the api using the functions defined in api.q", 
            "title": "apidetails.q"
        }, 
        {
            "location": "/utilities/#timerq", 
            "text": "kdb+ provides a single timer function, .z.ts which is triggered with the\nfrequency specified by -t. We have provided an extension to allow\nmultiple functions to be added to the timer and fired when required. The\nbasic concept is that timer functions are registered in a table, with\n.z.ts periodically checking the table and running whichever functions\nare required. This is not a suitable mechanism where very high frequency\ntimers are required (e.g. sub 500ms).  There are two ways a function can be added to a timer- either as a\nrepeating timer, or to fire at a specific time. When a repeating timer\nis specified, there are three options as to how the timer can be\nrescheduled. Assuming that a timer function with period P is scheduled\nto fire at time T0, actually fires at time T1 and finishes at time T2,\nthen    mode 0 will reschedule for T0+P;    mode 1 will reschedule for T1+P;    mode 2 will reschedule for T2+P.    Both mode 0 and mode 1 have the potential for causing the timer to back\nup if the finish time T2 is after the next schedule time. See\n.api.p\u201c.timer.*\u201dfor more details.", 
            "title": "timer.q"
        }, 
        {
            "location": "/utilities/#asyncq", 
            "text": "kdb+ processes can communicate with each using either synchronous or\nasynchronous calls. Synchronous calls expect a response and so the\nserver must process the request when it is received to generate the\nresult and return it to the waiting client. Asynchronous calls do not\nexpect a response so allow for greater flexibility. The effect of\nsynchronous calls can be replicated with asynchronous calls in one of\ntwo ways (further details in section\u00a0gateway):    deferred synchronous: the client sends an async request, then blocks\n    on the handle waiting for the result. This allows the server more\n    flexibility as to how and when the query is processed;    asynchronous postback: the client sends an async request which is\n      wrapped in a function to be posted back to the client when the\n      result is ready. This allows the server flexibility as to how and\n      when the query is processed, and allows the client to continue\n      processing while the server is generating the result.    The code for both of these can get a little tricky, largely due to the\namount of error trapping required. We have provided two functions to\nallow these methods to be used more easily. .async.deferred takes a list\nof handles and a query, and will return a two item list of\n(success;results).  q).async.deferred[3 5;({system\"sleep 1\";system\"p\"};())]                                                                                                                                                                                     \n1    1   \n9995 9996\nq).async.deferred[3 5;({x+y};1;2)]                                                                                                                                                                                                          \n1 1\n3 3\nq).async.deferred[3 5;({x+y};1;`a)]                                                                                                                                                                                                         \n0                         0                        \n\"error: server fail:type\" \"error: server fail:type\"\nq).async.deferred[3 5 87;({system\"sleep 1\";system\"p\"};())]                                                                                                                                                                                  \n1     1     0                                       \n9995i 9996i \"error: comm fail: failed to send query\"  .async.postback takes a list of handles, a query, and the name or lambda\nof the postback function to return the result to. It will immediately\nreturn a success vector, and the results will be posted back to the\nclient when ready.  q).async.postback[3 5;({system\"sleep 1\";system\"p\"};());`showresult]                                                                                                                                                                         \n11b\nq)                                                                                                                                                                                                                                          \nq)9995i\n9996i\n\nq).async.postback[3 5;({x+y};1;2);`showresult]                                                                                                                                                                                              \n11b\nq)3\n3\n\nq).async.postback[3 5;({x+y};1;`a);`showresult]                                                                                                                                                                                             \n11b\nq)\"error: server fail:type\"\n\"error: server fail:type\"\n\nq).async.postback[3 5;({x+y};1;`a);showresult]                                                                                                                                                                                              \n11b\nq)\"error: server fail:type\"\n\"error: server fail:type\"\n\nq).async.postback[3 5 87;({x+y};1;2);showresult]                                                                                                                                                                                            \n110b\nq)3\n3  For more details, see .api.p\u201c.async.*\u201d.", 
            "title": "async.q"
        }, 
        {
            "location": "/utilities/#cacheq", 
            "text": "cache.q provides a mechanism for storing function results in a cache and\nreturning them from the cache if they are available and non stale. This\ncan greatly boost performance for frequently run queries.  The result set cache resides in memory and as such takes up space. It is\nup to the programmer to determine which functions are suitable for\ncaching. Likely candidates are those where some or all of the following\nconditions hold:    the function is run multiple times with the same parameters (perhaps\n    different clients all want the same result set);    the result set changes infrequently or the clients can accept\n      slightly out-of-date values;    the result set is not too large and/or is relatively expensive to\n      produce. For example, it does not make sense to cache raw data\n      extracts.    The cache has a maximum size and a minimum size for any individual\nresult set, both of which are defined in the configuration file. Size\nchecks are done with -22! which will give an approximation (but\nunderestimate) of the result set size. In the worst case the estimate\ncould be half the size of the actual size.  If a new result set is to be cached, the size is checked. Assuming it\ndoes not exceed the maximum individual size then it is placed in the\ncache. If the new cache size would exceed the maximum allowed space,\nother result sets are evicted from the cache. The current eviction\npolicy is to remove the least recently accessed result sets until the\nrequired space is freed. The cache performance is tracked in a table.\nCache adds, hits, fails, reruns and evictions are monitored.  The main function to use the cache is .cache.execute[function;\nstaletime]. If the function has been executed within the last\nstaletime, then the result is returned from the cache. Otherwise the\nfunction is executed and placed in the cache.  The function is run and the result placed in the cache:  q)\\t r:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01]                                                                                                                                                                     \n2023\nq)r                                                                                                                                                                                                                             \n3  The second time round, the result set is returned immediately from the\ncache as we are within the staletime value:  q)\\t r1:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:01]                                                                                                                                                                    \n0\nq)r1                                                                                                                                                                                                                            \n3  If the time since the last execution is greater than the required stale\ntime, the function is re-run, the cached result is updated, and the\nresult returned:  q)\\t r2:.cache.execute[({system\"sleep 2\"; x+y};1;2);0D00:00]                                                                                                                                                                    \n2008\nq)r2                                                                                                                                                                                                                            \n3  The cache performance is tracked:  q).cache.getperf[]                                                                                                                                                                                                              \ntime                          id status function                  \n------------------------------------------------------------------\n2013.11.06D12:41:53.103508000 2  add    {system\"sleep 2\"; x+y} 1 2\n2013.11.06D12:42:01.647731000 2  hit    {system\"sleep 2\"; x+y} 1 2\n2013.11.06D12:42:53.930404000 2  rerun  {system\"sleep 2\"; x+y} 1 2  See .api.p.cache.*for more details.", 
            "title": "cache.q"
        }, 
        {
            "location": "/utilities/#emailq", 
            "text": "A library file is provided to allow TorQ processes to send emails using\nan SMTP server. This is a wrapper around the standard libcurl library.\nThe library file is currently available for Windows (32 bit), Linux (32\nand 64 bit) and OSX (32 and 64 bit). The associated q script contains\ntwo main methods for creating a connection and sending emails. The email\nlibrary requires a modification to the path to find the required libs -\nsee the top of email.q for details.  The main connection method .email.connect takes a single dictionary\nparameter and returns 0i for success and -1i for failure.     Parameter  Req  Type  Description      url  Y  symbol  URL of mail server e.g. smtp://mail.example.com    user  Y  symbol  Username of user to login as    password  Y  symbol  Password for user    usessl  N  boolean  Connect using SSL/TLS, defaults to false    from  N  symbol  Email from field, defaults to torq@aquaq.co.uk    debug  N  integer  Debug level. 0=no output, 1=normal output, 2=verbose output. Default is 1     An example is:  q).email.connect[`url`user`password`from`usessl`debug!(`$\"smtp://mail.example.com:80\";`$\"torquser@aquaq.co.uk\";`hello;`$\"torquser@aquaq.co.uk\";0b;1i)]\n02 Jan 2015 11:45:19   emailConnect: url is set to smtp://mail.example.com:80\n02 Jan 2015 11:45:19   emailConnect: user is set to torquser@aquaq.co.uk\n02 Jan 2015 11:45:19   emailConnect: password is set\n02 Jan 2015 11:45:19   emailConnect: from is set torquser@aquaq.co.uk\n02 Jan 2015 11:45:19   emailConnect: trying to connect\n02 Jan 2015 11:45:19   emailConnect: connected, socket is 5\n0i  The email sending function .email.send takes a single dictionary\nparameter containing the details of the email to send. A connection must\nbe established before an email can be sent. The send function returns an\ninteger of the email length on success, or -1 on failure.     Parameter  Req  Type  Description      to  Y  symbol (list)  addresses to send to    subject  Y  char list  email subject    body  Y  list of char lists  email body    cc  N  symbol (list)  cc list    bodyType  N  symbol  type of email body. Can be `text or `html. Default is `text    debug  N  integer  Debug level. 0=no output, 1=normal output,2=verbose output. Default is 1     An example is:  q).email.send[`to`subject`body`debug!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\");1i)]\n02 Jan 2015 12:39:29   sending email with subject: test email\n02 Jan 2015 12:39:29   email size in bytes is 16682\n02 Jan 2015 12:39:30   emailSend: email sent\n16682i  Note that if emails are sent infrequently the library must re-establish\nthe connection to the mail server (this will be done automatically after\nthe initial connection). In some circumstances it may be better to batch\nemails together to send, or to offload email sending to separate\nprocesses as communication with the SMTP server can take a little time.  Two further functions are available, .email.connectdefault and\n.email.senddefault. These are as above but will use the default\nconfiguration defined within the configuration files as the relevant\nparameters passed to the methods. In addition, .email.senddefault will\nautomatically establish a connection.  q).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email\";(\"hi\";\"this is an email from torq\"))]\n2015.01.02D12:43:34.646336000|aquaq||discovery1|INF|email|sending email\n2015.01.02D12:43:35.743887000|aquaq||discovery1|INF|email|connection to mail server successful\n2015.01.02D12:43:37.250427000|aquaq|discovery1|INF|email|email sent\n16673i\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\"))]\n2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email\n2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent\n16675i\nq).email.senddefault[`to`subject`body!(`$\"test@aquaq.co.uk\";\"test email 2\";(\"hi\";\"this is an email from torq\");`\"$/home/ashortt/example.txt\")]\n2015.01.02D12:43:48.115403000|aquaq|discovery1|INF|email|sending email\n2015.01.02D12:43:49.385807000|aquaq|discovery1|INF|email|email sent\n47338i  .email.test will attempt to establish a connection to the default\nconfigured email server and send a test email to the specified address.\ndebug should be set to 2i (verbose) to extract the full information.  q).email.debug:2i\nq).email.test `$\"test@aquaq.co.uk\"\n...  Additionally functions are available within the email library. See\n.api.p.email.*for more details.", 
            "title": "email.q"
        }, 
        {
            "location": "/utilities/#emails-with-ssl-certificates-from-windows", 
            "text": "If you wish to send emails via an account which requires authentication\nfrom Windows (e.g. Hotmail, Gmail) then you have to do a few extra steps\nas usessl must be true and Windows does not usually find the correct\ncertificate. The steps are:    download\n     this \n    and save it to your PC    set    CURLOPT_CAINFO=c:/path/to/cabundle_file/ca-bundle.crt    More information is available here \nand  here", 
            "title": "Emails with SSL certificates from Windows"
        }, 
        {
            "location": "/utilities/#timezoneq", 
            "text": "A slightly customised version of the timezone conversion functionality\nfrom code.kx. It loads a table of timezone information from\n$KDBCONFIG. See .api.p.tz.*for more details.", 
            "title": "timezone.q"
        }, 
        {
            "location": "/utilities/#compressq", 
            "text": "compress.q applies compression to any kdb+ database, handles all\npartition types including date, month, year, int, and can deal with top\nlevel splayed tables. It will also decompress files as required. Once\nthe compression/decompression is complete, summary statistics are\nreturned, with detailed statistics for each compressed or decompressed\nfile held in a table.  The utility is driven by the configuration specified within a csv file.\nDefault parameters can be given, and these can be used to compress all\nfiles within the database. However, the compress.q utility also provides\nthe flexibility to compress different tables with different compression\nparameters, and different columns within tables using different\nparameters. A function is provided which will return a table showing\neach file in the database to be compressed, and how, before the\ncompression is performed.  Compression is performed using the -19! operator, which takes 3\nparameters; the compression algorithm to use (0 - none, 1 - kdb+ IPC, 2\n- gzip), the compression blocksize as a power of 2 (between 12 and 19),\n  and the level of compression to apply (from 0 - 9, applicable only for\n  gzip). (For further information on -19! and the parameters used, see\n  code.kx.com.)  The compressionconfig.csv file should have the following format:  table,minage,column,calgo,cblocksize,clevel\ndefault,20,default,2,17,6\ntrades,20,default,1,17,0\nquotes,20,asize,2,17,7\nquotes,20,bsize,2,17,7  This file can be placed in the config folder, or a path to the file\ngiven at run time.  The compression utility compresses all tables and columns present in the\nHDB but not specified in the driver file according the default\nparameters. In effect, to compress an entire HDB using the same\ncompression parameters, a single row with name default would suffice. To\nspecify that a particular table should be compressed in a certain\ndifferent manner, it should be listed in the table. If default is given\nas the column for this table, then all of the columns of that table will\nbe compressed accordingly. To specify the compression parameters for\nparticular columns, these should be listed individually. For example,\nthe file above will compress trades tables 20 days old or more with an\nalgorithm of 1, and a blocksize of 17. The asize and bsize columns of\nany quotes tables older than 20 days old will be compressed using\nalgorithm 2, blocksize 17 and level 7. All other files present will be\ncompressed according to the default, using an algorithm 2, blocksize 17\nand compression level 6. To leave files uncompressed, you must specify\nthem explicitly in the table with a calgo of 0. If the file is already\ncompressed, note that an algorithm of 0 will decompress the file.  This utility should be used with caution. Before running the compression\nit is recommended to run the function .cmp.showcomp, which takes three\nparameters - the path to the database, the path to the csv file, and the\nmaximum age of the files to be compressed:  .cmp.showcomp[`:/full/path/to/HDB;.cmp.inputcsv;maxage]   \n        /- for using the csv file in the config folder\n.cmp.showcomp[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage]    \n        /- to specify a file  This function produces a table of the files to be compressed, the\nparameters with which they will be compressed, and the current size of\nthe file. Note that the current size column is calculated using hcount;\non a file which is already compressed this returns the uncompressed\nlength, i.e. this cannot be used as a signal as to whether the file is\ncompressed already.  fullpath                        column table  partition  age calgo cblocksize clevel compressage currentsize\n-------------------------------------------------------------------------------------\n:/home/hdb/2013.11.05/depth/asize1 asize1 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/asize2 asize2 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/asize3 asize3 depth  2013.11.05 146 0     17         8      1           787960\n:/home/hdb/2013.11.05/depth/ask1   ask1   depth  2013.11.05 146 0     17         8      1           1575904\n....  To then run the compression function, use .cmp.compressmaxage with the\nsame parameters as .cmp.showcomp (hdb path, csv path, maximum age of\nfiles):  .cmp.compressmaxage[`:/full/path/to/HDB;.cmp.inputcsv;maxage]   \n        /- for using the csv file in the config folder\n.cmp.compressmaxage[`:/full/path/to/HDB;`:/full/path/to/csvfile;maxage]    \n        /- to specify a file  To run compression on all files in the database disregarding the maximum\nage of the files (i.e. from minage as specified in the configuration\nfile to infinitely old), then use:  .cmp.docompression[`:/full/path/to/HDB;.cmp.inputcsv]   \n        /- for using the csv file in the config folder\n.cmp.docompression[`:/full/path/to/HDB;`:/full/path/to/csvfile]    \n        /- to specify a file  Logs are produced for each file which is compressed or decompressed.\nOnce the utility is complete, the statistics of the compression are also\nlogged. This includes the memory savings in MB from compression, the\nadditional memory usage in MB for decompression, the total compression\nratio, and the total decompression ratio:  |comp1|INF|compression|Memory savings from compression: 34.48MB. Total compression ratio: 2.51.\n|comp1|INF|compression|Additional memory used from de-compression: 0.00MB. Total de-compression ratio: .\n|comp1|INF|compression|Check .cmp.statstab for info on each file.  A table with the compressed and decompressed length for each individual\nfile, in descending order of compression ratio, is also produced. This\ncan be found in .cmp.statstab:  file                    algo compressedLength uncompressedLength compressionratio\n-----------------------------------------------------------------------------------\n:/hdb/2014.03.05/depth/asize1 2    89057            772600             8.675343\n:/hdb/2014.01.06/depth/asize1 2    114930           995532             8.662073\n:/hdb/2014.03.05/depth/bsize1 2    89210            772600             8.660464\n:/hdb/2014.03.12/depth/bsize1 2    84416            730928             8.658643\n:/hdb/2014.01.06/depth/bsize1 2    115067           995532             8.651759\n.....  A note for windows users - windows supports compression only with a\ncompression blocksize of 16 or more.", 
            "title": "compress.q"
        }, 
        {
            "location": "/utilities/#dataloaderq", 
            "text": "This script contains some utility functions to assist in loading data\nfrom delimited files (e.g. comma separated, tab delimited). It is a more\ngeneric version of  the data loader example on\ncode.kx .\nThe supplied functions allow data to be read in configurable size chunks\nand written out to the database. When all the data is written, the\non-disk data is re-sorted and the attributes are applied. The main\nfunction is .loader.loadalldata which takes two parameters- a dictionary\nof loading parameters and a directory containing the files to read. The\ndictionary should/can have the following fields:     Parameter  Req  Type  Description      headers  Y  symbol list  Names of the header columns in the file    types  Y  char list  Data types to read from the file    separator  Y  char[list]  Delimiting character. Enlist it if first line of file is header data    tablename  Y  symbol  Name of table to write data to    dbdir  Y  symbol  Directory to write data to    partitiontype  N  symbol  Partitioning to use. Must be one of `date`month`year`int. Default is `date    partitioncol  N  symbol  Column to use to extract partition information.Default is `time    dataprocessfunc  N  function  Diadic function to process data after it has been read in. First argument is load parameters dictionary, second argument is data which has been read in. Default is {[x;y] y}    chunksize  N  int  Data size in bytes to read in one chunk. Default is 100 MB    compression  N  int list  Compression parameters to use e.g. 17 2 6. Default is empty list for no compression    gc  N  boolean  Whether to run garbage collection at appropriate points. Default is 0b (false)     Example usage:  .loader.loadallfiles[`headers`types`separator`tablename`dbdir!(`sym`time`price`volume;\"SP  FI\";\",\";`trade;`:hdb); `:TDC/toload]\n.loader.loadallfiles[`headers`types`separator`tablename`dbdir`dataprocessfunc`chunksize`partitiontype`partitioncol`compression`gc!(`sym`time`price`volume;\"SP  FI\";enlist\",\";`tradesummary;`:hdb;{[p;t] select sum size, max price by date:time.date from t};`int$500*2 xexp 20;`month;`date;16 1 0;1b); `:TDC/toload]", 
            "title": "dataloader.q"
        }, 
        {
            "location": "/utilities/#subscriptionsq", 
            "text": "The subscription utilities allow multiple subscriptions to different\ndata sources to be managed and maintained. Automatic resubscriptions in\nthe event of failure are possible, along as specifying whether the\nprocess will get the schema and replay the log file from the remote\nsource (e.g. in the case of tickerplant subscriptions).  .sub.getsubscriptionhandles is used to get a table of processes to\nsubscribe to. The following can be used to return a table of all\nconnected processes of type tickerplant:  .sub.getsubscriptionhandles[`tickerplant;`;()!()]  .sub.subscribe is used to subscribe to a process for the supplied list\nof tables and instruments. For example, to subscribe to instruments A, B\nand C for the quote table from all tickerplants:  .sub.subscribe[`trthquote;`A`B;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;`;()!()]  The subscription method uses backtick for \u201call\u201d (which is the same as\nkdb+tick). To subscribe to all tables, all instruments, from all\ntickerplants:  .sub.subscribe[`;`;0b;0b] each .sub.getsubscriptionhandles[`tickerplant;`;()!()]  See .api.p\u201c.sub.*\u201d for more details.", 
            "title": "subscriptions.q"
        }, 
        {
            "location": "/utilities/#pubsubq", 
            "text": "pubsub.q is essentially a placeholder script to allow publish and\nsubscribe functionality to be implemented. Licenced kdb+tick users can\nuse the publish and subscribe functionality implemented in u.[k|q]. If\nu.[k|q] is placed in the common code directory and loaded before\npubsub.q (make sure u.[k|q] is listed before pubsub.q in order.txt)\nthen publish and subscribe will be implemented. You can also build out\nthis file to add your own publish and subscribe routines as required.", 
            "title": "pubsub.q"
        }, 
        {
            "location": "/utilities/#kafkaq", 
            "text": "kafka.q provides q language bindings for Apache Kafka, a 'distributed streaming\nplatform', a real time messaging system with persistent storage in message logs.  The core functionality of Kafka \u2013 pub/sub messaging with persisted logs, will be\nfamiliar to most readers as the functionality offered by the kdb+ tick\ntickerplant. The tickerplant log allows the real time database and other\nconsumers to replay a day\u2019s events to recover state. An application architecture\nbuilt around Kafka could dispense with a tickerplant component, and have RDBs\nand other real time clients query Kafka on startup for offsets, and play back\nthe data they need. While not suitable for very low latency access to streaming\ndata, it would carry some advantages for very high throughput applications,\nparticularly those in the cloud:   Kafka\u2019s distributed nature should allow it to scale more transparently than\nsplitting tickerplants by instrument universe or message type  Replaying from offsets is the same interface as live pub/sub and doesn\u2019t require\nfilesystem access to the tickerplant log, so RDB\u2019s and other consumer could be\non a different server   By default, the Kafka bindings will be loaded into all TorQ processes running on\nl64 systems (the only platform currently supported). An example of usage is\nshown here (this assumes a local running instance of kafka - instructions for\nthis are available on the  kafkaq  github \nrepo):  q).kafka.initconsumer[`localhost:9092;()]\nq).kafka.initproducer[`localhost:9092;()]\nq)kupd / print default definition for incoming data - ignore key, print message\nas ascii\n{[k;x] -1 `char$x;}\nq).kafka.subscribe[`test;0] / subscribe to topic test, partition 0\nq)pub:{.kafka.publish[`test;0;`;`byte$x]} / define pub to publish text input to topic\ntest on partition 0 with no key defined\nq)pub hello world \nq)hello world  Limitations of the current implementation:   Only l64 supported  Single consumer thread subscribed to one topic at a time", 
            "title": "kafka.q"
        }, 
        {
            "location": "/utilities/#tplogutilsq", 
            "text": "tplogutils.q contains functions for recovering tickerplant log files.\nUnder certain circumstances the tickerplant log file can become corrupt\nby having an invalid sequence of bytes written to it. A log file can be\nrecovered using a simple recovery method. However, this will only\nrecover messages up to the first invalid message. The recovery functions\ndefined in tplogutils.q allow all valid messages to be recovered from\nthe tickerplant log file.", 
            "title": "tplogutils.q"
        }, 
        {
            "location": "/utilities/#monitoringchecksq", 
            "text": "monitoringchecks.q implements a set of standard, basic monitoring\nchecks. They include checks to ensure:    table sizes are increasing during live capture    the HDB data saves down correctly    the allocated memory of a process does not increase past a certain\n      size    the size of the symbol list in memory doesn\u2019t grow to big    the process does not have too much on its pending subscriber queue    These checks are intended to be run by the reporter process on a\nschedule, and any alerts emailed to an appropriate recipient list.", 
            "title": "monitoringchecks.q"
        }, 
        {
            "location": "/utilities/#heartbeatq", 
            "text": "heartbeat.q implements heartbeating, and relies on both timer.q and\npubsub.q. A table called heartbeat will be published periodically,\nallowing downstream processes to detect the availability of upstream\ncomponents. The heartbeat table contains a heartbeat time and counter.\nThe heartbeat script contains functions to handle and process heartbeats\nand manage upstream process failures. See .api.p.hb.*for details.", 
            "title": "heartbeat.q"
        }, 
        {
            "location": "/utilities/#rmvrq", 
            "text": "This file contains a function which can be used to convert environment variable paths into a full path from the root directory.", 
            "title": "rmvr.q"
        }, 
        {
            "location": "/utilities/#osq", 
            "text": "A file with various q functions to perform system operations. This will detect your operating system and will perform the correct commands depending on what you are using.  This is a modification of a script developed by Simon Garland.", 
            "title": "os.q"
        }, 
        {
            "location": "/utilities/#dbwriteutilsq", 
            "text": "This contains a set of utility functions for writing data to historic\ndatabases.", 
            "title": "dbwriteutils.q"
        }, 
        {
            "location": "/utilities/#sorting-and-attributes", 
            "text": "The sort utilities allow the sort order and attributes of tables to be\nglobally defined. This helps to manage the code base when the data can\npotentially be written from multiple locations (e.g. written from the\nRDB, loaded from flat file, replayed from the tickerplant log). The\nconfiguration is defined in a csv which defaults to $KDBCONFG/sort.csv.\nThe default setup is that every table is sorted by sym and time, with a\np attribute on sym (this is the standard kdb+ tick configuration).  aquaq$ tail config/sort.csv \ntabname,att,column,sort\ndefault,p,sym,1\ndefault,,time,1  As an example, assume we have an optiontrade table which we want to be\ndifferent from the standard set up. We would like the table to be sorted\nby optionticker and then time, with a p attribute on optionticker. We\nalso have a column called underlyingticker which we can put an attribute\non as it is derived from optionticker (so there is an element of\nde-normalisation present in the table). We also have an exchange field\nwhich we would like to put a g attribute on. All other tables we want to\nbe sorted and parted in the standard way. The configuration file would\nlook like this (sort order is derived from the order within the file\ncombined with the sort flag being set to true):  aquaq$ tail config/sort.csv                \ntabname,att,column,sort\ndefault,p,sym,1\ndefault,,time,1\noptiontrade,p,optionticker,1\noptiontrade,,exchtime,1\noptiontrade,p,underlyingticker,0\noptiontrade,g,exchange,0  To invoke the sort utilities, supply a list of (tablename; partitions)\ne.g.  q).sort.sorttab(`trthtrade;`:hdb/2014.11.20/trthtrade`:hdb/2014.11.20/trthtrade)\n2014.12.03D09:56:19.214006000|aquaq|test|INF|sort|sorting the trthtrade table\n2014.12.03D09:56:19.214045000|aquaq|test|INF|sorttab|No sort parameters have been specified for : trthtrade. Using default parameters\n2014.12.03D09:56:19.214057000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.19/trthtrade/ by these columns : sym, time\n2014.12.03D09:56:19.219716000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.19/trthtrade/\n2014.12.03D09:56:19.220846000|aquaq|test|INF|sortfunction|sorting :hdb/2014.11.20/trthtrade/ by these columns : sym, time\n2014.12.03D09:56:19.226008000|aquaq|test|INF|applyattr|applying p attr to the sym column in :hdb/2014.11.20/trthtrade/\n2014.12.03D09:56:19.226636000|aquaq|test|INF|sort|finished sorting the trthtrade table  A different sort configuration file can be loaded with  .sort.getsortcsv[`:file]", 
            "title": "Sorting and Attributes"
        }, 
        {
            "location": "/utilities/#garbage-collection", 
            "text": "The garbage collection utility prints some debug information before and\nafter the garbage collection.  q).gc.run[]                                                                                                                                                      \n2014.12.03D10:22:51.688435000|aquaq|test|INF|garbagecollect|Starting garbage collect. mem stats: used=2 MB; heap=1984 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB\n2014.12.03D10:22:53.920656000|aquaq|test|INF|garbagecollect|Garbage collection returned 1472MB. mem stats: used=2 MB; heap=512 MB; peak=1984 MB; wmax=0 MB; mmap=0 MB; mphy=16384 MB; syms=0 MB; symw=0 MB", 
            "title": "Garbage Collection"
        }, 
        {
            "location": "/utilities/#table-manipulation", 
            "text": "The table manipulation utilities allow table manipulation routines to be\ndefined in a single place. This is useful when data can be written from\nmutliple different processes e.g. RDB, WDB, or tickerplant log replay.\nInstead of having to create a separate definition of customised\nmanipulation in each process, it can be done in a single location and\ninvokved in each process.", 
            "title": "Table Manipulation"
        }, 
        {
            "location": "/utilities/#helpq", 
            "text": "The standard help.q from code.kx provides help utilities in the console.\nThis should be kept up to date with\n[ code.kx ].  q)help`                                                                                                                                                                                                                         \nadverb    | adverbs/operators\nattributes| data attributes\ncmdline   | command line parameters\ndata      | data types\ndefine    | assign, define, control and debug\ndotz      | .z locale contents\nerrors    | error messages\nsave      | save/load tables\nsyscmd    | system commands\ntemporal  | temporal - date   time casts\nverbs     | verbs/functions", 
            "title": "help.q"
        }, 
        {
            "location": "/utilities/#htmlq", 
            "text": "An HTML utility has been added to accompany the HTML5 front end for the\nMonitoring process. It includes functions to format dates, tables to csv\nto configure the HTML file to work on the correct process. It is\naccessible from the  .html  namespace.", 
            "title": "html.q"
        }, 
        {
            "location": "/utilities/#eodtimeq", 
            "text": "This script provides functionality for managing timezones. TorQ can be \nconfigured to timestamp data in a specific timezone, while also being\nconfigured to perform the end of day rollover in another timezone, at a\nconfigurable time.  These options are handled by three settings:     Setting  Req  Type  Description      .eodtime.rolltimeoffset  Y  timespan  Offset from default midnight roll time    .eodtime.rolltimezone  Y  symbol  Time zone in which to rollover    .eodtime.datatimezone  Y  symbol  Time zone in which to timestamp data in TP     The default configuration sets both timezones to GMT and has the rollover\nperformed at midnight.  A table containing the valid timezones is loaded into TorQ processes as .tz.t  An example configuration where data is stamped in GMT, but the rollover\noccurs at 5PM New York time would be:  .eodtime.rolltimeoffset:-0D07:00:00.000000000; // 5 PM i.e. 7 hours before midnight\n.eodtime.rolltimezone:`$\"America/New_YorK\";    // roll in NYC time\n.eodtime.datatimezone:`$\"GMT\";                 // timestamp in GMT  Note that the rolltimeoffset can be negative - this will cause the rollover to happen \n\"yesterday\", meaning that at the rolltime, the trading date will become the day  after \nthe calendar date. Where this is positive, the rollover occurs \"today\" and so the trading\ndate will become the current calendar date.", 
            "title": "eodtime.q"
        }, 
        {
            "location": "/utilities/#subscribercutoffq", 
            "text": "This script is used to provide functionality for cutting off any slow subscribers on any\nTorQ processes. The script will periodically check (time between checks set in .subcut.checkfreq.\nDefault is 1 minute) the byte size of the queue for all the handles on the process to see if\nthey have exceeded a set cut-off point (set in the variable .subcut.maxsize) and will only\ncut-off the handle if it exceeds this limit a set number of times in a row (default is 3\nand set in the .subcut.breachlimit variable). This gives clients a chance to tidy up their\nbehavior and will avoid cutting off clients if they happened to have a spike just before the\ncheck was performed. The .subcut.state variable is used to keep track of the handles and the \nnumber of times they have exceeded the size limit in a row.   To enable this functionality the .subcut.enabled flag must be set to true and \nthe timer.q script must be loaded on the desired processes. By default the chained \ntickerplant is the only processes with the functionality enabled.", 
            "title": "subscribercutoff.q"
        }, 
        {
            "location": "/utilities/#datareplayq", 
            "text": "The datareplay utility provides functionality for generating tickerplant function calls from historcial\ndata which can be executed by subscriber functions. This can be used to test a known data-set against a \nsubscriber for testing or debugging purposes.  It can load this data from the current TorQ session, or from a remote hdb if given its connection handle.  It can also chunk the data by time increments (as if the tickerplant was in batch mode), and can also generate\ncalls to a custom timer function for the same time increments (defaults to .z.ts).  The functions provided by this utility are made available in the .datareplay namespace.  The utility is mainly used via the tabesToDataStreamFunction, which accepts a dictionary parameter with the following\nfields:     Key  Example Value  Description  Required  Default      tabs  `trade`quote or `trade  List of tables to include  Yes  N/A    sts  2014.04.04D07:00:00.000  Start timestamp for data  Yes  N/A    ets  2014.04.04D16:30:00.000  End of timestamp for data  Yes  N/A    syms  `AAPL`IBM  List of symbols to include  No  All syms    where  ,(=;`src;,`L)  Custom where clause in functional form  No  none    timer  1b  Generate timer function flag  No  0b    h  5i  Handle to hdb process  No  0i (self)    interval  0D00:00:01.00  Time interval used to chunk data, bucketed by timestamp if no time interval set  No  None    tc  `data_time  Name of time column to cut on  No  `time    timerfunc  .z.ts  Timer function to use if `timer parameter is set  No  .z.ts     When the timer flag is set, the utility will interleave timer function calls in the message column at intervals based on the interval parameter, or every 10 seconds if interval is not set. This is useful if testing requires a call to a function at a set time, to generate a VWAP every 10 minutes for example. The function the timer messages call is based on the timerfunc parameter, or .z.ts if this parameter is not set.  If the interval is set the messages will be aggregated into chunks based on the interval value, if no interval is specified, the data will be bucketed by timestamp (one message chunk per distinct timestamp per table).  If no connection handle is specified (h parameter), the utility will retrieve the data from the process the utility is running on, using handle 0.  The where parameter allows for the use of a custom where clause when extracting data, which can be useful when the dataset is large and only certain data is required, for example if only data where  src=`L  is required. The where clause(s) are required to be in functional form, for example  enlist (=;`src;,`L)  or  ((=;`src;enlist `L);( ;`size;100))  (note, that if only one custom where clause is included it is required to be enlisted).  It is possible to get the functional form of a where clause by running parse on a mock select string like below:  q)parse \"select from t where src=`L,size 100\"\n?\n`t\n,((=;`src;,`L);( ;`size;100))\n0b\n()  The where clause is then the 3rd item returned in the parse tree.", 
            "title": "datareplay.q"
        }, 
        {
            "location": "/utilities/#examples", 
            "text": "Extract all data between sts and ets from the trades table in the current process.  q)input\ntabs| `trades\nsts | 2014.04.21D07:00:00.000000000\nets | 2014.05.02D17:00:00.000000000\nq).datareplay.tablesToDataStream input\ntime                          msg                                            ..\n-----------------------------------------------------------------------------..\n2014.04.21D08:00:23.478000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n2014.04.21D08:00:49.511000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n2014.04.21D08:01:45.623000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n2014.04.21D08:02:41.346000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n..\nq)first .datareplay.tablesToDataStream input\ntime| 2014.04.21D08:00:23.478000000\nmsg | (`upd;`trades;`sym`time`src`price`size!(`YHOO;2014.04.21D08:00:23.47800..  Extract all data between sts and ets from the trades table from a remote hdb handle=3i.  q)input\ntabs| `trades\nsts | 2014.04.21D07:00:00.000000000\nets | 2014.05.02D17:00:00.000000000\nh   | 3i\nq).datareplay.tablesToDataStream input\ntime                          msg                                            ..\n-----------------------------------------------------------------------------..\n2014.04.21D08:00:07.769000000 `upd `trades `sym`time`src`price`size!(`IBM;201..\n2014.04.21D08:00:13.250000000 `upd `trades `sym`time`src`price`size!(`NOK;201..\n2014.04.21D08:00:19.070000000 `upd `trades `sym`time`src`price`size!(`MSFT;20..\n2014.04.21D08:00:23.678000000 `upd `trades `sym`time`src`price`size!(`YHOO;20..\n..\nq)first .datareplay.tablesToDataStream input\ntime| 2014.04.21D08:00:07.769000000\nmsg | (`upd;`trades;`sym`time`src`price`size!(`IBM;2014.04.21D08:00:07.769000..  Same as above but including quote table and with interval of 10 minutes:  q)input\ntabs    | `quotes`trades\nsts     | 2014.04.21D07:00:00.000000000\nets     | 2014.05.02D17:00:00.000000000\nh       | 3i\ninterval| 0D00:10:00.000000000\nq).datareplay.tablesToDataStream input\ntime                          msg                                            ..\n-----------------------------------------------------------------------------..\n2014.04.21D08:09:47.600000000 `upd `trades +`sym`time`src`price`size!(`YHOO`A..\n2014.04.21D08:09:55.210000000 `upd `quotes +`sym`time`src`bid`ask`bsize`asize..\n2014.04.21D08:19:39.467000000 `upd `trades +`sym`time`src`price`size!(`CSCO`N..\n2014.04.21D08:19:49.068000000 `upd `quotes +`sym`time`src`bid`ask`bsize`asize..\n..\nq)first .datareplay.tablesToDataStream input\ntime| 2014.04.21D08:09:47.600000000\nmsg | (`upd;`trades;+`sym`time`src`price`size!(`YHOO`AAPL`MSFT`NOK`DELL`YHOO`..  All messages from trades where  src=`L  bucketed in 10 minute intervals interleaved with calls to the function  `vwap .  q)input\ntabs     | `trades\nh        | 3i\nsts      | 2014.04.21D08:00:00.000000000\nets      | 2014.05.02D17:00:00.000000000\nwhere    | ,(=;`src;,`L)\ntimer    | 1b\ntimerfunc| `vwap\ninterval | 0D00:10:00.000000000\nq).datareplay.tablesToDataStream input\ntime                          msg                                            ..\n-----------------------------------------------------------------------------..\n2014.04.21D08:00:00.000000000 (`vwap;2014.04.21D08:00:00.000000000)          ..\n2014.04.21D08:09:46.258000000 (`upd;`trades;+`sym`time`src`price`size!(`AAPL`..\n2014.04.21D08:10:00.000000000 (`vwap;2014.04.21D08:10:00.000000000)          ..\n2014.04.21D08:18:17.188000000 (`upd;`trades;+`sym`time`src`price`size!(`AAPL`..\n..", 
            "title": "Examples:"
        }, 
        {
            "location": "/utilities/#modified-uq", 
            "text": "Starting in kdb+ v3.4, the new broadcast feature has some performance\nbenefits. It works by serialising a message once before sending it\nasynchronously to a list of subscribers whereas the previous method\nwould serialise it separately for each subscriber. To take advantage of\nthis, we\u2019ve modified u.q. This can be turned off by setting .u.broadcast\nto false. It is enabled by default, but will only override default\npublishing if the kdb+ version being used is 3.4 or after.", 
            "title": "Modified u.q"
        }, 
        {
            "location": "/utilities/#full-api", 
            "text": "The full public api can be found by running  q).api.u`                                                                                                                                                                                                                       \nname             | vartype  namespace public descrip                 ..\n-----------------| --------------------------------------------------..\n.proc.createlog  | function .proc     1      \"Create the standard out..\n.proc.rolllogauto| function .proc     1      \"Roll the standard out/e..\n.proc.loadf      | function .proc     1      \"Load the specified file..\n.proc.loaddir    | function .proc     1      \"Load all the .q and .k ..\n.lg.o            | function .lg       1      \"Log to standard out\"   ..\n..  Combined with the commented configuration file, this should give a good\noverview of the functionality available. A description of the individual\nnamespaces is below- run .api.u namespace*to list the functions.     Namespace  Description      .proc  Process API    .lg  Standard out/error logging API    .err  Error throwing API    .usage  Usage logging API    .access  Permissions API    .clients  Client tracking API    .servers  Server tracking API    .async  Async communication API    .timer  Timer API    .cache  Caching API    .tz  Timezone conversions API    .checks  Monitoring API    .cmp  Compression API    .ps  Publish and Subscribe API    .hb  Heartbeating API    .loader  Data Loader API    .sort  Data sorting and attribute setting API    .sub  Subscription API    .gc  Garbage Collection API    .tplog  Tickerplant Log Replay API    .api  API management API", 
            "title": "Full API"
        }, 
        {
            "location": "/utilities/#api-table", 
            "text": "name  vartype  namespace  descrip  params  return      .proc.createlog  function  .proc  Create the standard out and standard err log files. Redirect to them  [string: log directory; string: name of the log file;mixed: timestamp suffix for the file (can be null); boolean: suppress the generation of an alias link]  null    .proc.rolllogauto  function  .proc  Roll the standard out/err log files  []  null    .proc.loadf  function  .proc  Load the specified file  [string: filename]  null    .proc.loaddir  function  .proc  Load all the .q and .k files in the specified directory. If order.txt is found in the directory, use the ordering found in that file  [string: name of directory]  null    .proc.getattributes  function  .proc  Called by external processes to retrieve the attributes (advertised functionality) of this process  []  dictionary of attributes    .proc.override  function  .proc  Override configuration varibles with command line parameters.  For example, if you set -.servers.HOPENTIMEOUT 5000 on the command line and call this function, then the command line value will be used  []  null    .proc.overrideconfig  function  .proc  Override configuration varibles with values in supplied parameter dictionary. Generic version of .proc.override  [dictionary: command line parameters.  .proc.params should be used]  null    .lg.o  function  .lg  Log to standard out  [symbol: id of log message; string: message]  null    .lg.e  function  .lg  Log to standard err  [symbol: id of log message; string: message]  null    .lg.l  function  .lg  Log to either standard error or standard out, depending on the log level  [symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters, used in the logging extension function]  null    .lg.err  function  .lg  Log to standard err  [symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters, used in the logging extension function]  null    .lg.ext  function  .lg  Extra function invoked in standard logging function .lg.l.  Can be used to do more with the log message, e.g. publish externally  [symbol: log level; symbol: name of process; symbol: id of log message; string: message; dict: extra parameters]  null    .err.ex  function  .err  Log to standard err, exit  [symbol: id of log message; string: message; int: exit code]  null    .err.usage  function  .err  Throw a usage error and exit  []  null    .err.param  function  .err  Check a dictionary for a set of required parameters. Print an error and exit if not all required are supplied  [dict: parameters; symbol list: the required param values]  null    .err.env  function  .err  Check if a list of required environment variables are set.  If not, print an error and exit  [symbol list: list of required environment variables]  null    .usage.rolllogauto  function  .usage  Roll the .usage txt files  []  null    .usage.readlog  function  .usage  Read and return a usage log file as a table  [string: name of log file]  null    .usage.logtodisk  variable  .usage  whether to log to disk      .usage.logtomemory  variable  .usage  whether to log to .usage.usage      .usage.ignore  variable  .usage  whether to check the ignore list for functions to ignore      .usage.ignorelist  variable  .usage  the list of functions to ignore      .usage.logroll  variable  .usage  whether to automatically roll the log file      .usage.usage  table  .usage  log of messages through the message handlers      .clients.clients  table  .clients  table containing client handles and session values      .sub.getsubscriptionhandles  function  .sub  Connect to a list of processes of a specified type  [symbol: process type to match; symbol: process name to match; dictionary:attributes of process]  table of process names, types and the handle connected on    .sub.subscribe  function  .sub  Subscribe to a table or list of tables and specified instruments  [symbol (list):table names; symbol (list): instruments; boolean: whether to set the schema from the server; boolean: wether to replay the logfile; dictionary: procname,proctype,handle     .pm.adduser  function  .pm  Adds a user to be permissioned as well as setting their password and the method used to hash it.  [symbol: the username; symbol: method used to authenticate; symbol: method used to hash the password; string: password, hashed using the proper method]  null    .pm.addgroup  function  .pm  Add a group which will have access to certain tables and variables  [symbol: the name of the group; string: a description of the group]  null    .pm.addrole  function  .pm  Add a role which will have access to certain functions  [symbol: the name of the role; string: a description of the role]  null    .pm.addtogroup  function  .pm  Add a user to a group, giving them access to all of its variables  [symbol: the name of the user to add; symbol: group the user is to be added to]  null    .pm.assignrole  function  .pm  Assign a user a role, giving them access to all of its functions  [symbol: the name of the user to add; symbol: role the user is to be assigned to]  null    .pm.grantaccess  function  .pm  Give a group access to a variable  [symbol: the name of the variable the group should get access to; symbol: group that is to be given this access; symbol: the type of access that should be given, eg. read, write]  null    .pm.grantfunction  function  .pm  Give a role access to a function  symbol: name of the function to be added; symbol: role that is to be given this access; TO CLARIFY  null    .pm.createvirtualtable  function  .pm  Create a virtual table that a group might be able to access instead of the full table  [symbol: new name of the table; symbol: name of the actual table t add; TO CLARIFY]  null    .pm.cloneuser  function  .pm  Add a new user that is identical to another user  [symbol: name of the new user; symbol: name of the user to be cloned; string: password of the new user]  null    .access.addsuperuser  function  .access  Add a super user  [symbol: user]  null    .access.addpoweruser  function  .access  Add a power user  [symbol: user]  null    .access.adddefaultuser  function  .access  Add a default user  [symbol: user]  null    .access.readpermissions  function  .access  Read the permissions from a directory  [string: directory containing the permissions files]  null    .access.USERS  table  .access  Table of users and their types      .servers.opencon  function  .servers  open a connection to a process using the default timeout. If no user:pass supplied, the default one will be added if set  [symbol: the host:port[:user:pass]]  int: the process handle, null if the connection failed    .servers.addh  function  .servers  open a connection to a server, store the connection details  [symbol: the host:port:user:pass connection symbol]  int: the server handle    .servers.addw  function  .servers  add the connection details of a process behind the handle  [int: server handle]  null    .servers.addnthawc  function  .servers  add the details of a connection to the table  [symbol: process name; symbol: process type; hpup: host:port:user:pass connection symbol; dict: attributes of the process; int: handle to the process;boolean: whether to check the handle is valid on insert  int: the handle of the process    .servers.getservers  function  .servers  get a table of servers which match the given criteria  [symbol: pick the server based on the name value or the type value.  Can be either `procname`proctype; symbol(list): lookup values. ` for any; dict: requirements dictionary; boolean: whether to automatically open dead connections for the specified lookup values; boolean: if only one of each of the specified lookup values is required (means dead connections aren't opened if there is one available)]  table: processes details and requirements matches    .servers.gethandlebytype  function  .servers  get a server handle for the supplied type  [symbol: process type; symbol: selection criteria. One of `roundrobin`any`last]  int: handle of server    .servers.gethpbytype  function  .servers  get a server hpup connection symbol for the supplied type  [symbol: process type; symbol: selection criteria. One of `roundrobin`any`last]  symbol: h:p:u:p connection symbol of server    .servers.startup  function  .servers  initialise all the connections.  Must processes should call this during initialisation  []  null    .servers.refreshattributes  function  .servers  refresh the attributes registered with the discovery service.  Should be called whenever they change e.g. end of day for an HDB  []  null    .servers.SERVERS  table  .servers  table containing server handles and session values      .timer.repeat  function  .timer  Add a repeating timer with default next schedule  [timestamp: start time; timestamp: end time; timespan: period; mixedlist: (function and argument list); string: description string]  null    .timer.once  function  .timer  Add a one-off timer to fire at a specific time  [timestamp: execute time; mixedlist: (function and argument list); string: description string]  null    .timer.remove  function  .timer  Delete a row from the timer schedule  [int: timer id to delete]  null    .timer.removefunc  function  .timer  Delete a specific function from the timer schedule  [mixedlist: (function and argument list)]  null    .timer.rep  function  .timer  Add a repeating timer - more flexibility than .timer.repeat  [timestamp: execute time; mixedlist: (function and argument list); short: scheduling algorithm for next timer; string: description string; boolean: whether to check if this new function is already present on the schedule]  null    .timer.one  function  .timer  Add a one-off timer to fire at a specific time - more flexibility than .timer.once  [timestamp: execute time; mixedlist: (function and argument list); string: description string; boolean: whether to check if this new function is already present on the schedule]  null    .timer.timer  table  .timer  The table containing the timer information      .cache.execute  function  .cache  Check the cache for a valid result set, return the results if found, execute the function, cache it and return if not  [mixed: function or string to execute;timespan: maximum allowable age of cache item if found in cache]  mixed: result of function    .cache.getperf  function  .cache  Return the performance statistics of the cache  []  table: cache performance    .cache.maxsize  variable  .cache  The maximum size in MB of the cache. This is evaluated using -22!, so may be incorrect due to power of 2 memory allocation.  To be conservative and ensure it isn't exceeded, set max size to half of the actual max size that you want      .cache.maxindividual  variable  .cache  The maximum size in MB of an individual item in the cache. This is evaluated using -22!, so may be incorrect due to power of 2 memory allocation.  To be conservative and ensure it isn't exceeded, set max size to half of the actual max size that you want      .tz.dg  function  .tz  default from GMT. Convert a timestamp from GMT to the default timezone  [timestamp (list): timestamps to convert]  timestamp atom or list    .tz.lg  function  .tz  local from GMT. Convert a timestamp from GMT to the specified local timezone  [symbol (list): timezone ids;timestamp (list): timestamps to convert]  timestamp atom or list    .tz.gd  function  .tz  GMT from default. Convert a timestamp from the default timezone to GMT  [timestamp (list): timestamps to convert]  timestamp atom or list    .tz.gl  function  .tz  GMT from local. Convert a timestamp from the specified local timezone to GMT  [symbol (list): timezone ids; timestamp (list): timestamps to convert]  timestamp atom or list    .tz.ttz  function  .tz  Convert a timestamp from a specified timezone to a specified destination timezone  [symbol (list): destination timezone ids; symbol (list): source timezone ids; timestamp (list): timestamps to convert]  timestamp atom or list    .tz.default  variable  .tz  Default timezone      .tz.t  table  .tz  Table of timestamp information      .email.connectdefault  function  .email  connect to the default mail server specified in configuration  []     .email.senddefault  function  .email  connect to email server if not connected. Send email using default settings  [dictionary of email parameters. Required dictionary keys are to (symbol (list) of email address to send to), subject (character list), body (list of character arrays).  Optional parameters are cc (symbol(list) of addresses to cc), bodyType (can be `html, default is `text), attachment (symbol (list) of files to attach), image (symbol of image to append to bottom of email. `none is no image), debug (int flag for debug level of connection library. 0i=no info, 1i=normal. 2i=verbose)]  size in bytes of sent email. -1 if failure    .email.test  function  .email  send a test email  [symbol(list):email address to send test email to]  size in bytes of sent email. -1 if failure    .hb.addprocs  function  .hb  Add a set of process types and names to the heartbeat table to actively monitor for heartbeats.  Processes will be automatically added and monitored when the heartbeats are subscribed to, but this is to allow for the case where a process might already be dead and so can't be subscribed to  [symbol(list): process types; symbol(list): process names]     .hb.processwarning  function  .hb  Callback invoked if any process goes into a warning state.  Default implementation is to do nothing - modify as required  [table: processes currently in warning state]     .hb.processerror  function  .hb  Callback invoked if any process goes into an error state. Default implementation is to do nothing - modify as required  [table: processes currently in error state]     .hb.storeheartbeat  function  .hb  Store a heartbeat update.  This function should be added to you update callback when a heartbeat is received  [table: the heartbeat table data to store]     .hb.warningperiod  function  .hb  Return the warning period for a particular process type.  Default is to return warningtolerance * publishinterval. Can be overridden as required  [symbollist: the process types to return the warning period for]  timespan list of warning period    .hb.errorperiod  function  .hb  Return the error period for a particular process type.  Default is to return errortolerance * publishinterval. Can be overridden as required  [symbollist: the process types to return the error period for]  timespan list of error period    .rdb.moveandclear  function  .rdb  Move a variable (table) from one namespace to another, deleting its contents.  Useful during the end-of-day roll down for tables you do not want to save to the HDB  [symbol: the namespace to move the table from; symbol:the namespace to move the variable to; symbol: the name of the variable]  null    .api.f  function  .api  Find a function/variable/table/view in the current process  [string:search string]  table of matching elements    .api.p  function  .api  Find a public function/variable/table/view in the current process  [string:search string]  table of matching public elements    .api.u  function  .api  Find a non-standard q public function/variable/table/view in the current process.  This excludes the .q, .Q, .h, .o namespaces  [string:search string]  table of matching public elements    .api.s  function  .api  Search all function definitions for a specific string  [string: search string]  table of matching functions and definitions    .api.find  function  .api  Generic method for finding functions/variables/tables/views. f,p and u are based on this  [string: search string; boolean (list): public flags to include; boolean: whether the search is context senstive  table of matching elements    .api.search  function  .api  Generic method for searching all function definitions for a specific string. s is based on this  [string: search string; boolean: whether the search is context senstive  table of matching functions and definitions    .api.add  function  .api  Add a function to the api description table  [symbol:the name of the function; boolean:whether it should be called externally; string:the description; dict or string:the parameters for the function;string: what the function returns]  null    .api.fullapi  function  .api  Return the full function api table  []  api table    .api.m  function  .api  Return the ordered approximate memory usage of each variable and view in the process. Views will be re-evaluated if required  []  memory usage table    .api.mem  function  .api  Return the ordered approximate memory usage of each variable and view in the process. Views are only returned if view flag is set to true. Views will be re-evaluated if required  [boolean:return views]  memory usage table    .api.whereami  function  .api  Get the name of a supplied function definition. Can be used in the debugger e.g. .api.whereami[.z.s]  function definition  symbol: the name of the current function    .ps.publish  function  .ps  Publish a table of data  [symbol: name of table; table: table of data]     .ps.subscribe  function  .ps  Subscribe to a table and list of instruments  [symbol(list): table name. ` for all; symbol(list): symbols to subscribe to. ` for all]  mixed type list of table names and schemas    .ps.initialise  function  .ps  Initialise the pubsub routines.  Any tables that exist in the top level can be published  []     .async.deferred  function  .async  Use async messaging to simulate sync communication  [int(list): handles to query; query]  (boolean list:success status; result list)    .async.postback  function  .async  Send an async message to a process and the results will be posted back within the postback function call  [int(list): handles to query; query; postback function]  boolean list: successful send status    .cmp.showcomp  function  .cmp  Show which files will be compressed and how; driven from csv file  [`:/path/to/database; `:/path/to/configcsv; maxagefilestocompress]  table of files to be compressed    .cmp.compressmaxage  function  .cmp  Run compression on files using parameters specified in configuration csv file, and specifying the maximum age of files to compress  [`:/path/to/database; `:/path/to/configcsv; maxagefilestocompress]     .cmp.docompression  function  .cmp  Run compression on files using parameters specified in configuration csv file  [`:/path/to/database; `:/path/to/configcsv]     .loader.loadallfiles  function  .loader  Generic loader function to read a directory of files in chunks and write them out to disk  [dictionary of load parameters. Should have keys of headers (symbol list), types (character list), separator (character), tablename (symbol), dbdir (symbol).  Optional params of dataprocessfunc (diadic function), datecol (name of column to extract date from: symbol), chunksize (amount of data to read at once:int), compression (compression parameters to use e.g. 16 1 0:int list), gc (boolean flag of whether to run garbage collection:boolean); directory containing files to load (symbol)]     .sort.sorttab  function  .sort  Sort and set the attributes for a table and set of partitions based on a configuration file (default is $KDBCONFIG/sort.csv)  [2 item list of (tablename e.g. `trade; partitions to sort and apply attributes to e.g. `:/hdb/2000.01.01/trade`:hdb/2000.01.02/trade)]     .sort.getsortcsv  function  .sort  Read in the sort csv from the specified location  [symbol: the location of the file e.g. `:config/sort.csv]     .gc.run  function  .gc  Run garbage collection, print debug info before and after      .mem.objsize  function  .mem  Returns the calculated memory size in bytes used by an object.  It may take a little bit of time for objects with lots of nested structures (e.g. lots of nested columns)  [q object]  size of the object in bytes    .tplog.check  function  .tplog  Checks if tickerplant log can be replayed.  If it can or can replay the first X messages, then returns the log handle, else it will read log as byte stream and create a good log and then return the good log handle  [logfile (symbol), handle to the log file to check; lastmsgtoreplay (long), the index of the last message to be replayed from log ]  handle to log file, will be either the input log handle or handle to repaired log, depends on whether the log was corrupt", 
            "title": "API Table"
        }, 
        {
            "location": "/handlers/", 
            "text": "Message Handlers\n\n\nThere is a separate code directory containing message handler\ncustomizations. This is found at $KDBCODE/handlers. Much of the code is\nderived from Simon Garland\u2019s contributions to\n\ncode.kx\n.\n\n\nEvery external interaction with a process goes through a message\nhandler, and these can be modified to, for example, log or restrict\naccess. Passing through a bespoke function defined in a message handler\nwill add extra processing time and therefore latency to the message. All\nthe customizations we have provided aim to minimise additional latency,\nbut if a bespoke process is latency sensitive then some or all of the\ncustomizations could be switched off. We would argue though that\ngenerally it is better to switch on all the message handler functions\nwhich provide diagnostic information, as for most non-latency sensitive\nprocesses (HDBs, Gateways, some RDBs etc.) the extra information upon\nfailure is worth the cost. The message handlers can be globally switched\noff by setting .proc.loadhandlers to 0b in the configuration file.\n\n\n\n\n\n\n\n\nScript\n\n\nNS\n\n\nDiag\n\n\nFunction\n\n\nModifies\n\n\n\n\n\n\n\n\n\n\nlogusage.q\n\n\n.usage\n\n\nY\n\n\nLog all client interaction to an ascii log file and/or in-memory table. Messages can be logged before and after they are processed. Timer calls are also logged. Exclusion function list can be applied to .z.ps to disable logging of asynchronous real time updates\n\n\npw, po, pg, ps, pc, ws, ph, pp, pi, exit, timer\n\n\n\n\n\n\ncontrolaccess.q\n\n\n.access\n\n\nN\n\n\nRestrict access for set of users/user groups to a list of functions, and from a defined set of servers\n\n\npw, pg, ps, ws, ph, pp, pi\n\n\n\n\n\n\ntrackclients.q\n\n\n.clients\n\n\nY\n\n\nTrack client process details including then number of requests and cumulative data size returned\n\n\npo, pg, ps, ws, pc\n\n\n\n\n\n\ntrackservers.q\n\n\n.servers\n\n\nY\n\n\nDiscover and track server processes including name, type and attribute information. This also contains the core of the code which can be used in conjunction with the discovery service.\n\n\npc, timer\n\n\n\n\n\n\nzpsignore.q\n\n\n.zpsignore\n\n\nN\n\n\nOverride async message handler based on certain message patterns\n\n\nps\n\n\n\n\n\n\nwriteaccess.q\n\n\n.readonly\n\n\nN\n\n\nRestrict client write access to prevent any modification to data in place. Also disables all HTTP access.\n\n\npg, ps, ws, ph, pp\n\n\n\n\n\n\nldap.q\n\n\n.ldap\n\n\nN\n\n\nRestrict client access to process using ldap authentication.\n\n\npw\n\n\n\n\n\n\n\n\nEach customization can be turned on or off individually from the\nconfiguration file(s). Each script can be extensively customised using\nthe configuration file. Example customization for logusage.q, taken from\n$KDBCONFIG/settings/default.q is below. Please see default.q for the\nremaining configuration of the other message handler files.\n\n\n/- Configuration used by the usage functions - logging of client interaction\n\\d .usage\nenabled:1b      /- whether the usage logging is enabled\nlogtodisk:1b        /- whether to log to disk or not\nlogtomemory:1b      /- write query logs to memory\nignore:1b       /- check the ignore list for functions to ignore\nignorelist:(`upd;\"upd\") /- the list of functions to ignore in async calls\nflushtime:1D00      /- default value for how long to persist the\n            /- in-memory logs. Set to 0D for no flushing\nsuppressalias:0b    /- whether to suppress the log file alias creation\nlogtimestamp:{[].z.d}   /- function to generate the log file timestamp suffix\nLEVEL:3         /- log level. 0=none;1=errors;2=errors+complete\n            /- queries;3=errors+before a query+after\nlogroll:1b      /- Whether or not to roll the log file\n            /- automatically (on a daily schedule)\n\n\n\ndotz.q\n\n\nStores all the default values for the message handlers and can be used to revert back to the default if necessary.\n\n\n\n\nlogusage.q\n\n\nlogusage.q is probably the most important of the scripts from a\ndiagnostic perspective. It is a modified version of the logusage.q\nscript on code.kx.\n\n\nIn its most verbose mode it will log information to an in-memory table\n(.usage.usage) and an on-disk ASCII file, both before and after every\nclient interaction and function executed on the timer. These choices\nwere made because:\n\n\n\n\n\n\nlogging to memory enables easy interrogation of client interaction;\n\n\n\n\n\n\nlogging to disk allows persistence if the process fails or locks up.\n      ASCII text files allow interrogation using OS tools such as vi, grep\n      or tail;\n\n\n\n\n\n\nlogging before a query ensures any query that adversely effects the\n      process is definitely captured, as well as capturing some state\n      information before the query execution;\n\n\n\n\n\n\nlogging after a query captures the time taken, result set size and\n      resulting state;\n\n\n\n\n\n\nlogging timer calls ensures a full history of what the process is\n      actually doing. Also, timer call performance degradation over time\n      is a common source of problems in kdb+ systems.\n\n\n\n\n\n\nThe following fields are logged in .usage.usage:\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntime\n\n\nTime the row was added to the table\n\n\n\n\n\n\nid\n\n\nID of the query. Normally before and complete rows will be consecutive but it might not be the case if the incoming call invokes further external communication\n\n\n\n\n\n\ntimer\n\n\nExecution time. Null for rows with status=b (before)\n\n\n\n\n\n\nzcmd\n\n\n.z handler the query arrived through\n\n\n\n\n\n\nstatus\n\n\nQuery status. One of b, c or e (before, complete, error)\n\n\n\n\n\n\na\n\n\nAddress of sender. .dotz.ipa can be used to convert from the integer format to a hostname\n\n\n\n\n\n\nu\n\n\nUsername of sender\n\n\n\n\n\n\nw\n\n\nHandle of sender\n\n\n\n\n\n\ncmd\n\n\nCommand sent\n\n\n\n\n\n\nmem\n\n\nMemory statistics\n\n\n\n\n\n\nsz\n\n\nSize of result. Null for rows with status of b or e\n\n\n\n\n\n\nerror\n\n\nError message\n\n\n\n\n\n\n\n\n\n\ncontrolaccess.q\n\n\ncontrolaccess.q is used to restrict client access to the process. It is\nmodified version of controlaccess.q from code.kx. The script allows\ncontrol of several aspects:\n\n\n\n\n\n\nthe host/ip address of the servers which are allowed to access the\n    process;\n\n\n\n\n\n\ndefinition of three user groups (default, poweruser and superuser)\n      and the actions each group is allowed to do;\n\n\n\n\n\n\nthe group(s) each user is a member of, and any additional actions an\n      individual user is allowed/disallowed outside of the group\n      permissions;\n\n\n\n\n\n\nthe maximum size of the result set returned to a client.\n\n\n\n\n\n\nThe access restrictions are loaded from csv files. The permissions files\nare stored in $KDBCONFIG/permissions.\n\n\n\n\n\n\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n*_hosts.csv\n\n\nContains hostname and ip address (patterns) for servers which are allowed or disallowed access. If a server is not found in the list, it is disallowed\n\n\n\n\n\n\n*_users.csv\n\n\nContains individual users and the user groups they are are a member of\n\n\n\n\n\n\n*_functions.csv\n\n\nContains individual functions and whether each user group is allowed to execute them. ; separated user list enables functions to be allowed by individual users\n\n\n\n\n\n\n\n\nThe permissions files are loaded using a similar hierarchical approach\nas for the configuration and code loading. Three files can be provided-\ndefault_.csv, [proctype]_.csv, and [procname]_.csv. All of the\nfiles will be loaded, but permissions for the same entity (hostpattern,\nuser, or function) defined in [procname]_.csv will override those in\n[proctype]_.csv which will in turn override [procname]_.csv.\n\n\nWhen a client makes a query which is refused by the permissioning layer,\nan error will be raised and logged in .usuage.usage if it is enabled.\n\n\n\n\ntrackclients.q\n\n\ntrackclients.q is used to track client interaction. It is a slightly\nmodified version of trackclients.q from code.kx, and extends the\nfunctionality to handle interaction with the discovery service.\n\n\nWhenever a client opens a connection to the q process, it will be\nregistered in the .clients.clients table. Various details are logged,\nbut from a diagnostic perspective the most important information are the\nclient details, the number of queries it has run, the last time it ran a\nquery, the number of failed queries and the cumulative size of results\nreturned to it.\n\n\n\n\ntrackservers.q\n\n\ntrackservers.q is used to register and maintain handles to external\nservers. It is a heavily modified version of trackservers.q from\ncode.kx. It is explained more in section\u00a0connectionmanagement.\n\n\n\n\nzpsignore.q\n\n\nzpsignore.q is used to check incoming async calls for certain patterns\nand to bypass all further message handler checks for messages matching\nthe pattern. This is useful for handling update messages published to a\nprocess from a data source.\n\n\n\n\nwriteaccess.q\n\n\nwriteaccess.q is used to restrict client write access to data within a\nprocess. The script uses the reval function, released in KDB+ 3.3, to\nprevent client queries from modifying any data in place. At present only\nqueries in the form of strings are passed through the reval function.\nAdditonally the script disables any form of HTTP access. If using\nversions of KDB+ prior to 3.3, this feature must be disabled. An attempt\nto use this feature on previous KDB+ versions will result in an error\nand the relevant process exiting.\n\n\npermissions.q\n\n\npermissions.q is used to control client access to a server process. It\nallows:\n\n\n\n\n\n\nAccess control via username/password access, either in combination\n    with the -u/U process flags or in place of them.\n\n\n\n\n\n\nDefinition of user groups, which control variable access.\n\n\n\n\n\n\nDefinition of user roles, which allow control over function\n    execution.\n\n\n\n\n\n\nDeeper control over table subsetting through the use of \u201cvirtual\n    tables\u201d, using enforced where clauses.\n\n\n\n\n\n\nAccess restriction in TorQ can be enabled on all processes, each of\nwhich can then load the default.q in $KDBCONFIG/permissions/, which\nadds users, groups and roles allowing standard operation of TorQ. The\nadmin user and role by default can access all functions, and each of the\nsystem processes has access only to the required system functions.\n\n\nPermissions are enabled or disabled on a per-process basis through\nsetting .pm.enabled as 1b or 0b at process load (set to 0b by default).\nA permissioned process can safely interact with a non-permissioned\nprocess while still controlling access to itself.\n\n\nThe access schema consists of 7 control tables:\n\n\n\n\n\n\n\n\nName\n\n\nDescriptions\n\n\n\n\n\n\n\n\n\n\nUser\n\n\nUsername, locality, encryption type and password hash\n\n\n\n\n\n\nUsergroup\n\n\nUser and their group.\n\n\n\n\n\n\nUserrole\n\n\nUser and role.\n\n\n\n\n\n\nFunctiongroup\n\n\nFunctions and their group\n\n\n\n\n\n\nFunction\n\n\nFunction names, the roles which can access them, and a lambda checking the parameters those roles can use.\n\n\n\n\n\n\nAccess\n\n\nVariable names, the groups which can access them, and the read or write access level.\n\n\n\n\n\n\nVirtualtable\n\n\nVirtual table name, main table name, and the where clause it enforces on access to that table.\n\n\n\n\n\n\n\n\nIn addition to groupinfo and roleinfo tables, which contain the\ngroup/role name and a string describing each group and role. A user can\nbelong to multiple groups, and have multiple roles. In particular the\nschema supports group hierarchy, where a user group can be listed as a\nuser in the group table, and inherit all the permissions from another\nother group, effectively inheriting the second group itself.\n\n\nA user belonging to a group listed in the access table will have the\nspecified level of access (read or write) to that group\u2019s variables,\ne.g.\n\n\n\n\n\n\n\n\nTable\n\n\nGroup\n\n\nLevel\n\n\n\n\n\n\n\n\n\n\nquote\n\n\nheadtrader\n\n\nwrite\n\n\n\n\n\n\ntrade\n\n\njuniortrader\n\n\nread\n\n\n\n\n\n\n\n\nHere, users in headtrader will have write access to the quote table,\nwhile juniortrader group has read access to the trade table. If\nheadtraders have been set to inherit the juniortrader group, they will\nalso have read access to trade. Note that read access is distinct from\nwrite access. Headtraders in this circumstance do not have implicit read\naccess to the quote table. This control is for direct name access only.\nSelects, execs and updates are controlled via the function table, as\nbelow.\n\n\nThe permissions script can be set to have permissive mode enabled with\npermissivemode:1b (disabled by default). When enabled at script loading,\nthis bypasses access checks on variables which are not listed in the\naccess table, effectively auto-whitelisting any variables not listed in\nthe access table for all users, which may be useful in partly restricted\ndevelopment environments.\n\n\nFunction access is controlled through non-hierarchical roles. A user\nattempting to run a named function will have their access checked\nagainst the function table through their role, for example, trying to\nrun a function timedata[syms;bkttype], which selects from a table by a\ntime bucket type bkttype on xbar:\n\n\n\n\n\n\n\n\nFunction\n\n\nRole\n\n\nParam. Check\n\n\n\n\n\n\n\n\n\n\ntimedata\n\n\nquant\n\n\n{1b}\n\n\n\n\n\n\ntimedata\n\n\nnormal user\n\n\n{x[`bkttype] in `hh}\n\n\n\n\n\n\nselect\n\n\nquant\n\n\n{1b}\n\n\n\n\n\n\n\n\nThe parameter check in the third column must be a lambda accepting a\ndictionary of parameters and their values, which can then return a\nboolean if some parameter condition is met. Here, any normal user must\nhave their bucket type as an hour. If they try anything else, the\nfunction is not permitted. This could be extended to restriction to\ncertain syms as well, in this example, the quant can run this function\nwith any parameters. Anything passed to the param. check function\nreturns 1b. A quant having general select access is listed as having\n1b in the param. check.\n\n\nFurther restriction of data can be achieved with virtual tables, via\nwhich users can be restricted to having a certain subset of data from a\nmain table available. To avoid the need to replicate a potentially large\nsubset of a table into a separately-controlled variable, this is done\nthrough pointing to the table under a different name via a where clause,\ne.g.\n\n\n\n\n\n\n\n\nVirtual Table\n\n\nTable\n\n\nWhere Clause\n\n\n\n\n\n\n\n\n\n\ntrade_lse\n\n\ntrade\n\n\n,(in;`src;\u201cL\u201d)\n\n\n\n\n\n\nquote_new\n\n\nquote\n\n\n,(\n;`time;(-;`.z.p;01:00))\n\n\n\n\n\n\n\n\nWhen a select from trade_lse is performed, a select on trade is\nmodified to contain the where clause above. Access to virtual tables can\nbe controlled identically to access to real tables through the access\ntable.\n\n\nIf the process is given the flag \u201c-public 1\u201d, it will run in public\naccess mode. This allows a user to log in without a password and be\ngiven the publicuser role and membership of the public group, which can\nbe configured as any other group or role.\n\n\nThe permissions control has a default size restriction of 2GB, set (as\nbytes) on .pm.maxsize. This is a global restriction and is not affected\nby user permissions.\n\n\nAdding to the groups and roles is handled by the functions:\n\n\nadduser[`user;`locality;`hash type; md5\"password\"]\nremoveuser[`user]\naddgroup[`groupname; \"description\"]\nremovegroup[`groupname]\naddrole[`rolename; \"description\"]\nremoverole[`rolename]\naddtogroup[`user;`groupname]\nremovefromgroup[`user; `groupname]\nassignrole[`user; `rolename]\nunassignrole[`user; `rolename]\naddfunction[`function; `functiongroup]\nremovefunction[`function; `functiongroup]\ngrantaccess[`variable; `groupname; `level]\nrevokeaccess[`variable; `groupname; `level]\ngrantfunction[`function; `rolename; {paramCheckFn}]\nrevokefunction[`function; `rolename]\ncreatevirtualtable[`vtablename; `table; ,(whereclause)]\nremovevirtualtable[`vtablename]\ncloneuser[`user;`newuser;\"password\"]\n\n\n\nwhich are further explained in the script API.\n\n\nPermission control operates identically on the gateway. A user connected\nto the gateway must have access to the gateway, and their roles must\nhave access to the .gw.syncexec or .gw.asyncexec functions.\n\n\nUsage Example\n\n\nTo connect to a permissioned RDB in the TorQ system, a group and role\nfor the user must be established. If the RDB contains the tables trade,\nquote, and depth, and the process contains the functions getdata[syms,\nbkttype,bktsize] and hloc[table], restricted access would be\nconfigured like so:\n\n\n.pm.adduser[`adam;`local;`md5;md5\"pass\"]\n.pm.adduser[`bob;`local;`md5;md5\"pass\"]\n\n.pm.addtogroup[`adam;`fulluser]\n.pm.addtogroup[`bob;`partuser]\n.pm.addtogroup[`fulluser;`partuser]\n.pm.grantaccess[`quote;`fulluser;`read]\n.pm.grantaccess[`trade;`partuser;`read]\n\n.pm.createvirtualtable[`quotenew;`quote;enlist(\n;`time;(-;`.z.p;01:00))]\n.pm.grantaccess[`quotenew;`partuser;`read]\n\n.pm.assignrole[`adam;`toplevel]\n.pm.assignrole[`bob;`lowlevel]\n.pm.grantfunction[`getdata;`toplevel;{1b}]\n.pm.grantfunction[`getdata;`lowlevel;{x[`syms] in `GOOG}]\n.pm.grantfunction[`hloc;`toplevel;{1b}]\n.pm.grantfunction[`hloc;`lowlevel;{x[`table] in `trade}]\n\n\n\nThis provides a system in which Bob can access only the trade table,\nwhile Adam has access to the trade table and quote table (through\ninheritance from Bob\u2019s group). Through a virtual table, if Bob runs\n\u201cselect from quotenew\u201d, he is able to get a table of the last hour of\nquotes. When the system is started in normal mode, there is no IPC\naccess to the depth table, however if the system was started in\npermissive mode, in this case any user who could log in could access\ndepth.\n\n\nAdam can run the getdata function however he wants, and Bob can only run\nit against sym GOOG. Similarly Adam can run hloc against any table, but\nBob can only look at trade with it.\n\n\nAdditionally, any system calls would need to be actively permissioned in\nthe same way, after defining a systemuser role (or expanding the default\nrole in TorQ). The superuser is given global function access by\nassigning them .pm.ALL in the function table, for example a tickerplant\npushing to the RDB would need to have a user and role defined:\n\n\n.pm.adduser[`ticker;`local;`md5;md5\"plant\"]\n.pm.assignrole[`ticker;`tp]\n\n\n\nAnd then grant that role access to the .u.upd function:\n\n\n.pm.grantfunction[`.u.upd;`tp;{1b}]\n\n\n\nAlthough the .u.upd function updates to a table, there is no need to\ngrant direct access to that table.\n\n\nGateway Example\n\n\nThe gateway user will have superuser role by default. The execution of a\nfunction passed through the gateway is checked against the user who sent\nthe call. This should not be modified.\n\n\nWithin the gateway itself, access to target processes can be controlled\nvia the function table. For example, if Adam in the previous example was\nallowed to access only the RDB with .gw.syncexec, you could use:\n\n\n.pm.grantfunction[`.gw.syncexec;`toplevel;{x[`1] in `rdb}]\n\n\n\nSince .gw.syncexec is a projection, the arguments supplied are checked\nin order, with dictionary keys `0`1`2... etc. This could be further\nextended to restrict access to queries with the\n.pm.allowed[user;query] function, which checks permissions of the\ncurrent user as listed on the gateway permission tables:\n\n\n.pm.grantfunction[`.gw.syncexec;`toplevel;\n    {.pm.allowed[.z.u;x[`0]] and x[`1] in `rdb}]\n\n\n\n\n\nldap.q\n\n\nAuthentication with an ldap server is managed with ldap.q. It allows:\n\n\n\n\n\n\nA user to authenticate against an ldap server;\n\n\n\n\n\n\nCaching of user attempts to allow reauthentication without server if within checktime period;\n\n\n\n\n\n\nUsers to be blocked if too many failed authentication attempts are made.\n\n\n\n\n\n\nDefault parameters in the ldap namespace are set in {TORQHOME}/config/settings/default.q.\n\n\n\n\n\n\n\n\nparameter\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nWhether ldap authentication is enabled\n\n\n\n\n\n\ndebug\n\n\nWhether logging message are written to console\n\n\n\n\n\n\nserver\n\n\nHost for ldap server.\n\n\n\n\n\n\nport\n\n\nPort number for ldap server.\n\n\n\n\n\n\nversion\n\n\nLdap version number.\n\n\n\n\n\n\nblocktime\n\n\nTime that must elapse before a blocked user can attempt to authenticate. If set to 0Np then the user is permanently blocked until an admin unblocks them.\n\n\n\n\n\n\nchecklimit\n\n\nLogin attempts before user is blocked.\n\n\n\n\n\n\nchecktime\n\n\nPeriod of time that allows user to reauthenticate without confirming with ldap server.\n\n\n\n\n\n\nbuildDNsuf\n\n\nSuffix for building distinguished name.\n\n\n\n\n\n\nbuildDN\n\n\nFunction to build distiniguished name.\n\n\n\n\n\n\n\n\nTo get started the following will need altered from their default values: enabled, port, server, buildDNsuf.\n\n\nThe value buildDNsuf is required to build a users bind_dn from the supplied username and is called by the function buildDN. An example definition is:\n\n\n.ldap.buildDNsuf:\"ou=users,dc=website,dc=com\";\n\n\n\nAuthentication is handled by .ldap.authenticate which is wrapped by .ldap.login, which is in turn wrapped by .z.pw when ldap authentication is enabled. When invoked .ldap.login retrieves the users latest authentication attempt from the cache, if it exists, and performs several checks before authenticating the user.\n\n\nTo authenticate the function first checks whether the user has been blocked by reaching the checklimit and blocktime has not passed, immediately returning false if this is the case. If the user has previously successfully authenticated within the period defined by checktime and is using the same credentials authentication will be permitted. For all other cases an authentication attempt will be made against the ldap server. \n\n\nExample authentication attempt:\n\n\n.ldap.login[`user;pass]\n0b\n\n\n\nTo manually unblock a user the function .ldap.unblock must be passed their userame as a symbol. The function checks the cache to see whether a user is blocked and will reset the blocked status if necessary. An example usage of this function is:\n\n\n.ldap.unblock[`user]\n\n\n\n\n\nDiagnostic Reporting\n\n\nThe message handler modifications provide a wealth of diagnostic\ninformation including:\n\n\n\n\n\n\nthe timings and memory usage for every query run on a process;\n\n\n\n\n\n\nfailed queries;\n\n\n\n\n\n\nclients trying to do things they are not permissioned for;\n\n\n\n\n\n\nthe clients which are querying often and/or regularly extracting\n      large datasets;\n\n\n\n\n\n\nthe number of clients currently connected;\n\n\n\n\n\n\ntimer calls and how long they take.\n\n\n\n\n\n\nAlthough not currently implemented, it would be straightforward to use\nthis information to implement reports on the behaviour of each process\nand the overall health of the system. Similarly it would be\nstraightforward to set up periodic publication to a central repository\nto have a single point for system diagnostic statistics.", 
            "title": "Handlers"
        }, 
        {
            "location": "/handlers/#message-handlers", 
            "text": "There is a separate code directory containing message handler\ncustomizations. This is found at $KDBCODE/handlers. Much of the code is\nderived from Simon Garland\u2019s contributions to code.kx .  Every external interaction with a process goes through a message\nhandler, and these can be modified to, for example, log or restrict\naccess. Passing through a bespoke function defined in a message handler\nwill add extra processing time and therefore latency to the message. All\nthe customizations we have provided aim to minimise additional latency,\nbut if a bespoke process is latency sensitive then some or all of the\ncustomizations could be switched off. We would argue though that\ngenerally it is better to switch on all the message handler functions\nwhich provide diagnostic information, as for most non-latency sensitive\nprocesses (HDBs, Gateways, some RDBs etc.) the extra information upon\nfailure is worth the cost. The message handlers can be globally switched\noff by setting .proc.loadhandlers to 0b in the configuration file.     Script  NS  Diag  Function  Modifies      logusage.q  .usage  Y  Log all client interaction to an ascii log file and/or in-memory table. Messages can be logged before and after they are processed. Timer calls are also logged. Exclusion function list can be applied to .z.ps to disable logging of asynchronous real time updates  pw, po, pg, ps, pc, ws, ph, pp, pi, exit, timer    controlaccess.q  .access  N  Restrict access for set of users/user groups to a list of functions, and from a defined set of servers  pw, pg, ps, ws, ph, pp, pi    trackclients.q  .clients  Y  Track client process details including then number of requests and cumulative data size returned  po, pg, ps, ws, pc    trackservers.q  .servers  Y  Discover and track server processes including name, type and attribute information. This also contains the core of the code which can be used in conjunction with the discovery service.  pc, timer    zpsignore.q  .zpsignore  N  Override async message handler based on certain message patterns  ps    writeaccess.q  .readonly  N  Restrict client write access to prevent any modification to data in place. Also disables all HTTP access.  pg, ps, ws, ph, pp    ldap.q  .ldap  N  Restrict client access to process using ldap authentication.  pw     Each customization can be turned on or off individually from the\nconfiguration file(s). Each script can be extensively customised using\nthe configuration file. Example customization for logusage.q, taken from\n$KDBCONFIG/settings/default.q is below. Please see default.q for the\nremaining configuration of the other message handler files.  /- Configuration used by the usage functions - logging of client interaction\n\\d .usage\nenabled:1b      /- whether the usage logging is enabled\nlogtodisk:1b        /- whether to log to disk or not\nlogtomemory:1b      /- write query logs to memory\nignore:1b       /- check the ignore list for functions to ignore\nignorelist:(`upd;\"upd\") /- the list of functions to ignore in async calls\nflushtime:1D00      /- default value for how long to persist the\n            /- in-memory logs. Set to 0D for no flushing\nsuppressalias:0b    /- whether to suppress the log file alias creation\nlogtimestamp:{[].z.d}   /- function to generate the log file timestamp suffix\nLEVEL:3         /- log level. 0=none;1=errors;2=errors+complete\n            /- queries;3=errors+before a query+after\nlogroll:1b      /- Whether or not to roll the log file\n            /- automatically (on a daily schedule)", 
            "title": "Message Handlers"
        }, 
        {
            "location": "/handlers/#dotzq", 
            "text": "Stores all the default values for the message handlers and can be used to revert back to the default if necessary.", 
            "title": "dotz.q"
        }, 
        {
            "location": "/handlers/#logusageq", 
            "text": "logusage.q is probably the most important of the scripts from a\ndiagnostic perspective. It is a modified version of the logusage.q\nscript on code.kx.  In its most verbose mode it will log information to an in-memory table\n(.usage.usage) and an on-disk ASCII file, both before and after every\nclient interaction and function executed on the timer. These choices\nwere made because:    logging to memory enables easy interrogation of client interaction;    logging to disk allows persistence if the process fails or locks up.\n      ASCII text files allow interrogation using OS tools such as vi, grep\n      or tail;    logging before a query ensures any query that adversely effects the\n      process is definitely captured, as well as capturing some state\n      information before the query execution;    logging after a query captures the time taken, result set size and\n      resulting state;    logging timer calls ensures a full history of what the process is\n      actually doing. Also, timer call performance degradation over time\n      is a common source of problems in kdb+ systems.    The following fields are logged in .usage.usage:     Field  Description      time  Time the row was added to the table    id  ID of the query. Normally before and complete rows will be consecutive but it might not be the case if the incoming call invokes further external communication    timer  Execution time. Null for rows with status=b (before)    zcmd  .z handler the query arrived through    status  Query status. One of b, c or e (before, complete, error)    a  Address of sender. .dotz.ipa can be used to convert from the integer format to a hostname    u  Username of sender    w  Handle of sender    cmd  Command sent    mem  Memory statistics    sz  Size of result. Null for rows with status of b or e    error  Error message", 
            "title": "logusage.q"
        }, 
        {
            "location": "/handlers/#controlaccessq", 
            "text": "controlaccess.q is used to restrict client access to the process. It is\nmodified version of controlaccess.q from code.kx. The script allows\ncontrol of several aspects:    the host/ip address of the servers which are allowed to access the\n    process;    definition of three user groups (default, poweruser and superuser)\n      and the actions each group is allowed to do;    the group(s) each user is a member of, and any additional actions an\n      individual user is allowed/disallowed outside of the group\n      permissions;    the maximum size of the result set returned to a client.    The access restrictions are loaded from csv files. The permissions files\nare stored in $KDBCONFIG/permissions.     File  Description      *_hosts.csv  Contains hostname and ip address (patterns) for servers which are allowed or disallowed access. If a server is not found in the list, it is disallowed    *_users.csv  Contains individual users and the user groups they are are a member of    *_functions.csv  Contains individual functions and whether each user group is allowed to execute them. ; separated user list enables functions to be allowed by individual users     The permissions files are loaded using a similar hierarchical approach\nas for the configuration and code loading. Three files can be provided-\ndefault_.csv, [proctype]_.csv, and [procname]_.csv. All of the\nfiles will be loaded, but permissions for the same entity (hostpattern,\nuser, or function) defined in [procname]_.csv will override those in\n[proctype]_.csv which will in turn override [procname]_.csv.  When a client makes a query which is refused by the permissioning layer,\nan error will be raised and logged in .usuage.usage if it is enabled.", 
            "title": "controlaccess.q"
        }, 
        {
            "location": "/handlers/#trackclientsq", 
            "text": "trackclients.q is used to track client interaction. It is a slightly\nmodified version of trackclients.q from code.kx, and extends the\nfunctionality to handle interaction with the discovery service.  Whenever a client opens a connection to the q process, it will be\nregistered in the .clients.clients table. Various details are logged,\nbut from a diagnostic perspective the most important information are the\nclient details, the number of queries it has run, the last time it ran a\nquery, the number of failed queries and the cumulative size of results\nreturned to it.", 
            "title": "trackclients.q"
        }, 
        {
            "location": "/handlers/#trackserversq", 
            "text": "trackservers.q is used to register and maintain handles to external\nservers. It is a heavily modified version of trackservers.q from\ncode.kx. It is explained more in section\u00a0connectionmanagement.", 
            "title": "trackservers.q"
        }, 
        {
            "location": "/handlers/#zpsignoreq", 
            "text": "zpsignore.q is used to check incoming async calls for certain patterns\nand to bypass all further message handler checks for messages matching\nthe pattern. This is useful for handling update messages published to a\nprocess from a data source.", 
            "title": "zpsignore.q"
        }, 
        {
            "location": "/handlers/#writeaccessq", 
            "text": "writeaccess.q is used to restrict client write access to data within a\nprocess. The script uses the reval function, released in KDB+ 3.3, to\nprevent client queries from modifying any data in place. At present only\nqueries in the form of strings are passed through the reval function.\nAdditonally the script disables any form of HTTP access. If using\nversions of KDB+ prior to 3.3, this feature must be disabled. An attempt\nto use this feature on previous KDB+ versions will result in an error\nand the relevant process exiting.", 
            "title": "writeaccess.q"
        }, 
        {
            "location": "/handlers/#permissionsq", 
            "text": "permissions.q is used to control client access to a server process. It\nallows:    Access control via username/password access, either in combination\n    with the -u/U process flags or in place of them.    Definition of user groups, which control variable access.    Definition of user roles, which allow control over function\n    execution.    Deeper control over table subsetting through the use of \u201cvirtual\n    tables\u201d, using enforced where clauses.    Access restriction in TorQ can be enabled on all processes, each of\nwhich can then load the default.q in $KDBCONFIG/permissions/, which\nadds users, groups and roles allowing standard operation of TorQ. The\nadmin user and role by default can access all functions, and each of the\nsystem processes has access only to the required system functions.  Permissions are enabled or disabled on a per-process basis through\nsetting .pm.enabled as 1b or 0b at process load (set to 0b by default).\nA permissioned process can safely interact with a non-permissioned\nprocess while still controlling access to itself.  The access schema consists of 7 control tables:     Name  Descriptions      User  Username, locality, encryption type and password hash    Usergroup  User and their group.    Userrole  User and role.    Functiongroup  Functions and their group    Function  Function names, the roles which can access them, and a lambda checking the parameters those roles can use.    Access  Variable names, the groups which can access them, and the read or write access level.    Virtualtable  Virtual table name, main table name, and the where clause it enforces on access to that table.     In addition to groupinfo and roleinfo tables, which contain the\ngroup/role name and a string describing each group and role. A user can\nbelong to multiple groups, and have multiple roles. In particular the\nschema supports group hierarchy, where a user group can be listed as a\nuser in the group table, and inherit all the permissions from another\nother group, effectively inheriting the second group itself.  A user belonging to a group listed in the access table will have the\nspecified level of access (read or write) to that group\u2019s variables,\ne.g.     Table  Group  Level      quote  headtrader  write    trade  juniortrader  read     Here, users in headtrader will have write access to the quote table,\nwhile juniortrader group has read access to the trade table. If\nheadtraders have been set to inherit the juniortrader group, they will\nalso have read access to trade. Note that read access is distinct from\nwrite access. Headtraders in this circumstance do not have implicit read\naccess to the quote table. This control is for direct name access only.\nSelects, execs and updates are controlled via the function table, as\nbelow.  The permissions script can be set to have permissive mode enabled with\npermissivemode:1b (disabled by default). When enabled at script loading,\nthis bypasses access checks on variables which are not listed in the\naccess table, effectively auto-whitelisting any variables not listed in\nthe access table for all users, which may be useful in partly restricted\ndevelopment environments.  Function access is controlled through non-hierarchical roles. A user\nattempting to run a named function will have their access checked\nagainst the function table through their role, for example, trying to\nrun a function timedata[syms;bkttype], which selects from a table by a\ntime bucket type bkttype on xbar:     Function  Role  Param. Check      timedata  quant  {1b}    timedata  normal user  {x[`bkttype] in `hh}    select  quant  {1b}     The parameter check in the third column must be a lambda accepting a\ndictionary of parameters and their values, which can then return a\nboolean if some parameter condition is met. Here, any normal user must\nhave their bucket type as an hour. If they try anything else, the\nfunction is not permitted. This could be extended to restriction to\ncertain syms as well, in this example, the quant can run this function\nwith any parameters. Anything passed to the param. check function\nreturns 1b. A quant having general select access is listed as having\n1b in the param. check.  Further restriction of data can be achieved with virtual tables, via\nwhich users can be restricted to having a certain subset of data from a\nmain table available. To avoid the need to replicate a potentially large\nsubset of a table into a separately-controlled variable, this is done\nthrough pointing to the table under a different name via a where clause,\ne.g.     Virtual Table  Table  Where Clause      trade_lse  trade  ,(in;`src;\u201cL\u201d)    quote_new  quote  ,( ;`time;(-;`.z.p;01:00))     When a select from trade_lse is performed, a select on trade is\nmodified to contain the where clause above. Access to virtual tables can\nbe controlled identically to access to real tables through the access\ntable.  If the process is given the flag \u201c-public 1\u201d, it will run in public\naccess mode. This allows a user to log in without a password and be\ngiven the publicuser role and membership of the public group, which can\nbe configured as any other group or role.  The permissions control has a default size restriction of 2GB, set (as\nbytes) on .pm.maxsize. This is a global restriction and is not affected\nby user permissions.  Adding to the groups and roles is handled by the functions:  adduser[`user;`locality;`hash type; md5\"password\"]\nremoveuser[`user]\naddgroup[`groupname; \"description\"]\nremovegroup[`groupname]\naddrole[`rolename; \"description\"]\nremoverole[`rolename]\naddtogroup[`user;`groupname]\nremovefromgroup[`user; `groupname]\nassignrole[`user; `rolename]\nunassignrole[`user; `rolename]\naddfunction[`function; `functiongroup]\nremovefunction[`function; `functiongroup]\ngrantaccess[`variable; `groupname; `level]\nrevokeaccess[`variable; `groupname; `level]\ngrantfunction[`function; `rolename; {paramCheckFn}]\nrevokefunction[`function; `rolename]\ncreatevirtualtable[`vtablename; `table; ,(whereclause)]\nremovevirtualtable[`vtablename]\ncloneuser[`user;`newuser;\"password\"]  which are further explained in the script API.  Permission control operates identically on the gateway. A user connected\nto the gateway must have access to the gateway, and their roles must\nhave access to the .gw.syncexec or .gw.asyncexec functions.", 
            "title": "permissions.q"
        }, 
        {
            "location": "/handlers/#usage-example", 
            "text": "To connect to a permissioned RDB in the TorQ system, a group and role\nfor the user must be established. If the RDB contains the tables trade,\nquote, and depth, and the process contains the functions getdata[syms,\nbkttype,bktsize] and hloc[table], restricted access would be\nconfigured like so:  .pm.adduser[`adam;`local;`md5;md5\"pass\"]\n.pm.adduser[`bob;`local;`md5;md5\"pass\"]\n\n.pm.addtogroup[`adam;`fulluser]\n.pm.addtogroup[`bob;`partuser]\n.pm.addtogroup[`fulluser;`partuser]\n.pm.grantaccess[`quote;`fulluser;`read]\n.pm.grantaccess[`trade;`partuser;`read]\n\n.pm.createvirtualtable[`quotenew;`quote;enlist( ;`time;(-;`.z.p;01:00))]\n.pm.grantaccess[`quotenew;`partuser;`read]\n\n.pm.assignrole[`adam;`toplevel]\n.pm.assignrole[`bob;`lowlevel]\n.pm.grantfunction[`getdata;`toplevel;{1b}]\n.pm.grantfunction[`getdata;`lowlevel;{x[`syms] in `GOOG}]\n.pm.grantfunction[`hloc;`toplevel;{1b}]\n.pm.grantfunction[`hloc;`lowlevel;{x[`table] in `trade}]  This provides a system in which Bob can access only the trade table,\nwhile Adam has access to the trade table and quote table (through\ninheritance from Bob\u2019s group). Through a virtual table, if Bob runs\n\u201cselect from quotenew\u201d, he is able to get a table of the last hour of\nquotes. When the system is started in normal mode, there is no IPC\naccess to the depth table, however if the system was started in\npermissive mode, in this case any user who could log in could access\ndepth.  Adam can run the getdata function however he wants, and Bob can only run\nit against sym GOOG. Similarly Adam can run hloc against any table, but\nBob can only look at trade with it.  Additionally, any system calls would need to be actively permissioned in\nthe same way, after defining a systemuser role (or expanding the default\nrole in TorQ). The superuser is given global function access by\nassigning them .pm.ALL in the function table, for example a tickerplant\npushing to the RDB would need to have a user and role defined:  .pm.adduser[`ticker;`local;`md5;md5\"plant\"]\n.pm.assignrole[`ticker;`tp]  And then grant that role access to the .u.upd function:  .pm.grantfunction[`.u.upd;`tp;{1b}]  Although the .u.upd function updates to a table, there is no need to\ngrant direct access to that table.", 
            "title": "Usage Example"
        }, 
        {
            "location": "/handlers/#gateway-example", 
            "text": "The gateway user will have superuser role by default. The execution of a\nfunction passed through the gateway is checked against the user who sent\nthe call. This should not be modified.  Within the gateway itself, access to target processes can be controlled\nvia the function table. For example, if Adam in the previous example was\nallowed to access only the RDB with .gw.syncexec, you could use:  .pm.grantfunction[`.gw.syncexec;`toplevel;{x[`1] in `rdb}]  Since .gw.syncexec is a projection, the arguments supplied are checked\nin order, with dictionary keys `0`1`2... etc. This could be further\nextended to restrict access to queries with the\n.pm.allowed[user;query] function, which checks permissions of the\ncurrent user as listed on the gateway permission tables:  .pm.grantfunction[`.gw.syncexec;`toplevel;\n    {.pm.allowed[.z.u;x[`0]] and x[`1] in `rdb}]", 
            "title": "Gateway Example"
        }, 
        {
            "location": "/handlers/#ldapq", 
            "text": "Authentication with an ldap server is managed with ldap.q. It allows:    A user to authenticate against an ldap server;    Caching of user attempts to allow reauthentication without server if within checktime period;    Users to be blocked if too many failed authentication attempts are made.    Default parameters in the ldap namespace are set in {TORQHOME}/config/settings/default.q.     parameter  description      enabled  Whether ldap authentication is enabled    debug  Whether logging message are written to console    server  Host for ldap server.    port  Port number for ldap server.    version  Ldap version number.    blocktime  Time that must elapse before a blocked user can attempt to authenticate. If set to 0Np then the user is permanently blocked until an admin unblocks them.    checklimit  Login attempts before user is blocked.    checktime  Period of time that allows user to reauthenticate without confirming with ldap server.    buildDNsuf  Suffix for building distinguished name.    buildDN  Function to build distiniguished name.     To get started the following will need altered from their default values: enabled, port, server, buildDNsuf.  The value buildDNsuf is required to build a users bind_dn from the supplied username and is called by the function buildDN. An example definition is:  .ldap.buildDNsuf:\"ou=users,dc=website,dc=com\";  Authentication is handled by .ldap.authenticate which is wrapped by .ldap.login, which is in turn wrapped by .z.pw when ldap authentication is enabled. When invoked .ldap.login retrieves the users latest authentication attempt from the cache, if it exists, and performs several checks before authenticating the user.  To authenticate the function first checks whether the user has been blocked by reaching the checklimit and blocktime has not passed, immediately returning false if this is the case. If the user has previously successfully authenticated within the period defined by checktime and is using the same credentials authentication will be permitted. For all other cases an authentication attempt will be made against the ldap server.   Example authentication attempt:  .ldap.login[`user;pass]\n0b  To manually unblock a user the function .ldap.unblock must be passed their userame as a symbol. The function checks the cache to see whether a user is blocked and will reset the blocked status if necessary. An example usage of this function is:  .ldap.unblock[`user]", 
            "title": "ldap.q"
        }, 
        {
            "location": "/handlers/#diagnostic-reporting", 
            "text": "The message handler modifications provide a wealth of diagnostic\ninformation including:    the timings and memory usage for every query run on a process;    failed queries;    clients trying to do things they are not permissioned for;    the clients which are querying often and/or regularly extracting\n      large datasets;    the number of clients currently connected;    timer calls and how long they take.    Although not currently implemented, it would be straightforward to use\nthis information to implement reports on the behaviour of each process\nand the overall health of the system. Similarly it would be\nstraightforward to set up periodic publication to a central repository\nto have a single point for system diagnostic statistics.", 
            "title": "Diagnostic Reporting"
        }, 
        {
            "location": "/conn/", 
            "text": "Connection Management\n\n\ntrackservers.q is used to register and maintain handles to external\nservers. It is a heavily modified version of trackservers.q from\ncode.kx. All the options are described in the default config file. All\nconnections are tracked in the .servers.SERVERS table. When the handle\nis used the count and last query time are updated.\n\n\nq).servers.SERVERS \nprocname     proctype  hpup                            w  hits startp                        lastp                         endp                          attributes                   \n---------------------------------------------------------------------------------\ndiscovery1   discovery :aquaq:9996    0                                  2014.01.08D11:13:10.583056000                               ()!()                        \ndiscovery2   discovery :aquaq:9995 6  0    2014.01.07D16:44:47.175757000 2014.01.07D16:44:47.174408000                               ()!()                        \nrdb_europe_1 rdb       :aquaq:9998 12 0    2014.01.07D16:46:47.897910000 2014.01.07D16:46:47.892901000 2014.01.07D16:46:44.626293000 `datacentre`country!`essex`uk\nrdb1         rdb       :aquaq:5011 7  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\nrdb_europe_1 hdb       :aquaq:9997    0                                  2014.01.08D11:13:10.757801000                               ()!()                        \nhdb1         hdb       :aquaq:9999    0                                  2014.01.08D11:13:10.757801000                               ()!()                        \nhdb2         hdb       :aquaq:5013 8  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\nhdb1         hdb       :aquaq:5012 9  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\n\nq)last .servers.SERVERS \nprocname  | `hdb2\nproctype  | `hdb\nhpup      | `:aquaq:5013\nw         | 8i\nhits      | 0i\nstartp    | 2014.01.08D11:51:01.928045000\nlastp     | 2014.01.08D11:51:01.925078000\nendp      | 0Np\nattributes| `datacentre`country!`essex`uk\n\n\n\nConnections\n\n\nProcesses locate other processes based on their process type. The\nlocation is done either statically using the process.csv file or\ndynamically using a discovery service. It is recommended to use the\ndiscovery service as it allows the process to be notified as new\nprocesses become available.\n\n\nThe main configuration variable is .servers.CONNECTIONS, which dictates\nwhich process type(s) to create connections to. .servers.startup[]\nmust be called to initialise the connections. When connections are\nclosed, the connection table is automatically updated. The process can\nbe set to periodically retry connections.\n\n\nProcess Attributes\n\n\nEach process can report a set of attributes. When process A connects to\nprocess B, process A will try to retrieve the attributes of process B.\nThe attributes are defined by the result of the .proc.getattributes\nfunction, which is by default an empty dictionary. Attributes are used\nto retrieve more detail about the capabilities of each process, rather\nthan relying on the broad brush process type and process name\ncategorization. Attributes can be used for intelligent query routing.\nPotential fields for attributes include:\n\n\n\n\n\n\nrange of data contained in the process;\n\n\n\n\n\n\navailable tables;\n\n\n\n\n\n\ninstrument universe;\n\n\n\n\n\n\nphysical location;\n\n\n\n\n\n\nany other fields of relevance.\n\n\n\n\n\n\nConnection Passwords\n\n\nThe password used by a process to connect to external processes is\nretrieved using the .servers.loadpassword function call. By default,\nthis will read the password from a txt file contained in\n$KDBCONFIG/passwords. A default password can be used, which is\noverridden by one for the process type, which is itself overridden by\none for the process name. For greater security, the\n.servers.loadpassword function should be modified.\n\n\nSome non-torq processes require a username and password to allow connection. \nThese will be stored in a passwords dictionary. \nPassing the host and port of a process into this dictionary will return the full connection string \nif it is present within the dictionary. \nIf however it is not present in the dictionary then the default username and password will be returned.\n\n\nRetrieving and Using Handles\n\n\nA function .servers.getservers is supplied to return a table of handle\ninformation. .servers.getservers takes five parameters:\n\n\n\n\n\n\ntype-or-name: whether the lookup is to be done by type or name (can\n    be either proctype or procname);\n\n\n\n\n\n\ntypes-or-names: the types or names to retrieve e.g. hdb;\n\n\n\n\n\n\nrequired-attributes: the dictionary of attributes to match on;\n\n\n\n\n\n\nopen-dead-connections: whether to re-open dead connections;\n\n\n\n\n\n\nonly-one: whether we only require one handle. So for example if 3\n      services of the supplied type are registered, and we have an open\n      handle to 1 of them, the open handle will be returned and the others\n      left closed irrespective of the open-dead-connections parameter.\n\n\n\n\n\n\n.servers.getservers will compare the required parameters with the\navailable parameters for each handle. The resulting table will have an\nextra column called attribmatch which can be used to determine how good\na match the service is with the required attributes. attribmatch is a\ndictionary of (required attribute key) ! (Boolean full match;\nintersection of attributes).\n\n\nq).servers.SERVERS \nprocname     proctype  hpup                            w hits startp                        lastp                         endp attributes                   \n---------------------------------------------------------------------------------\ndiscovery1   discovery :aquaq:9996   0                                  2014.01.08D11:51:01.922390000      ()!()                        \ndiscovery2   discovery :aquaq:9995 6 0    2014.01.08D11:51:01.923812000 2014.01.08D11:51:01.922390000      ()!()                        \nrdb_europe_1 rdb       :aquaq:9998   0                                  2014.01.08D11:51:38.347598000      ()!()                        \nrdb_europe_2 rdb       :aquaq:9997   0                                  2014.01.08D11:51:38.347598000      ()!()                        \nrdb1         rdb       :aquaq:5011 7 0    2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000      `datacentre`country!`essex`uk\nhdb3         hdb       :aquaq:5012 9 0    2014.01.08D11:51:38.349472000 2014.01.08D11:51:38.347598000      `datacentre`country!`essex`uk\nhdb2         hdb       :aquaq:5013 8 0    2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000      `datacentre`country!`essex`uk\n\n/- pull back hdbs.  Leave the attributes empty\nq).servers.getservers[`proctype;`hdb;()!();1b;f0b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch\n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk ()!()      \nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk ()!()\n\n/- supply some attributes\nq).servers.getservers[`proctype;`hdb;(enlist`country)!enlist`uk;1b;0b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch           \n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk)\nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk)\nq).servers.getservers[`proctype;`hdb;`country`datacentre!`uk`slough;1b;0b]                                                                                                                                                                                                    \nprocname proctype lastp                         w hpup        attributes                    attribmatch                                    \n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$()))\nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$()))\n\n\n\n.servers.getservers will try to automatically re-open connections if\nrequired.\n\n\nq).servers.getservers[`proctype;`rdb;()!();1b;0b] \n2014.01.08D12:01:06.023146000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9998\n2014.01.08D12:01:06.023581000|aquaq|gateway1|INF|conn|connection to :aquaq:9998 failed: hop: Connection refused\n2014.01.08D12:01:06.023597000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9997\n2014.01.08D12:01:06.023872000|aquaq|gateway1|INF|conn|connection to :aquaq:9997 failed: hop: Connection refused\nprocname proctype lastp                         w hpup         attributes                    attribmatch\n-------------------------------------------------------------------------------\nrdb1     rdb      2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!()\n\n/- If we only require one connection, and we have one open,then it doesn't retry connections\nq).servers.getservers[`proctype;`rdb;()!();1b;1b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch\n-------------------------------------------------------------------------------\nrdb1     rdb      2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!()\n\n\n\nThere are two other functions supplied for retrieving server details,\nboth of which are based on .servers.getservers. .servers.gethandlebytype\nreturns a single handle value, .servers.gethpupbytype returns a single\nhost:port value. Both will re-open connections if there are not any\nvalid connections. Both take two parameters:\n\n\n\n\n\n\ntypes: the type to retrieve e.g. hdb;\n\n\n\n\n\n\nselection-algorithm: can be one of any, last or roundrobin.\n\n\n\n\n\n\nConnecting To Non-TorQ Processes\n\n\nConnections to non-torq (external) processes can also be established.\nThis is useful if you wish to integrate TorQ with an existing\ninfrastructure. Any process can connect to external processes, or it can\nbe managed by the discovery service only. Every external process should\nhave a type and name in the same way as TorQ processes, to enable them\nto be located and used as required.\n\n\nNon-TorQ processes need to be listed by default in\n$KDBCONFIG/settings/nontorqprocess.csv. This file has the same format\nas the standard process.csv file. The location of the non-TorQ process\nfile can be adjusted using the .servers.NONTORQPROCESSFILE variable. To\nenable connections, set .servers.TRACKNONTORQPROCESS to 1b.\n\n\nExample of nontorqprocess.csv file:\n\n\nhost,port,proctype,procname\naquaq,5533,hdb,extproc01\naquaq,5577,hdb,extproc02\n\n\n\nManually Adding And Using Connections\n\n\nConnections can also be manually added and used. See .api.p\u201c.servers.*\u201d\nfor details.\n\n\nIPC types\n\n\nIn version kdb+ v3.4, two new IPC connection types were added. These new\ntypes are unix domain sockets and SSL/TLS (tcps). The incoming\nconnections to a proctype can be set by updating .servers.SOCKETTYPE.\n\n\nIn the settings example below, everything that connects to the\ntickerplant will use unix domain sockets.\n\n\n\\d .servers \nSOCKETTYPE:enlist[`tickerplant]!enlist `unix\n\n\n\nAttempting to open a unix domain socket connection to a process which\nhas an older kdb+ version will fail. We allow for processes to fallback\nto tcp if this happens by setting .servers.SOCKETFALLBACK to true. It\nwill not fallback if the connection error message returned is one of the\nfollowing : timeout, access. It will also not fallback for SSL/TLS\n(tcps) due to security concerns.\n\n\nAt the time of writing, using unix domain sockets syntax on windows will\nappear to work whilst it\u2019s actually falling back to tcp in the\nbackground. This can be misleading so we disabled using them on windows.", 
            "title": "Connection Management"
        }, 
        {
            "location": "/conn/#connection-management", 
            "text": "trackservers.q is used to register and maintain handles to external\nservers. It is a heavily modified version of trackservers.q from\ncode.kx. All the options are described in the default config file. All\nconnections are tracked in the .servers.SERVERS table. When the handle\nis used the count and last query time are updated.  q).servers.SERVERS \nprocname     proctype  hpup                            w  hits startp                        lastp                         endp                          attributes                   \n---------------------------------------------------------------------------------\ndiscovery1   discovery :aquaq:9996    0                                  2014.01.08D11:13:10.583056000                               ()!()                        \ndiscovery2   discovery :aquaq:9995 6  0    2014.01.07D16:44:47.175757000 2014.01.07D16:44:47.174408000                               ()!()                        \nrdb_europe_1 rdb       :aquaq:9998 12 0    2014.01.07D16:46:47.897910000 2014.01.07D16:46:47.892901000 2014.01.07D16:46:44.626293000 `datacentre`country!`essex`uk\nrdb1         rdb       :aquaq:5011 7  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\nrdb_europe_1 hdb       :aquaq:9997    0                                  2014.01.08D11:13:10.757801000                               ()!()                        \nhdb1         hdb       :aquaq:9999    0                                  2014.01.08D11:13:10.757801000                               ()!()                        \nhdb2         hdb       :aquaq:5013 8  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\nhdb1         hdb       :aquaq:5012 9  0    2014.01.07D16:44:47.180684000 2014.01.07D16:44:47.176994000                               `datacentre`country!`essex`uk\n\nq)last .servers.SERVERS \nprocname  | `hdb2\nproctype  | `hdb\nhpup      | `:aquaq:5013\nw         | 8i\nhits      | 0i\nstartp    | 2014.01.08D11:51:01.928045000\nlastp     | 2014.01.08D11:51:01.925078000\nendp      | 0Np\nattributes| `datacentre`country!`essex`uk", 
            "title": "Connection Management"
        }, 
        {
            "location": "/conn/#connections", 
            "text": "Processes locate other processes based on their process type. The\nlocation is done either statically using the process.csv file or\ndynamically using a discovery service. It is recommended to use the\ndiscovery service as it allows the process to be notified as new\nprocesses become available.  The main configuration variable is .servers.CONNECTIONS, which dictates\nwhich process type(s) to create connections to. .servers.startup[]\nmust be called to initialise the connections. When connections are\nclosed, the connection table is automatically updated. The process can\nbe set to periodically retry connections.", 
            "title": "Connections"
        }, 
        {
            "location": "/conn/#process-attributes", 
            "text": "Each process can report a set of attributes. When process A connects to\nprocess B, process A will try to retrieve the attributes of process B.\nThe attributes are defined by the result of the .proc.getattributes\nfunction, which is by default an empty dictionary. Attributes are used\nto retrieve more detail about the capabilities of each process, rather\nthan relying on the broad brush process type and process name\ncategorization. Attributes can be used for intelligent query routing.\nPotential fields for attributes include:    range of data contained in the process;    available tables;    instrument universe;    physical location;    any other fields of relevance.", 
            "title": "Process Attributes"
        }, 
        {
            "location": "/conn/#connection-passwords", 
            "text": "The password used by a process to connect to external processes is\nretrieved using the .servers.loadpassword function call. By default,\nthis will read the password from a txt file contained in\n$KDBCONFIG/passwords. A default password can be used, which is\noverridden by one for the process type, which is itself overridden by\none for the process name. For greater security, the\n.servers.loadpassword function should be modified.  Some non-torq processes require a username and password to allow connection. \nThese will be stored in a passwords dictionary. \nPassing the host and port of a process into this dictionary will return the full connection string \nif it is present within the dictionary. \nIf however it is not present in the dictionary then the default username and password will be returned.", 
            "title": "Connection Passwords"
        }, 
        {
            "location": "/conn/#retrieving-and-using-handles", 
            "text": "A function .servers.getservers is supplied to return a table of handle\ninformation. .servers.getservers takes five parameters:    type-or-name: whether the lookup is to be done by type or name (can\n    be either proctype or procname);    types-or-names: the types or names to retrieve e.g. hdb;    required-attributes: the dictionary of attributes to match on;    open-dead-connections: whether to re-open dead connections;    only-one: whether we only require one handle. So for example if 3\n      services of the supplied type are registered, and we have an open\n      handle to 1 of them, the open handle will be returned and the others\n      left closed irrespective of the open-dead-connections parameter.    .servers.getservers will compare the required parameters with the\navailable parameters for each handle. The resulting table will have an\nextra column called attribmatch which can be used to determine how good\na match the service is with the required attributes. attribmatch is a\ndictionary of (required attribute key) ! (Boolean full match;\nintersection of attributes).  q).servers.SERVERS \nprocname     proctype  hpup                            w hits startp                        lastp                         endp attributes                   \n---------------------------------------------------------------------------------\ndiscovery1   discovery :aquaq:9996   0                                  2014.01.08D11:51:01.922390000      ()!()                        \ndiscovery2   discovery :aquaq:9995 6 0    2014.01.08D11:51:01.923812000 2014.01.08D11:51:01.922390000      ()!()                        \nrdb_europe_1 rdb       :aquaq:9998   0                                  2014.01.08D11:51:38.347598000      ()!()                        \nrdb_europe_2 rdb       :aquaq:9997   0                                  2014.01.08D11:51:38.347598000      ()!()                        \nrdb1         rdb       :aquaq:5011 7 0    2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000      `datacentre`country!`essex`uk\nhdb3         hdb       :aquaq:5012 9 0    2014.01.08D11:51:38.349472000 2014.01.08D11:51:38.347598000      `datacentre`country!`essex`uk\nhdb2         hdb       :aquaq:5013 8 0    2014.01.08D11:51:01.928045000 2014.01.08D11:51:01.925078000      `datacentre`country!`essex`uk\n\n/- pull back hdbs.  Leave the attributes empty\nq).servers.getservers[`proctype;`hdb;()!();1b;f0b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch\n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk ()!()      \nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk ()!()\n\n/- supply some attributes\nq).servers.getservers[`proctype;`hdb;(enlist`country)!enlist`uk;1b;0b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch           \n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk)\nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk (,`country)!,(1b;,`uk)\nq).servers.getservers[`proctype;`hdb;`country`datacentre!`uk`slough;1b;0b]                                                                                                                                                                                                    \nprocname proctype lastp                         w hpup        attributes                    attribmatch                                    \n-------------------------------------------------------------------------------\nhdb3     hdb      2014.01.08D11:51:38.347598000 9 :aquaq:5012 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$()))\nhdb2     hdb      2014.01.08D11:51:01.925078000 8 :aquaq:5013 `datacentre`country!`essex`uk `country`datacentre!((1b;,`uk);(0b;`symbol$()))  .servers.getservers will try to automatically re-open connections if\nrequired.  q).servers.getservers[`proctype;`rdb;()!();1b;0b] \n2014.01.08D12:01:06.023146000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9998\n2014.01.08D12:01:06.023581000|aquaq|gateway1|INF|conn|connection to :aquaq:9998 failed: hop: Connection refused\n2014.01.08D12:01:06.023597000|aquaq|gateway1|INF|conn|attempting to open handle to :aquaq:9997\n2014.01.08D12:01:06.023872000|aquaq|gateway1|INF|conn|connection to :aquaq:9997 failed: hop: Connection refused\nprocname proctype lastp                         w hpup         attributes                    attribmatch\n-------------------------------------------------------------------------------\nrdb1     rdb      2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!()\n\n/- If we only require one connection, and we have one open,then it doesn't retry connections\nq).servers.getservers[`proctype;`rdb;()!();1b;1b] \nprocname proctype lastp                         w hpup        attributes                    attribmatch\n-------------------------------------------------------------------------------\nrdb1     rdb      2014.01.08D11:51:01.925078000 7 :aquaq:5011 `datacentre`country!`essex`uk ()!()  There are two other functions supplied for retrieving server details,\nboth of which are based on .servers.getservers. .servers.gethandlebytype\nreturns a single handle value, .servers.gethpupbytype returns a single\nhost:port value. Both will re-open connections if there are not any\nvalid connections. Both take two parameters:    types: the type to retrieve e.g. hdb;    selection-algorithm: can be one of any, last or roundrobin.", 
            "title": "Retrieving and Using Handles"
        }, 
        {
            "location": "/conn/#connecting-to-non-torq-processes", 
            "text": "Connections to non-torq (external) processes can also be established.\nThis is useful if you wish to integrate TorQ with an existing\ninfrastructure. Any process can connect to external processes, or it can\nbe managed by the discovery service only. Every external process should\nhave a type and name in the same way as TorQ processes, to enable them\nto be located and used as required.  Non-TorQ processes need to be listed by default in\n$KDBCONFIG/settings/nontorqprocess.csv. This file has the same format\nas the standard process.csv file. The location of the non-TorQ process\nfile can be adjusted using the .servers.NONTORQPROCESSFILE variable. To\nenable connections, set .servers.TRACKNONTORQPROCESS to 1b.  Example of nontorqprocess.csv file:  host,port,proctype,procname\naquaq,5533,hdb,extproc01\naquaq,5577,hdb,extproc02", 
            "title": "Connecting To Non-TorQ Processes"
        }, 
        {
            "location": "/conn/#manually-adding-and-using-connections", 
            "text": "Connections can also be manually added and used. See .api.p\u201c.servers.*\u201d\nfor details.", 
            "title": "Manually Adding And Using Connections"
        }, 
        {
            "location": "/conn/#ipc-types", 
            "text": "In version kdb+ v3.4, two new IPC connection types were added. These new\ntypes are unix domain sockets and SSL/TLS (tcps). The incoming\nconnections to a proctype can be set by updating .servers.SOCKETTYPE.  In the settings example below, everything that connects to the\ntickerplant will use unix domain sockets.  \\d .servers \nSOCKETTYPE:enlist[`tickerplant]!enlist `unix  Attempting to open a unix domain socket connection to a process which\nhas an older kdb+ version will fail. We allow for processes to fallback\nto tcp if this happens by setting .servers.SOCKETFALLBACK to true. It\nwill not fallback if the connection error message returned is one of the\nfollowing : timeout, access. It will also not fallback for SSL/TLS\n(tcps) due to security concerns.  At the time of writing, using unix domain sockets syntax on windows will\nappear to work whilst it\u2019s actually falling back to tcp in the\nbackground. This can be misleading so we disabled using them on windows.", 
            "title": "IPC types"
        }, 
        {
            "location": "/Processes/", 
            "text": "Processes\n\n\nA set of processes is included. These processes build upon AquaQ TorQ,\nproviding specific functionality. All the process scripts are contained\nin $KDBCODE/processes. All processes should have an entry in\n$KDBCONFIG/process.csv. All processes can have any type and name,\nexcept for discovery services which must have a process type of\n\u201cdiscovery\u201d. An example process.csv is:\n\n\naquaq$ cat config/process.csv\nhost,port,proctype,procname\naquaq,9998,rdb,rdb_europe_1\naquaq,9997,hdb,rdb_europe_1aquaq,9999,hdb,hdb1\naquaq,9996,discovery,discovery1\naquaq,9995,discovery,discovery2\naquaq,8000,gateway,gateway1\naquaq,5010,tickerplant,tickerplant1\naquaq,5011,rdb,rdb1\naquaq,5012,hdb,hdb1\naquaq,5013,hdb,hdb2\naquaq,9990,tickerlogreplay,tpreplay1\naquaq,20000,kill,killhdbs\naquaq,20001,monitor,monitor1\naquaq,20002,housekeeping,hk1\n\n\n\n\n\nDiscovery Service\n\n\nOverview\n\n\nProcesses use the discovery service to register their own availability,\nfind other processes (by process type) and subscribe to receive updates\nfor new process availability (by process type). The discovery service\ndoes not manage connections- it simply returns tables of registered\nprocesses, irrespective of their current availability. It is up to each\nindividual process to manage its own connections.\n\n\nThe discovery service uses the process.csv file to make connections to\nprocesses on start up. After start up it is up to each individual\nprocess to attempt connections and register with the discovery service.\nThis is done automatically, depending on the configuration parameters.\nMultiple discovery services can be run in which case each process will\ntry to register and retrieve process details from each discovery process\nit finds in its process.csv file. Discovery services do not replicate\nbetween themselves. A discovery process must have its process type\nlisted as discovery.\n\n\nTo run the discovery service, use a start line such as:\n\n\naquaq $ q torq.q -load code/processes/discovery.q -p 9995\n\n\n\nModify the configuration as required.\n\n\nOperation\n\n\n\n\n\n\nProcesses register with the discovery service.\n\n\n\n\n\n\n\n\nProcesses use the discovery service to locate other processes.\n\n\n\n\n\n\n\n\nWhen new services register, any processes which have registered an\n     interest in that process type are notified.\n\n\n\n\n\n\n\n\nAvailable Processes\n\n\nThe list of available processes can be found in the .servers.SERVERS\ntable.\n\n\nq).servers.SERVERS                                                                                                                                                                                                                                                            \nprocname     proctype        hpup            w  hits startp                        lastp                         endp attributes                                                                   \n-------------------------------------------------------------------------------------\ndiscovery1   discovery       :aquaq:9995     0                                  2014.01.22D17:00:40.947470000      ()!()                                                                        \ndiscovery2   discovery       :aquaq:9996     0                                  2014.01.22D17:00:40.947517000      ()!()                                                                        \nhdb2         hdb             :aquaq:5013     0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \nkilltick     kill            :aquaq:20000    0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \ntpreplay1    tickerlogreplay :aquaq:20002    0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \ntickerplant1 tickerplant     :aquaq:5010  6  0    2014.01.22D17:00:40.967699000 2014.01.22D17:00:40.967698000      ()!()                                                                        \nmonitor1     monitor         :aquaq:20001 9  0    2014.01.22D17:00:40.971344000 2014.01.22D17:00:40.971344000      ()!()                                                                        \nrdb1         rdb             :aquaq:5011  7  0    2014.01.22D17:06:13.032883000 2014.01.22D17:06:13.032883000      `date`tables!(,2014.01.22;`fxquotes`heartbeat`logmsg`quotes`trades)          \nhdb3         hdb             :aquaq:5012  8  0    2014.01.22D17:06:18.647349000 2014.01.22D17:06:18.647349000      `date`tables!(2014.01.13 2014.01.14;`fxquotes`heartbeat`logmsg`quotes`trades)\ngateway1     gateway         :aquaq:5020  10 0    2014.01.22D17:06:32.152836000 2014.01.22D17:06:32.152836000      ()!()\n\n\n\n\n\nGateway\n\n\nA synchronous and asynchronous gateway is provided. The gateway can be\nused for load balancing and/or to join the results of queries across\nheterogeneous servers (e.g. an RDB and HDB). Ideally the gateway should\nonly be used with asynchronous calls. Synchronous calls cause the\ngateway to block so limits the gateway to serving one query at a time\n(although if querying across multiple backend servers the backend\nqueries will be run in parallel). When using asynchronous calls the\nclient can either block and wait for the result (deferred synchronous)\nor post a call back function which the gateway will call back to the\nclient with. With both asynchronous and synchronous queries the backend\nservers to execute queries against are selected using process type. The\ngateway API can be seen by querying .api.p\u201c.gw.*\u201d within a gateway\nprocess.\n\n\n\n\nAsynchronous Behaviour\n\n\nAsynchronous queries allow much greater flexibility. They allow multiple\nqueries to be serviced at once, prioritisation, and queries to be timed\nout. When an asynchronous query is received the following happens:\n\n\n\n\n\n\nthe query is placed in a queue;\n\n\n\n\n\n\nthe list of available servers is retrieved;\n\n\n\n\n\n\nthe queue is prioritised, so those queries with higher priority are\n      serviced first;\n\n\n\n\n\n\nqueries are sent to back end servers as they become available. Once\n      the backend server returns its result, it is given another query;\n\n\n\n\n\n\nwhen all the partial results from the query are returned the results\n      are aggregated and returned to the client. They are either returned\n      directly, or wrapped in a callback and posted back asynchronously to\n      the client.\n\n\n\n\n\n\nThe two main customisable features of the gateway are the selection of\navailable servers (.gw.availableservers) and the queue prioritisation\n(.gw.getnextqueryid). With default configuration, the available servers\nare those servers which are not currently servicing a query from the\ngateway, and the queue priority is a simple FIFO queue. The available\nservers could be extended to handle process attributes, such as the\navailable datasets or the location of the process, and the queue\nprioritisation could be modified to anything required e.g. based on the\nquery itself, the username, host of the client etc.\n\n\nAn asynchronous query can be timed out using a timeout defined by the\nclient. The gateway will periodically check if any client queries have\nnot completed in the alotted time, and return a timeout error to the\nclient. If the query is already running on any backend servers then they\ncannot be timed out other than by using the standard -T flag.\n\n\nSynchronous Behaviour\n\n\nWhen using synchronous queries the gateway can only handle one query at\na time and cannot timeout queries other than with the standard -T flag.\nAll synchronous queries will be immediately dispatched to the back end\nprocesses. They will be dispatched using an asyhcnronous call, allowing\nthem to run in parallel rather than serially. When the results are\nreceived they are aggregated and returned to the client.\n\n\nProcess Discovery\n\n\nThe gateway uses the discovery service to locate processes to query\nacross. The discovery service will notify the gateway when new processes\nbecome available and the gateway will automatically connect and start\nusing them. The gateway can also use the static information in\nprocess.csv, but this limits the gateway to a predefined list of\nprocesses rather than allowing new services to come online as demand\nrequires.\n\n\nError Handling\n\n\nWhen synchronous calls are used, q errors are returned to clients as\nthey are encountered. When using asynchronous calls there is no way to\nreturn actual errors and appropriately prefixed strings must be used\ninstead. It is up to the client to check the type of the received result\nand if it is a string then whether it contains the error prefix. The\nerror prefix can be changed, but the default is \u201cerror: \u201d. Errors will\nbe returned when:\n\n\n\n\n\n\nthe client requests a query against a server type which the gateway\n    does not currently have any active instances of (this error is\n    returned immediately);\n\n\n\n\n\n\nthe query is timed out;\n\n\n\n\n\n\na back end server returns an error;\n\n\n\n\n\n\na back end server fails;\n\n\n\n\n\n\nthe join function fails.\n\n\n\n\n\n\nIf postback functions are used, the error string will be posted back\nwithin the postback function (i.e. it will be packed the same way as a\nvalid result).\n\n\nClient Calls\n\n\nThere are four main client calls. The .gw.sync* methods should only be\ninvoked synchronously, and the .gw.async* methods should only be\ninvoked asynchronously. Each of these are documented more extensively in\nthe gateway api. Use .api.p\u201c.gw.*\u201d for more details.\n\n\n\n\n\n\n\n\nFunction\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n.gw.syncexec[query; servertypes]\n\n\nExecute the specified query synchronously against the required list of servers. If more than one server, the results will be razed.\n\n\n\n\n\n\n.gw.syncexecj[query; servertypes; joinfunction]\n\n\nExecute the specified query against the required list of servers. Use the specified join function to aggregate the results.\n\n\n\n\n\n\n.gw.asyncexec[query; servertypes]\n\n\nExecute the specified query against the required list of servers. If more than one server, the results will be razed. The client must block and wait for the results.\n\n\n\n\n\n\n.gw.asyncexecjpt[query; servertypes; joinfunction; postback; timeout]\n\n\nExecute the specified query against the required list of servers. Use the specified join function to aggregate the results. If the postback function is not set, the client must block and wait for the results. If it is set, the result will be wrapped in the specified postback function and returned asynchronously to the client. The query will be timed out if the timeout value is exceeded.\n\n\n\n\n\n\n\n\nFor the purposes of demonstration, assume that the queries must be run\nacross an RDB and HDB process, and the gateway has one RDB and two HDB\nprocesses available to it.\n\n\nq).gw.servers                                                                                                                                                                                                                                                                 \nhandle| servertype inuse active querycount lastquery                     usage                attributes                   \n------| --------------------------------------------------------------------\n7     | rdb        0     1      17         2014.01.07D17:05:03.113927000 0D00:00:52.149069000 `datacentre`country!`essex`uk\n8     | hdb        0     1      17         2014.01.07D17:05:03.113927000 0D00:01:26.143564000 `datacentre`country!`essex`uk\n9     | hdb        0     1      2          2014.01.07D16:47:33.615538000 0D00:00:08.019862000 `datacentre`country!`essex`uk\n12    | rdb        0     1      2          2014.01.07D16:47:33.615538000 0D00:00:04.018349000 `datacentre`country!`essex`uk\n\n\n\nBoth the RDB and HDB processes have a function f and table t defined. f\nwill run for 2 seconds longer on the HDB processes then it will the RDB.\n\n\nq)f                                                                                                                                                                                                                                                                           \n{system\"sleep \",string x+$[`hdb=.proc.proctype;2;0]; t}\nq)t                                                                                                                                                                                                                                                                           \na   \n----\n5013\n5014\n5015\n5016\n5017\n\n\n\nRun the gateway. The main parameter which should be set is the\n.servers.CONNECTIONS parameter, which dictates the process types the\ngateway queries across. Also, we need to explicitly allow sync calls. We\ncan do this from the config or from the command line.\n\n\nq torq.q -load code/processes/gateway.q -p 8000 -.gw.synccallsallowed 1 -.servers.CONNECTIONS hdb rdb\n\n\n\nStart a client and connect to the gateway. Start with a sync query. The\nHDB query should take 4 seconds and the RDB query should take 2 seconds.\nIf the queries run in parallel, the total query time should be 4\nseconds.\n\n\nq)h:hopen 8000                                                                                                                                                                                                                                                                \nq)h(`.gw.syncexec;(`f;2);`hdb`rdb)                                                                                                                                                                                                                                            \na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\nq)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb)                                                                                                                                                                                                                                         \n4009\n\n\n\nIf a query is done for a server type which is not registered, an error\nis returned:\n\n\nq)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb`other)                                                                                                                                                                                                                                   \n`not all of the requested server types are available; missing other\n\n\n\nCustom join functions can be specified:\n\n\nq)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by a from x} each x})                                                                                                                                                                                                  \na   | x\n----| -\n5014| 2\n5015| 2\n5016| 2\n5017| 1\n5018| 1\n5012| 1\n5013| 1\n\n\n\nCustom joins can fail with appropriate errors:\n\n\nq)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by b from x} each x})                                                                                                                                                                                                  \n`failed to apply supplied join function to results: b\n\n\n\nAsynchronous queries must be sent in async and blocked:\n\n\nq)(neg h)(`.gw.asyncexec;(`f;2);`hdb`rdb); r:h(::)                                                                                                                                                                                                                          \n    /- This white space is from pressing return\n    /- the client is blocked and unresponsive\n\nq)q)q)                                                                                                                                                                                                                                                                        \nq)                                                                                                                                                                                                                                                                            \nq)r                                                                                                                                                                                                                                                                           \na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\nq)\n\n\n\nWe can send multiple async queries at once. Given the gateway has two\nRDBs and two HDBs avaialble to it, it should be possible to service two\nof these queries at the same time.\n\n\nq)h:hopen each 8000 8000                                                                                                                                                                                                                                                      \nq)\\t (neg h)@\\:(`.gw.asyncexec;(`f;2);`hdb`rdb); (neg h)@\\:(::); r:h@\\:(::)\n4012\nq)r                                                                                                                                                                                                                                                                           \n+(,`a)!,5014 5015 5016 5017 5018 5012 5013 5014 5015 5016\n+(,`a)!,5013 5014 5015 5016 5017 9999 10000 10001 10002 10003\n\n\n\nAlternatively async queries can specify a postback so the client does\nnot have to block and wait for the result. The postback function must\ntake two parameters- the first is the function that was sent up, the\nsecond is the results. The postback can either be a lambda, or the name\nof a function.\n\n\nq)h:hopen 8000                                                                                                                                                                                                                                                                \nq)handleresults:{-1(string .z.z),\" got results\"; -3!x; show y}                                                                                                                                                                                                                \nq)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;handleresults;0Wn)                                                                                                                                                                                                           \nq)\nq)  /- These q prompts are from pressing enter\nq)  /- The q client is not blocked, unlike the previous example\nq)\nq)2014.01.07T16:53:42.481 got results\na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\n\n/- Can also use a named function rather than a lambda\nq)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;`handleresults;0Wn)\nq)\nq)              \nq)2014.01.07T16:55:12.235 got results\na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\n\n\n\nAsynchronous queries can also be timed out. This query will run for 22\nseconds, but should be timed out after 5 seconds. There is a tolerance\nof +5 seconds on the timeout value, as that is how often the query list\nis checked. This can be reduced as required.\n\n\nq)(neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::)\n\nq)q)q)r                                                                                                                                                                                                                                                                       \n\"error: query has exceeded specified timeout value\"\nq)\\t (neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::)                                                                                                                                                                                                  \n6550\n\n\n\nNon kdb+ Clients\n\n\nAll the examples in the previous section are from clients written in q.\nHowever it should be possible to do most of the above from non kdb+\nclients. The officially supported APIs for Java, C# and C allow the\nasynchronous methods above. For example, we can modify the try block in\nthe main function of the \nJava Grid\nViewer\n:\n\n\nimport java.awt.BorderLayout;\nimport java.awt.Color;\nimport java.io.IOException;\nimport java.lang.reflect.Array;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\nimport javax.swing.JFrame;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTable;\nimport javax.swing.table.AbstractTableModel;\nimport kx.c;\n\npublic class Main {\n    public static class KxTableModel extends AbstractTableModel {\n        private c.Flip flip;\n        public void setFlip(c.Flip data) {\n            this.flip = data;\n        }\n\n        public int getRowCount() {\n            return Array.getLength(flip.y[0]);\n        }\n\n        public int getColumnCount() {\n            return flip.y.length;\n        }\n\n        public Object getValueAt(int rowIndex, int columnIndex) {\n            return c.at(flip.y[columnIndex], rowIndex);\n        }\n\n        public String getColumnName(int columnIndex) {\n            return flip.x[columnIndex];\n        }\n    };\n\n    public static void main(String[] args) {\n        KxTableModel model = new KxTableModel();\n        c c = null;\n        try {\n            c = new c(\"localhost\", 8000,\"username:password\");\n            // Create the query to send\n        String query=\".gw.asyncexec[(`f;2);`hdb`rdb]\";\n            // Send the query \n        c.ks(query);\n            // Block on the socket and wait for the result\n        model.setFlip((c.Flip) c.k());\n        } catch (Exception ex) {\n            Logger.getLogger(Main.class.getName()).log(Level.SEVERE, null, ex);\n        } finally {\n            if (c != null) {try{c.close();} catch (IOException ex) {}\n          }\n        }\n        JTable table = new JTable(model);\n        table.setGridColor(Color.BLACK);\n        String title = \"kdb+ Example - \"+model.getRowCount()+\" Rows\";\n        JFrame frame = new JFrame(title);\n        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n        frame.getContentPane().add(new JScrollPane(table), BorderLayout.CENTER);\n        frame.setSize(300, 300);\n        frame.setVisible(true);\n    }\n}\n\n\n\nSome of the unofficially supported APIs may only allow synchronous calls\nto be made.\n\n\n\n\nTickerplant\n\n\nThe tickerplant is a modified version of the standard kdb+tick tickerplant.\nThe modifications from the standard tick.q include:\n\n\n\n\n\n\nApplies timestamps as timestamp rather than timespan;\n\n\n\n\n\n\nTracks per table record counts in .u.icounts dictionary for faster recovery\n    of real time subscribers;\n\n\n\n\n\n\nAllows configuration of timezones for timestamping data and performing\n    end of day rollover (see \neodtime.q\n);\n\n\n\n\n\n\nThe tickerplant log file will be written to hdb/database.\n\n\n\n\nReal Time Database (RDB)\n\n\nThe Real Time Database is a modified version of r.q found in kdb+tick.\nThe modifications from the standard r.q include:\n\n\n\n\n\n\nTickerplant (data source) and HDB location derived from processes\n    defined by the discovery service or from config file;\n\n\n\n\n\n\nAutomatic re-connection and resubscription to tickerplant;\n\n\n\n\n\n\nList of tables to subscribe to supplied as configuration setting;\n\n\n\n\n\n\nMore pre-built flexibility in end-of-day;\n\n\n\n\n\n\nMore verbose end-of-day logging;\n\n\n\n\n\n\nReload multiple authenticated HDBs after end-of-day;\n\n\n\n\n\n\nEnd-of-day save down manipulation code is shared between RDB, WDB\n      and tickerplant log replay\n\n\n\n\n\n\nSee the top of the file for more information.\n\n\n\n\nWrite Database (WDB)\n\n\nThe Write Database or WDB is based on w.q. This process features a\nnumber of modifications and enhancements over w.q:\n\n\n\n\n\n\nProvides the option to write down to a custom partition scheme,\n    defined by parted columns in sort.csv, which removes the need for\n    end of day sorting;\n\n\n\n\n\n\nGreater configuration options; max rows on a per table basis, list\n     subscription tables, upd function etc. See the top of the process\n     file for the options;\n\n\n\n\n\n\nUse of common code with the RDB and Tickerplant Log Replay process\n     to manipulate tables before save, sort and apply attributes;\n\n\n\n\n\n\nChecks whether to persist data to disk on a timer rather than on\n     each tick;\n\n\n\n\n\n\nInforms other RDB, HDB and GW processes that end of day save and\n     sort has completed;\n\n\n\n\n\n\nMore log information supplied;\n\n\n\n\n\n\nEnd of day timezone can be configured (see \neodtime.q\n).\n\n\n\n\n\n\nThe WDB process can broken down into two main functions:\n\n\n\n\n\n\nPeriodically saving data to disk and\n\n\n\n\n\n\nSorting data at end of day\n\n\n\n\n\n\nThe WDB process provides flexibility so it can be set-up as a\nstand-alone process that will both save and sort data or two separate\nprocesses (one that saves the data and another that will sort the data\non disk). This allows greater flexibility around the end of day event as\nsorting data can be time consuming. It is also helps when implementing\nseemless rollovers (i.e. no outage window at end-of-day).\n\n\nThe behaviour of the WDB process is controlled by the \n.wdb.mode\n\nparameter. This should be set to one of following three values:\n\n\n\n\n\n\nsaveandsort - the process will subscribe for data, periodically\n    write data to disk and at EOD it will flush remaining data to disk\n    before sorting it and informing GWs, RDBs and HDBs etc.\n\n\n\n\n\n\nsave - the process will subscribe for data, periodically write data\n      to disk and at EOD it will flush remaining data to disk. It will\n      then inform its respective sort mode process to sort the data\n\n\n\n\n\n\nsort - the process will wait to get a trigger from its respective\n      save mode process. When this is triggered it will sort the data on\n      disk, apply attributes and the trigger a reload on the RDB, HDB and\n      GW processes\n\n\n\n\n\n\nWhen running a system with separate save and sort process, the sort\nprocess should be configured in the processes.csv file with a proctype\nof sort. The save process will check for processes with a proctype of\nsort when it attempts to trigger the end of day sort of the data.\n\n\nThe wdb process provides two methods for persisting data to disk and\nsorting at the end of the day.\n\n\n\n\n\n\ndefault - Data is persisted into a partition defined by the\n    [partitiontype] variable, similar to the hdb partition scheme. The\n    general scheme is of the form\n    [wdbdir]/[partitiontype]/[table]/. And a typical partition\n    directory would be similar to wdb/database/2015.11.26/trades/. At\n    the end of the day, before being moved to the hdb, the data is\n    sorted according to parameters defined in sort.csv. For each table,\n    sort.csv will specify the columns to sort (using xasc) and apply\n    attributes to.\n\n\n\n\n\n\npartbyattr - Data is persisted to a custom partition scheme, derived\n      from parameters in the sort.csv file. The write down scheme is taken\n      from sort.csv, to reflect the effect of using xasc at the end of\n      day. For each table, the columns defined in sort.csv, with the\n      parted attribute, are used to create custom partitions in the wdb.\n      Multiple columns can be defined with the parted attribute and\n      distinct combinations of each are generated for custom partitions.\n      The general partition scheme is of the form\n      [wdbdir]/[partitiontype]/[table]/[parted column(s)]/. And a\n      typical partition directory would be similar to\n      wdb/database/2015.11.26/trade/MSFT_N. In the above example, the\n      data is parted by sym and source, and so a unique partition\n      directory MSFT_N is created in the wdb directory.\n\n\nAt the end of the day, data is upserted into the hdb without the\n  need for sorting. The number of rows that are joined at once is\n  limited by the mergenumrows and mergenumtab parameters.\n\n\n\n\n\n\nThe optional partbyattr method may provide a significant saving in time\nat the end of day, allowing the hdb to be accessed sooner. For large\ndata sets with a low cardinality (ie. small number of distinct elements)\nthe optional method may provide a significant time saving, upwards of\n50%. The optional method should also reduce the memory usage at the end\nof day event, as joining data is generally less memory intensive than\nsorting.\n\n\n\n\nTickerplant Log Replay\n\n\nThe Tickerplant Log Replay script is for replaying tickerplant logs.\nThis is useful for:\n\n\n\n\n\n\nhandling end of day save down failures;\n\n\n\n\n\n\nhandling large volumes of data (larger than can fit into RAM).\n\n\n\n\n\n\nThe process takes as the main input either an individual log file to\nreplay, or a directory containing a set of log files. Amongst other\nfunctionality, the process can:\n\n\n\n\n\n\nreplay specific message ranges;\n\n\n\n\n\n\nreplay in manageable message chunks;\n\n\n\n\n\n\nrecover as many messages as possible from a log file rather than\n      just stopping at the first bad message;\n\n\n\n\n\n\nignore specific tables;\n\n\n\n\n\n\nmodify the tables before or after they are saved;\n\n\n\n\n\n\napply sorting and parting after all the data is written out.\n\n\n\n\n\n\nThe process must have some variables set (the tickerplant log file or\ndirectory, the schema file, and the on-disk database directory to write\nto) or it will fail on startup. These can either be set in the config\nfile, or overridden from the command line in the usual way. An example\nstart line would be:\n\n\nq torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.tplogfile ../test/tplogs/marketdata2013.12.17 -.replay.schemafile ../test/marketdata.q -.replay.hdbdir ../test/hdb1\n\n\n\nThe tickerplant log replay script has extended usage information which\ncan be accessed with -.replay.usage.\n\n\nq torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.usage\n\n\n\n\n\nHousekeeping\n\n\nThe housekeeping process is used to undertake periodic system\nhousekeeping and maintenance, such as compressing or removing files\nwhich are no longer required. The process will run the housekeeping jobs\nperiodically on a timer. Amongst other functionality the process:\n\n\n\n\n\n\nAllows for removing and zipping of directory files;\n\n\n\n\n\n\nProvides an inbuilt search utility and selectively searches using a\n      \u2018find\u2019 and \u2018exclude\u2019 string, and an \u2018older than\u2019 parameter;\n\n\n\n\n\n\nReads all tasks from a single CSV;\n\n\n\n\n\n\nRuns on a user defined timer;\n\n\n\n\n\n\nCan be run immediately from command line or within the process;\n\n\n\n\n\n\nCan be easily extended to include new user defined housekeeping\n      tasks.\n\n\n\n\n\n\nThe process has two main parameters that should be set prior to use;\nruntimes and inputcsv.\u2018Runtimes\u2019 sets the timer to run housekeeping at\nthe set time(s), and \u2018Inputcsv\u2019 provides the location of the\nhousekeeping csv file. These can either be set in the config file, or\noverridden via the command line. If these are not set, then default\nparameters are used; 12.00 and \u2018KDBCONFIG/housekeeping.csv\u2019\nrespectively. The process is designed to run from a single csv file with\nfive headings:\n\n\n\n\n\n\nFunction details the action that you wish to be carried out on the\n    files, initially, this can be rm (remove) and zip (zipping);\n\n\n\n\n\n\nPath specifies the directory that the files are in;\n\n\n\n\n\n\nMatch provides the search string to the find function, files\n      returned will have names that match this string;\n\n\n\n\n\n\nExclude provides a second string to the find function, and these\n      files are excluded from the match list;\n\n\n\n\n\n\nAge is the \u2018older than\u2019 parameter, and the function will only be\n      carried out on files older than the age given (in days).\n\n\n\n\n\n\nAn example csv file would be:\n\n\nfunction,path,match,exclude,age\nzip,./logs/,*.log,*tick*,2\nrm,./logs/,*.log*,*tick*,4\nzip,./logs/,*tick*,,1\nrm,./logs/,*tick*,,3\n\nfunction path      match    exclude  age\n----------------------------------------\nzip      \"./logs/\" \"*.log\"  \"*tick*\" 2\nrm       \"./logs/\" \"*.log*\" \"*tick*\" 4\nzip      \"./logs/\" \"*tick*\" \"\"       1\nrm       \"./logs/\" \"*tick*\" \"\"       3\n\n\n\nThe process reads in the csv file, and passes it line by line to a\n\u2018find\u2019 function; providing a dictionary of values that can be used to\nlocate the files required. The find function takes advantage of system\ncommands to search for the files according to the specifications in the\ndictionary. A search is performed for both the match string and the\nexclude string, and cross referenced to produce a list of files that\nmatch the parameters given. The files are then each passed to a further\nset of system commands to perform the task of either zipping or\nremoving. Note that an incomplete csv or non-existant path will throw an\nerror.\n\n\nThe remove and zipping functions form only basic implimentations of the\nhousekeeping process; it is designed to be exended to include more\nactions than those provided. Any user function defined in the\nhousekeeping code can be employed in the same fashion by providing the\nname of the function,search string and age of files to the csv.\n\n\nAs well as being scheduled on a timer, the process can also be run\nimmediately. Adding \u2018-hk.runnow 1\u2019 to the command line when starting the\nprocess will force immediate running of the actions in the housekeeping\ncsv. Likewise, setting runnow to 1b in the config file will immediately\nrun the cleaning process. Both methods will cause the process to exit\nupon completion. Calling hkrun[] from within the q process will also\nrun the csv instructions immediately. This will not affect any timer\nscheduling and the process will remain open upon completion.\n\n\nHousekeeping works both on windows and unix based systems. Since the\nprocess utilizes inbuilt system commands to perform maintenances, a\nunix/windows switch detects the operating system of the host and applies\neither unix or widows functions appropriately. Extensions need only be\nmade in the namespace of the hosting operating system (i.e. if you are\nusing a unix system, and wish to add a new function, you do not need to\nadd the function to the windows namespace to). Usage information can be\naccessed using the \u2018-hkusage\u2019 flag:\n\n\nq torq.q -load code/processes/housekeeping.q -p 9999 -proctype housekeeping -procname hk1 -debug -hkusage\n\n\n\n\n\nFile Alerter\n\n\nThe file alerter process is a long-running process which periodically\nscans a set of directories for user-specified files. If a matching file\nis found it will then carry out a user-defined function on it. The files\nto search for and the functions to run are read in from a csv file.\nAdditionally, the file alerter process can:\n\n\n\n\n\n\nrun more than one function on the specified file.\n\n\n\n\n\n\noptionally move the file to a new directory after running the\n      function.\n\n\n\n\n\n\nstore a table of files that have already been processed.\n\n\n\n\n\n\nrun the function only on new files or run it every time the file is\n      modified.\n\n\n\n\n\n\nignore any matching files already on the system when the process\n      starts and only run a function if a new file is added or a file is\n      modified.\n\n\n\n\n\n\nThe file alerter process has four parameters which should be set prior\nto use. These parameters can either be set in the config file or\noverridden on the command-line. If they are not set, the default\nparameters will be used. The parameters are as follows.\n\n\ninputcsv\n - The name and location of the csv file which defines the\nbehaviour of the process. The default is KDBCONFIG/filealerter.csv.\n\n\npolltime\n - How often the process will scan for matching files. The\ndefault is 0D:00:01, i.e., every minute.\n\n\nalreadyprocessed\n - The name and location of the already-processed\ntable. The default is KDBCONFIG/filealerterprocessed. This table will\nbe created automatically the first time the process is ran.\n\n\nskipallonstart\n - If this is set to 1, it will ignore all files\nalready on the system; if it is set to 0, it will not. The default\nvalue is 0.\n\n\nThe files to find and the functions to run are read in from a csv file\ncreated by the user. This file has five columns, which are detailed\nbelow.\n\n\npath\n - This is the path to the directory that will be scanned for\nthe file.\n\n\nmatch\n - This is a search string matching the name of the file to be\nfound. Wildcards can be used in this search, for example, \u201cfile*\u201d will\nfind all files starting with \u201cfil\u201d.\n\n\nfunction\n - This is the name of the function to be run on the file.\nThis function must be defined in the script\nKDBCODE/processes/filealerter.q. If the function is not defined or fails\nto run, the process will throw an error and ignore that file from then\non.\n\n\nnewonly\n - This is a boolean value. If it is set to 1, it will\nonly run the function on the file if it has been newly created. If it is\nset to 0, then it will run the function every time the file is\nmodified.\n\n\nmovetodirectory\n - This is the path of the directory you would like\nto move the file to after it has been processed. If this value is left\nblank, the file will not be moved.\n\n\nIt is possible to run two separate functions on the same file by adding\nthem as separate lines in the csv file. If the file is to be moved after\nit is processed, the file alerter will run both functions on the file\nand then attempt to move it. A typical csv file to configure the file\nalerter would look like:\n\n\npath,match,function,newonly,movetodirectory\n/path/to/dirA,fileA.*,copy,0,/path/to/newDir\n/path/to/dirB,fileB.txt,email,1,\n/path/to/dirA,fileA.*,delete,0,/path/to/newDir\n\npath        match      function  newonly    movetodirectory\n---------------------------------------------------\n\"/path/to/dirA\" \"fileA.*\"   copy    0   \"/path/to/newDir\"\n\"/path/to/dirB\" \"fileB.txt\" email   1   \"\"\n\"/path/to/dirA\" \"fileA.*\"   delete  0   \"/path/to/newDir\"\n\n\n\nThe file alerter process reads in each line of the csv file and searches\nfiles matching the search string specified in that line. Note that there\nmay be more than one file found if a wildcard is used in the search\nstring. If it finds any files, it will check that they are not in the\nalready processed table. If newonly is set to 1, it only checks if\nthe filename is already in the table. If newonly is set to 0, it\nchecks against the filename, filesize and a md5 hash of the file. The\nmd5 hash and the filesize are used to determine if the file has been\nmodified since it was processed last. If the found files have not been\nprocessed already, it then attempts to run the specified function to\nthese files.\n\n\nAfter the process has run through each line of the csv, it generates a\ntable of all files that were processed on that run. These files are\nappended to the already processed table which is then saved to disk. The\nfile alerter will attempt to move the files to the \u2018movetodirectory\u2019, if\nspecified. If the file has already been moved during the process (for\nexample, if the function to run on it was \u2018delete\u2019), the file alerter\nwill not attempt to move it.\n\n\nThe file alerter is designed to be extended by the user. Customised\nfunctions should be defined within the filealerter.q script. They should\nbe diadic functions, i.e., they take two parameters: the path and the\nfilename. As an example, a simple function to make a copy of a file in\nanother directory could be:\n\n\ncopy:{[path;file] system \"cp \", path,\"/\", file, \" /path/to/newDir\"}\n\n\n\nAlthough the process is designed to run at regular intervals throughout\nthe day, it can be called manually by invoking the FArun[] command\nfrom within the q session. Similarly, if new lines are added to the csv\nfile, then it can be re-loaded by calling the loadcsv[] command\nfrom the q session.\n\n\nEach stage of the process, along with any errors which may occur, are\nappropriately logged in the usual manner.\n\n\nThe file alerter process is designed to work on both Windows and Unix\nbased systems. Since many of the functions defined will use inbuilt\nsystem command they will be need to written to suit the operating system\nin use. It should also be noted that Windows does not have an inbuilt\nmd5 hashing function so the file alerter will only detect different\nversions of files if the filename or filesize changes.\n\n\n\n\nReporter\n\n\nOverview\n\n\nThe reporter process is used to run periodic reports on specific\nprocesses. A report is the result of a query that is run on a process at\na specific time. The result of the query is then handled by one of the\ninbuilt result handlers, with the ability to add custom result handlers.\n\n\n\n\nFeatures:\n\n\n\n\n\n\nEasily create a report for information that you want;\n\n\n\n\n\n\nFully customizable scheduling such as start time, end time and days\n      of the week;\n\n\n\n\n\n\nRun reports repeatedly with a custom period between them;\n\n\n\n\n\n\nAsynchronous querying with custom timeout intervals;\n\n\n\n\n\n\nInbuilt result handlers allow reports to be written to file or\n      published;\n\n\n\n\n\n\nCustom result handlers can be defined;\n\n\n\n\n\n\nLogs each step of the report process;\n\n\n\n\n\n\nFully integrated with the TorQ gateway to allow reports to be run\n      across backend processes.\n\n\n\n\n\n\nThe reporter process has three parameters that are read in on\ninitialisation from the reporter.q file found in the\n$KDBCONFIG/settings directory. These settings are the string filepath\nof the input csv file, a boolean to output log messages and timestamp\nfor flushing the query log table.\n\n\nTo run the reporter process:\n\n\nq torq.q -load code/processes/reporter.q -p 20004\n\n\n\nOnce the reporter process has been initiated, the reports will be\nscheduled and no further input is required from the user.\n\n\nReport Configuration\n\n\nBy default, the process takes its inputs from a file called reporter.csv\nwhich is found in the $KDBCONFIG directory. This allows the user\ncomplete control over the configuration of the reports. As the queries\nare evaluated on the target process, local variables can be referenced\nor foreign functions can be run. Table [table:reportertable] shows the\nmeaning of the csv schema.\n\n\n\n\n\n\n\n\nColumn Header\n\n\nDescription and Example\n\n\n\n\n\n\n\n\n\n\nname\n\n\nReport name e.g. Usage\n\n\n\n\n\n\nquery\n\n\nQuery to be evaluated on that process. It can be a string query or function\n\n\n\n\n\n\nresulthandler\n\n\nResult handlers are run on the returned result. Custom result handlers can be added. The result handler must be a monadic function with the result data being passed in e.g. writetofile[\u201c./output\u201d;\u201cusage\u201d]\n\n\n\n\n\n\ngateway\n\n\nIf non null the reporter will query processes route the query to the proctype specified in this field. The values in the proctype field will be the process types on which the gateway runs the backend query. e.g. `gateway\n\n\n\n\n\n\njoinfunction\n\n\nUsed to join the results when a gateway query is being used. The choice of joinfunction must take into account the result that will be received. The function must be monadic and the parameter will be the list of results returned from the backend processes e.g. raze\n\n\n\n\n\n\nproctype\n\n\nThe type of process that the report will be run on. If the gateway field is not empty this may be a list of process types, otherwise the reporter will throw an error on startup. e.g. `rdb\n\n\n\n\n\n\nprocname\n\n\nThe name of a specific process to run the report on. If left null, the reporter process will select a random process with the specified proctype. If the gateway field is not null, this field specifies the specific gateway process name to run the query against e.g. `hdb1\n\n\n\n\n\n\nstart\n\n\nTime on that day to start at e.g. 12:00\n\n\n\n\n\n\nend\n\n\nTime on that day that the report will stop at e.g. 23:00\n\n\n\n\n\n\nperiod\n\n\nThe period between each report query e.g. 00:00:10\n\n\n\n\n\n\ntimeoutinterval\n\n\nThe amount of time the reporter waits before timing out a report e.g. 00:00:30\n\n\n\n\n\n\ndaysofweek\n\n\nNumeric value required for the day of the week. Where 0 is Saturday and 2 is Monday\n\n\n\n\n\n\n\n\nWhen running a report on a gateway, the gateway field must be set to the\nproctype of the gateway that will be queried. It will then run the\nreport on the processes which are listed in the proctype field and join\nthe results by using the function specified in the joinfunction field.\nIf there is no join function then the reporter process will not start.\nMultiple entries in the proctype field must be separated by a space and\nare only allowed when the gateway field is not empty. If gateway field\nis empty and there are multiple entries in the proctype field then the\nreporter process will not load.\n\n\nListing [code:csvschema] shows an example of the schema needed in the\ninput csv file.\n\n\nname|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek\nusage|10#.usage.usage|writetofiletype[\"./output/\";\"usage\";\"csv\"]|||rdb||00:01|23:50|00:01|00:00:01|0 1 2 3 4 5 6\nmemory|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|||rdb|rdb1|00:05|18:00|00:01|00:00:08|0 1 2 3 4 5 6\nusage_gateway|10#.usage.usage||gateway|raze|rdb hdb||00:02|22:00|00:01|00:00:10|0 1 2 3 4 5 6\n\n\n\nResult Handlers\n\n\nThere are several default result handlers which are listed below. Custom\nresult handlers can be defined as required. The result handler will be\ninvoked with a single parameter (the result of the query).\n\n\nwritetofiletype\n - Accepts 3 parameters: path, filename, filetype and\ndata. When writing to file it uses a date time suffix so the resultant\nfilename will be \nusage_rdb_2014_01_02_15_00_12.txt\n e.g.\n\n\nwritetofiletype[\"./output/\";\"usage\";\"csv\"]\n\n\n\nsplaytable\n - This accepts 3 parameters: path, file and data. This\nsplays the result to a directory. The result must be a table in order to\nuse this function e.g.\n\n\nsplaytable[\"./output/\";\"usage\"]\n\n\n\nemailalert\n - This accepts 3 parameters: period, recipient list and\ndata. The period dictates the throttle i.e. emails will be sent at most\nevery period. The result of the report must be a table with a single\ncolumn called messages which contains the character list of the email\nmessage. This is used with the monitoring checks to raise alerts, but\ncan be used with other functions.\n\n\nemailalert[0D00:30;(\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\")]\n\n\n\nemailreport\n - This accepts 3 parameters: temporary path, recipient\nlist, file name, file type and data. The data is written out as the file\ntype (e.g. csv, xml, txt, xls, json) with the given file name to the\ntemporary path. It is then emailed to the recipient list, and the\ntemporary file removed.\n\n\nemailreport[\"./tempdir/\"; (\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\"); \"EndOfDayReport\"; \"csv\"]\n\n\n\npublishresult\n - Accepts 1 parameter and that is the data. This is\ndiscussed later in the subsection\u00a0subresults.\nCustom result handlers can be added to $KDBCODE/processes/reporter.q .\nIt is important to note that the result handler is referencing local\nfunctions as it is executed in the reporter process and not the target\nprocess. When the query has been successful the result handler will be\npassed a dictionary with the following keys: queryid, time, name,\nprocname, proctype and result.\n\n\nReport Process Tracking\n\n\nEach step of the query is logged by the reporter process. Each query is\ngiven a unique id and regular system messages are given the id 0. The\nstage column specifies what stage the query is in and these are shown in\ntable [table:stagetable]. An appropriate log message is also shown so\nany problems can easily be diagnosed. The in memory table is flushed\nevery interval depending on the value of the flushqueryloginterval\nvariable in the reporter.q file found in the $KDBCONFIG/settings\ndirectory. \n\n\n\n\n\n\n\n\nStage symbol\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nR\n\n\nThe query is currently running\n\n\n\n\n\n\nE\n\n\nAn error has occurred during the query\n\n\n\n\n\n\nC\n\n\nThe query has been completed with no errors\n\n\n\n\n\n\nT\n\n\nThe query has exceeded the timeout interval\n\n\n\n\n\n\nS\n\n\nSystem message e.g. \u201cReporter Process Initialised\u201d\n\n\n\n\n\n\n\n\ntime                         | queryid stage message\n-----------------------------| ------------------------------------------------------------------------\n2014.10.20D22:20:06.597035000| 37 R \"Received result\"\n2014.10.20D22:20:06.600692000| 37 R \"Running resulthandler\"\n2014.10.20D22:20:06.604455000| 37 C \"Finished report\"\n2014.10.20D22:30:00.984572000| 38 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\"\n2014.10.20D22:30:00.991862000| 38 R \"Received result\"\n2014.10.20D22:30:00.995527000| 38 R \"Running resulthandler\"\n2014.10.20D22:30:00.999236000| 38 C \"Finished report\"\n2014.10.20D22:30:06.784419000| 39 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\"\n2014.10.20D22:30:06.796431000| 39 R \"Received result\"\n\n\n\nSubscribing for Results\n\n\nTo publish the results of the report, the reporter process uses the pub\nsub functionality of TorQ. This is done by using the using the inbuilt\nresult handler called publishresult. In order to subscribe to this feed,\nconnect to the reporter process and send the function shown below over\nthe handle. To subscribe to all reports use a backtick as the second\nparameter and to subscribe to a specific reports results include the\nreporter name as a symbol.\n\n\n/- define a upd function\nupd:insert\n\n/- handle to reporter process\nh: hopen 20004\n\n/- Subscribe to all results that use the publishresult handler\nh(`.ps.subscribe;`reporterprocessresults;`)\n\n/- Subscribe to a specific report called testreport\nh(`.ps.subscribe;`reporterprocessresults;`testreport)\n\n\n\nExample reports\n\n\nThe following are examples of reports that could be used in the reporter\nprocess. The rdbtablecount report will run hourly and return the count\nof all the tables in a rdb process. The memoryusage report will run\nevery 10 minutes against the gateway for multiple processes and will\nreturn the \n.Q.w[]\n information. Both of these reports run between\n9:30am to 4:00pm during the weekdays. The report onetimequery is an\nexample of a query that is run one time, in order to run a query once,\nthe period must be the same as the difference between the start and end\ntime.\n\n\nname|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek\nrdbtablecount|ts!count each value each ts:tables[]|{show x`result}|||rdb|rdb1|09:30|16:00|01:00|00:00:10|2 3 4 5 6\nmemoryusage|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|gateway1|{enlist raze x}|rdb hdb||09:30|16:00|00:10|00:00:10|2 3 4 5 6\nonetimequery|10#.usage.usage|writetofile[\"./output/\";\"onetime.csv\"]|||rdb||10:00|10:01|00:01|00:00:10|2 3 4 5 6\n\n\n\n\n\nMonitor\n\n\nThe Monitor process is a simple process to monitor the health of the\nother processes in the system. It connects to each process that it finds\n(by default using the discovery service, though can use the static file\nas well) and subscribes to both heartbeats and log messages. It\nmaintains a keyed table of heartbeats, and a table of all log messages\nreceived.\n\n\nRun it with:\n\n\naquaq $ q torq.q -load code/processes/monitor.q -p 20001\n\n\n\nIt is probably advisable to run the monitor process with the -trap flag,\nas there may be some start up errors if the processes it is connecting\nto do not have the necessary heartbeating or publish/subscribe code\nloaded.\n\n\naquaq $ q torq.q -load code/processes/monitor.q -p 20001 -trap\n\n\n\nThe current heartbeat statuses are tracked in .hb.hb, and the log\nmessages in logmsg\n\n\nq)show .hb.hb                                                                                                                                                                                                                                                                 \nsym       procname    | time                          counter warning error\n----------------------| ---------------------------------------------------\ndiscovery discovery2  | 2014.01.07D13:24:31.848257000 893     0       0    \nhdb       hdb1        | 2014.01.07D13:24:31.866459000 955     0       0    \nrdb       rdb_europe_1| 2014.01.07D13:23:31.507203000 901     1       0    \nrdb       rdb1        | 2014.01.07D13:24:31.848259000 34      0       0\n\nq)show select from logmsg where loglevel=`ERR                                                                                                              \ntime                          sym  host  loglevel id      message                               \n-------------------------------------------------------------------------------------\n2014.01.07D12:25:17.457535000 hdb1 aquaq ERR      reload  \"failed to reload database\"           \n2014.01.07D13:29:28.784333000 rdb1 aquaq ERR      eodsave \"failed to save tables : trade, quote\"\n\n\n\nHTML5 front end\n\n\nA HTML5 front end has been built to display important process\ninformation that is sent from the monitor process. It uses HTML5,\nWebSockets and JavaScript on the front end and interacts with the\nmonitor process in the kdb+ side. The features of the front end include:\n\n\n\n\n\n\nHeartbeat table with processes that have warnings highlighted in\n    orange and errors in red\n\n\n\n\n\n\nLog message table displaying the last 30 errors\n\n\n\n\n\n\nLog message error chart that is by default displayed in 5 minute\n      bins\n\n\n\n\n\n\nChart\u2019s bin value can be changed on the fly\n\n\n\n\n\n\nResponsive design so works on all main devices i.e. phones, tablets\n      and desktop\n\n\n\n\n\n\nIt is accessible by going to the url \nhttp://HOST:PORT/.non?monitorui\n\n\n\n\n\n\nCompression\n\n\nThe compression process is a thin wrapper around the compression utility\nlibrary. It allows periodic compression of whole or parts of databases\n(e.g. data is written out uncompressed and then compressed after a\ncertain period of time). It uses four variables defined in\nKDBCONFIG/settings/compression.q which specify\n\n\n\n\n\n\nthe compression configuration file to use\n\n\n\n\n\n\nthe database directory to compress\n\n\n\n\n\n\nthe maximum age of data to attempt to compress\n\n\n\n\n\n\nwhether the process should exit upon completion\n\n\n\n\n\n\nThe process is run like other TorQ processes:\n\n\nq torq.q -load code/processes/compression.q -p 20005\n\n\n\nModify the settings file or override variables from the command line as\nappropriate.\n\n\n\n\nKill\n\n\nThe kill process is used to connect to and terminate currently running\nprocesses. It kills the process by sending the exit command therefore\nthe kill process must have appropriate permissions to send the command,\nand it must be able to create a connection (i.e. it will not be able to\nkill a blocked process in the same way that the unix command kill -9\nwould). By default, the kill process will connect to the discovery\nservice(s), and kill the processes of the specified types. The kill\nprocess can be modified to not use the discovery service and instead use\nthe process.csv file via the configuration in the standard way.\n\n\nIf run without any command line parameters, kill.q will try to kill each\nprocess it finds with type defined by its .servers.CONNECTIONS variable.\n\n\nq torq.q -load code/processes/kill.q -p 20000\n\n\n\n.servers.CONNECTIONS can optionally be overridden from the command line\n(as can any other process variable):\n\n\nq torq.q -load code/processes/kill.q -p 20000 -.servers.CONNECTIONS rdb tickerplant\n\n\n\nThe kill process can also be used to kill only specific named processes\nwithin the process types:\n\n\nq torq.q -load code/processes/kill.q -p 20000 -killnames hdb1 hdb2\n\n\n\n\n\nChained Tickerplant\n\n\nIn tick+ architecture the main tickerplant is the most important\ncomponent, as it is relied upon by all the real time subscribers. When\nthe tickerplant goes down data will be lost, compare this to an rdb\nwhich can be recovered after it fails. The chained tickerplant process\nis an additional tickerplant that is a real time subscriber to the main\ntickerplant but replicates its behaviour. It will have its own real time\nsubscribers and can be recovered when it fails. This is the recommended\napproach when users want to perform their own custom real time analysis.\n\n\nThe chained tickerplant can:\n\n\n\n\n\n\nsubscribe to specific tables and syms\n\n\n\n\n\n\nbatch publish at an interval or publish tick by tick\n\n\n\n\n\n\ncreate a tickerplant log that its real time subscribers can replay\n\n\n\n\n\n\nreplay the source tickerplant log\n\n\n\n\n\n\nTo launch the chained tickerplant\n\n\nq torq.q -load code/processes/chainedtp.q -p 12009\n\n\n\nChained tickerplant settings are found in \nconfig/settings/chainedtp.q\n\nand are under the \n.ctp\n namespace.\n\n\n\n\n\n\n\n\nSetting\n\n\nExplanation\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\ntickerplantname\n\n\nlist of tickerplant names to try and make a connection to\n\n\n`tickerplant1\n\n\n\n\n\n\npubinterval\n\n\npublish batch updates at this interval. If the value is 0D00:00:00 then it will publish tick by tick\n\n\n0D00:00:00\n\n\n\n\n\n\ntpconnsleep\n\n\nnumber of seconds between attempts to connect to the source tickerplant\n\n\n10\n\n\n\n\n\n\ncreatelogfile\n\n\ncreate a log file\n\n\n0b\n\n\n\n\n\n\nlogdir\n\n\ndirectory containing chained tickerplant logs\n\n\n`:hdb\n\n\n\n\n\n\nsubscribeto\n\n\nsubscribe to these tables only (null for all)\n\n\n`\n\n\n\n\n\n\nsubscribesyms\n\n\nsubscribe to these syms only (null for all)\n\n\n`\n\n\n\n\n\n\nreplay\n\n\nreplay the tickerplant log file\n\n\n0b\n\n\n\n\n\n\nschema\n\n\nretrieve schema from tickerplant\n\n\n1b\n\n\n\n\n\n\nclearlogonsubscription\n\n\nclear log on subscription, only called if createlogfile is also enabled\n\n\n0b", 
            "title": "Processes"
        }, 
        {
            "location": "/Processes/#processes", 
            "text": "A set of processes is included. These processes build upon AquaQ TorQ,\nproviding specific functionality. All the process scripts are contained\nin $KDBCODE/processes. All processes should have an entry in\n$KDBCONFIG/process.csv. All processes can have any type and name,\nexcept for discovery services which must have a process type of\n\u201cdiscovery\u201d. An example process.csv is:  aquaq$ cat config/process.csv\nhost,port,proctype,procname\naquaq,9998,rdb,rdb_europe_1\naquaq,9997,hdb,rdb_europe_1aquaq,9999,hdb,hdb1\naquaq,9996,discovery,discovery1\naquaq,9995,discovery,discovery2\naquaq,8000,gateway,gateway1\naquaq,5010,tickerplant,tickerplant1\naquaq,5011,rdb,rdb1\naquaq,5012,hdb,hdb1\naquaq,5013,hdb,hdb2\naquaq,9990,tickerlogreplay,tpreplay1\naquaq,20000,kill,killhdbs\naquaq,20001,monitor,monitor1\naquaq,20002,housekeeping,hk1", 
            "title": "Processes"
        }, 
        {
            "location": "/Processes/#discovery-service", 
            "text": "", 
            "title": "Discovery Service"
        }, 
        {
            "location": "/Processes/#overview", 
            "text": "Processes use the discovery service to register their own availability,\nfind other processes (by process type) and subscribe to receive updates\nfor new process availability (by process type). The discovery service\ndoes not manage connections- it simply returns tables of registered\nprocesses, irrespective of their current availability. It is up to each\nindividual process to manage its own connections.  The discovery service uses the process.csv file to make connections to\nprocesses on start up. After start up it is up to each individual\nprocess to attempt connections and register with the discovery service.\nThis is done automatically, depending on the configuration parameters.\nMultiple discovery services can be run in which case each process will\ntry to register and retrieve process details from each discovery process\nit finds in its process.csv file. Discovery services do not replicate\nbetween themselves. A discovery process must have its process type\nlisted as discovery.  To run the discovery service, use a start line such as:  aquaq $ q torq.q -load code/processes/discovery.q -p 9995  Modify the configuration as required.", 
            "title": "Overview"
        }, 
        {
            "location": "/Processes/#operation", 
            "text": "Processes register with the discovery service.     Processes use the discovery service to locate other processes.     When new services register, any processes which have registered an\n     interest in that process type are notified.", 
            "title": "Operation"
        }, 
        {
            "location": "/Processes/#available-processes", 
            "text": "The list of available processes can be found in the .servers.SERVERS\ntable.  q).servers.SERVERS                                                                                                                                                                                                                                                            \nprocname     proctype        hpup            w  hits startp                        lastp                         endp attributes                                                                   \n-------------------------------------------------------------------------------------\ndiscovery1   discovery       :aquaq:9995     0                                  2014.01.22D17:00:40.947470000      ()!()                                                                        \ndiscovery2   discovery       :aquaq:9996     0                                  2014.01.22D17:00:40.947517000      ()!()                                                                        \nhdb2         hdb             :aquaq:5013     0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \nkilltick     kill            :aquaq:20000    0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \ntpreplay1    tickerlogreplay :aquaq:20002    0                                  2014.01.22D17:00:40.947602000      ()!()                                                                        \ntickerplant1 tickerplant     :aquaq:5010  6  0    2014.01.22D17:00:40.967699000 2014.01.22D17:00:40.967698000      ()!()                                                                        \nmonitor1     monitor         :aquaq:20001 9  0    2014.01.22D17:00:40.971344000 2014.01.22D17:00:40.971344000      ()!()                                                                        \nrdb1         rdb             :aquaq:5011  7  0    2014.01.22D17:06:13.032883000 2014.01.22D17:06:13.032883000      `date`tables!(,2014.01.22;`fxquotes`heartbeat`logmsg`quotes`trades)          \nhdb3         hdb             :aquaq:5012  8  0    2014.01.22D17:06:18.647349000 2014.01.22D17:06:18.647349000      `date`tables!(2014.01.13 2014.01.14;`fxquotes`heartbeat`logmsg`quotes`trades)\ngateway1     gateway         :aquaq:5020  10 0    2014.01.22D17:06:32.152836000 2014.01.22D17:06:32.152836000      ()!()", 
            "title": "Available Processes"
        }, 
        {
            "location": "/Processes/#gateway", 
            "text": "A synchronous and asynchronous gateway is provided. The gateway can be\nused for load balancing and/or to join the results of queries across\nheterogeneous servers (e.g. an RDB and HDB). Ideally the gateway should\nonly be used with asynchronous calls. Synchronous calls cause the\ngateway to block so limits the gateway to serving one query at a time\n(although if querying across multiple backend servers the backend\nqueries will be run in parallel). When using asynchronous calls the\nclient can either block and wait for the result (deferred synchronous)\nor post a call back function which the gateway will call back to the\nclient with. With both asynchronous and synchronous queries the backend\nservers to execute queries against are selected using process type. The\ngateway API can be seen by querying .api.p\u201c.gw.*\u201d within a gateway\nprocess.", 
            "title": "Gateway"
        }, 
        {
            "location": "/Processes/#asynchronous-behaviour", 
            "text": "Asynchronous queries allow much greater flexibility. They allow multiple\nqueries to be serviced at once, prioritisation, and queries to be timed\nout. When an asynchronous query is received the following happens:    the query is placed in a queue;    the list of available servers is retrieved;    the queue is prioritised, so those queries with higher priority are\n      serviced first;    queries are sent to back end servers as they become available. Once\n      the backend server returns its result, it is given another query;    when all the partial results from the query are returned the results\n      are aggregated and returned to the client. They are either returned\n      directly, or wrapped in a callback and posted back asynchronously to\n      the client.    The two main customisable features of the gateway are the selection of\navailable servers (.gw.availableservers) and the queue prioritisation\n(.gw.getnextqueryid). With default configuration, the available servers\nare those servers which are not currently servicing a query from the\ngateway, and the queue priority is a simple FIFO queue. The available\nservers could be extended to handle process attributes, such as the\navailable datasets or the location of the process, and the queue\nprioritisation could be modified to anything required e.g. based on the\nquery itself, the username, host of the client etc.  An asynchronous query can be timed out using a timeout defined by the\nclient. The gateway will periodically check if any client queries have\nnot completed in the alotted time, and return a timeout error to the\nclient. If the query is already running on any backend servers then they\ncannot be timed out other than by using the standard -T flag.", 
            "title": "Asynchronous Behaviour"
        }, 
        {
            "location": "/Processes/#synchronous-behaviour", 
            "text": "When using synchronous queries the gateway can only handle one query at\na time and cannot timeout queries other than with the standard -T flag.\nAll synchronous queries will be immediately dispatched to the back end\nprocesses. They will be dispatched using an asyhcnronous call, allowing\nthem to run in parallel rather than serially. When the results are\nreceived they are aggregated and returned to the client.", 
            "title": "Synchronous Behaviour"
        }, 
        {
            "location": "/Processes/#process-discovery", 
            "text": "The gateway uses the discovery service to locate processes to query\nacross. The discovery service will notify the gateway when new processes\nbecome available and the gateway will automatically connect and start\nusing them. The gateway can also use the static information in\nprocess.csv, but this limits the gateway to a predefined list of\nprocesses rather than allowing new services to come online as demand\nrequires.", 
            "title": "Process Discovery"
        }, 
        {
            "location": "/Processes/#error-handling", 
            "text": "When synchronous calls are used, q errors are returned to clients as\nthey are encountered. When using asynchronous calls there is no way to\nreturn actual errors and appropriately prefixed strings must be used\ninstead. It is up to the client to check the type of the received result\nand if it is a string then whether it contains the error prefix. The\nerror prefix can be changed, but the default is \u201cerror: \u201d. Errors will\nbe returned when:    the client requests a query against a server type which the gateway\n    does not currently have any active instances of (this error is\n    returned immediately);    the query is timed out;    a back end server returns an error;    a back end server fails;    the join function fails.    If postback functions are used, the error string will be posted back\nwithin the postback function (i.e. it will be packed the same way as a\nvalid result).", 
            "title": "Error Handling"
        }, 
        {
            "location": "/Processes/#client-calls", 
            "text": "There are four main client calls. The .gw.sync* methods should only be\ninvoked synchronously, and the .gw.async* methods should only be\ninvoked asynchronously. Each of these are documented more extensively in\nthe gateway api. Use .api.p\u201c.gw.*\u201d for more details.     Function  Description      .gw.syncexec[query; servertypes]  Execute the specified query synchronously against the required list of servers. If more than one server, the results will be razed.    .gw.syncexecj[query; servertypes; joinfunction]  Execute the specified query against the required list of servers. Use the specified join function to aggregate the results.    .gw.asyncexec[query; servertypes]  Execute the specified query against the required list of servers. If more than one server, the results will be razed. The client must block and wait for the results.    .gw.asyncexecjpt[query; servertypes; joinfunction; postback; timeout]  Execute the specified query against the required list of servers. Use the specified join function to aggregate the results. If the postback function is not set, the client must block and wait for the results. If it is set, the result will be wrapped in the specified postback function and returned asynchronously to the client. The query will be timed out if the timeout value is exceeded.     For the purposes of demonstration, assume that the queries must be run\nacross an RDB and HDB process, and the gateway has one RDB and two HDB\nprocesses available to it.  q).gw.servers                                                                                                                                                                                                                                                                 \nhandle| servertype inuse active querycount lastquery                     usage                attributes                   \n------| --------------------------------------------------------------------\n7     | rdb        0     1      17         2014.01.07D17:05:03.113927000 0D00:00:52.149069000 `datacentre`country!`essex`uk\n8     | hdb        0     1      17         2014.01.07D17:05:03.113927000 0D00:01:26.143564000 `datacentre`country!`essex`uk\n9     | hdb        0     1      2          2014.01.07D16:47:33.615538000 0D00:00:08.019862000 `datacentre`country!`essex`uk\n12    | rdb        0     1      2          2014.01.07D16:47:33.615538000 0D00:00:04.018349000 `datacentre`country!`essex`uk  Both the RDB and HDB processes have a function f and table t defined. f\nwill run for 2 seconds longer on the HDB processes then it will the RDB.  q)f                                                                                                                                                                                                                                                                           \n{system\"sleep \",string x+$[`hdb=.proc.proctype;2;0]; t}\nq)t                                                                                                                                                                                                                                                                           \na   \n----\n5013\n5014\n5015\n5016\n5017  Run the gateway. The main parameter which should be set is the\n.servers.CONNECTIONS parameter, which dictates the process types the\ngateway queries across. Also, we need to explicitly allow sync calls. We\ncan do this from the config or from the command line.  q torq.q -load code/processes/gateway.q -p 8000 -.gw.synccallsallowed 1 -.servers.CONNECTIONS hdb rdb  Start a client and connect to the gateway. Start with a sync query. The\nHDB query should take 4 seconds and the RDB query should take 2 seconds.\nIf the queries run in parallel, the total query time should be 4\nseconds.  q)h:hopen 8000                                                                                                                                                                                                                                                                \nq)h(`.gw.syncexec;(`f;2);`hdb`rdb)                                                                                                                                                                                                                                            \na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\nq)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb)                                                                                                                                                                                                                                         \n4009  If a query is done for a server type which is not registered, an error\nis returned:  q)\\t h(`.gw.syncexec;(`f;2);`hdb`rdb`other)                                                                                                                                                                                                                                   \n`not all of the requested server types are available; missing other  Custom join functions can be specified:  q)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by a from x} each x})                                                                                                                                                                                                  \na   | x\n----| -\n5014| 2\n5015| 2\n5016| 2\n5017| 1\n5018| 1\n5012| 1\n5013| 1  Custom joins can fail with appropriate errors:  q)h(`.gw.syncexecj;(`f;2);`hdb`rdb;{sum{select count i by b from x} each x})                                                                                                                                                                                                  \n`failed to apply supplied join function to results: b  Asynchronous queries must be sent in async and blocked:  q)(neg h)(`.gw.asyncexec;(`f;2);`hdb`rdb); r:h(::)                                                                                                                                                                                                                          \n    /- This white space is from pressing return\n    /- the client is blocked and unresponsive\n\nq)q)q)                                                                                                                                                                                                                                                                        \nq)                                                                                                                                                                                                                                                                            \nq)r                                                                                                                                                                                                                                                                           \na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\nq)  We can send multiple async queries at once. Given the gateway has two\nRDBs and two HDBs avaialble to it, it should be possible to service two\nof these queries at the same time.  q)h:hopen each 8000 8000                                                                                                                                                                                                                                                      \nq)\\t (neg h)@\\:(`.gw.asyncexec;(`f;2);`hdb`rdb); (neg h)@\\:(::); r:h@\\:(::)\n4012\nq)r                                                                                                                                                                                                                                                                           \n+(,`a)!,5014 5015 5016 5017 5018 5012 5013 5014 5015 5016\n+(,`a)!,5013 5014 5015 5016 5017 9999 10000 10001 10002 10003  Alternatively async queries can specify a postback so the client does\nnot have to block and wait for the result. The postback function must\ntake two parameters- the first is the function that was sent up, the\nsecond is the results. The postback can either be a lambda, or the name\nof a function.  q)h:hopen 8000                                                                                                                                                                                                                                                                \nq)handleresults:{-1(string .z.z),\" got results\"; -3!x; show y}                                                                                                                                                                                                                \nq)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;handleresults;0Wn)                                                                                                                                                                                                           \nq)\nq)  /- These q prompts are from pressing enter\nq)  /- The q client is not blocked, unlike the previous example\nq)\nq)2014.01.07T16:53:42.481 got results\na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016\n\n/- Can also use a named function rather than a lambda\nq)(neg h)(`.gw.asyncexecjpt;(`f;2);`hdb`rdb;raze;`handleresults;0Wn)\nq)\nq)              \nq)2014.01.07T16:55:12.235 got results\na   \n----\n5014\n5015\n5016\n5017\n5018\n5012\n5013\n5014\n5015\n5016  Asynchronous queries can also be timed out. This query will run for 22\nseconds, but should be timed out after 5 seconds. There is a tolerance\nof +5 seconds on the timeout value, as that is how often the query list\nis checked. This can be reduced as required.  q)(neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::)\n\nq)q)q)r                                                                                                                                                                                                                                                                       \n\"error: query has exceeded specified timeout value\"\nq)\\t (neg h)(`.gw.asyncexecjpt;(`f;20);`hdb`rdb;raze;();0D00:00:05); r:h(::)                                                                                                                                                                                                  \n6550", 
            "title": "Client Calls"
        }, 
        {
            "location": "/Processes/#non-kdb-clients", 
            "text": "All the examples in the previous section are from clients written in q.\nHowever it should be possible to do most of the above from non kdb+\nclients. The officially supported APIs for Java, C# and C allow the\nasynchronous methods above. For example, we can modify the try block in\nthe main function of the  Java Grid\nViewer :  import java.awt.BorderLayout;\nimport java.awt.Color;\nimport java.io.IOException;\nimport java.lang.reflect.Array;\nimport java.util.logging.Level;\nimport java.util.logging.Logger;\nimport javax.swing.JFrame;\nimport javax.swing.JScrollPane;\nimport javax.swing.JTable;\nimport javax.swing.table.AbstractTableModel;\nimport kx.c;\n\npublic class Main {\n    public static class KxTableModel extends AbstractTableModel {\n        private c.Flip flip;\n        public void setFlip(c.Flip data) {\n            this.flip = data;\n        }\n\n        public int getRowCount() {\n            return Array.getLength(flip.y[0]);\n        }\n\n        public int getColumnCount() {\n            return flip.y.length;\n        }\n\n        public Object getValueAt(int rowIndex, int columnIndex) {\n            return c.at(flip.y[columnIndex], rowIndex);\n        }\n\n        public String getColumnName(int columnIndex) {\n            return flip.x[columnIndex];\n        }\n    };\n\n    public static void main(String[] args) {\n        KxTableModel model = new KxTableModel();\n        c c = null;\n        try {\n            c = new c(\"localhost\", 8000,\"username:password\");\n            // Create the query to send\n        String query=\".gw.asyncexec[(`f;2);`hdb`rdb]\";\n            // Send the query \n        c.ks(query);\n            // Block on the socket and wait for the result\n        model.setFlip((c.Flip) c.k());\n        } catch (Exception ex) {\n            Logger.getLogger(Main.class.getName()).log(Level.SEVERE, null, ex);\n        } finally {\n            if (c != null) {try{c.close();} catch (IOException ex) {}\n          }\n        }\n        JTable table = new JTable(model);\n        table.setGridColor(Color.BLACK);\n        String title = \"kdb+ Example - \"+model.getRowCount()+\" Rows\";\n        JFrame frame = new JFrame(title);\n        frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);\n        frame.getContentPane().add(new JScrollPane(table), BorderLayout.CENTER);\n        frame.setSize(300, 300);\n        frame.setVisible(true);\n    }\n}  Some of the unofficially supported APIs may only allow synchronous calls\nto be made.", 
            "title": "Non kdb+ Clients"
        }, 
        {
            "location": "/Processes/#tickerplant", 
            "text": "The tickerplant is a modified version of the standard kdb+tick tickerplant.\nThe modifications from the standard tick.q include:    Applies timestamps as timestamp rather than timespan;    Tracks per table record counts in .u.icounts dictionary for faster recovery\n    of real time subscribers;    Allows configuration of timezones for timestamping data and performing\n    end of day rollover (see  eodtime.q );    The tickerplant log file will be written to hdb/database.", 
            "title": "Tickerplant"
        }, 
        {
            "location": "/Processes/#real-time-database-rdb", 
            "text": "The Real Time Database is a modified version of r.q found in kdb+tick.\nThe modifications from the standard r.q include:    Tickerplant (data source) and HDB location derived from processes\n    defined by the discovery service or from config file;    Automatic re-connection and resubscription to tickerplant;    List of tables to subscribe to supplied as configuration setting;    More pre-built flexibility in end-of-day;    More verbose end-of-day logging;    Reload multiple authenticated HDBs after end-of-day;    End-of-day save down manipulation code is shared between RDB, WDB\n      and tickerplant log replay    See the top of the file for more information.", 
            "title": "Real Time Database (RDB)"
        }, 
        {
            "location": "/Processes/#write-database-wdb", 
            "text": "The Write Database or WDB is based on w.q. This process features a\nnumber of modifications and enhancements over w.q:    Provides the option to write down to a custom partition scheme,\n    defined by parted columns in sort.csv, which removes the need for\n    end of day sorting;    Greater configuration options; max rows on a per table basis, list\n     subscription tables, upd function etc. See the top of the process\n     file for the options;    Use of common code with the RDB and Tickerplant Log Replay process\n     to manipulate tables before save, sort and apply attributes;    Checks whether to persist data to disk on a timer rather than on\n     each tick;    Informs other RDB, HDB and GW processes that end of day save and\n     sort has completed;    More log information supplied;    End of day timezone can be configured (see  eodtime.q ).    The WDB process can broken down into two main functions:    Periodically saving data to disk and    Sorting data at end of day    The WDB process provides flexibility so it can be set-up as a\nstand-alone process that will both save and sort data or two separate\nprocesses (one that saves the data and another that will sort the data\non disk). This allows greater flexibility around the end of day event as\nsorting data can be time consuming. It is also helps when implementing\nseemless rollovers (i.e. no outage window at end-of-day).  The behaviour of the WDB process is controlled by the  .wdb.mode \nparameter. This should be set to one of following three values:    saveandsort - the process will subscribe for data, periodically\n    write data to disk and at EOD it will flush remaining data to disk\n    before sorting it and informing GWs, RDBs and HDBs etc.    save - the process will subscribe for data, periodically write data\n      to disk and at EOD it will flush remaining data to disk. It will\n      then inform its respective sort mode process to sort the data    sort - the process will wait to get a trigger from its respective\n      save mode process. When this is triggered it will sort the data on\n      disk, apply attributes and the trigger a reload on the RDB, HDB and\n      GW processes    When running a system with separate save and sort process, the sort\nprocess should be configured in the processes.csv file with a proctype\nof sort. The save process will check for processes with a proctype of\nsort when it attempts to trigger the end of day sort of the data.  The wdb process provides two methods for persisting data to disk and\nsorting at the end of the day.    default - Data is persisted into a partition defined by the\n    [partitiontype] variable, similar to the hdb partition scheme. The\n    general scheme is of the form\n    [wdbdir]/[partitiontype]/[table]/. And a typical partition\n    directory would be similar to wdb/database/2015.11.26/trades/. At\n    the end of the day, before being moved to the hdb, the data is\n    sorted according to parameters defined in sort.csv. For each table,\n    sort.csv will specify the columns to sort (using xasc) and apply\n    attributes to.    partbyattr - Data is persisted to a custom partition scheme, derived\n      from parameters in the sort.csv file. The write down scheme is taken\n      from sort.csv, to reflect the effect of using xasc at the end of\n      day. For each table, the columns defined in sort.csv, with the\n      parted attribute, are used to create custom partitions in the wdb.\n      Multiple columns can be defined with the parted attribute and\n      distinct combinations of each are generated for custom partitions.\n      The general partition scheme is of the form\n      [wdbdir]/[partitiontype]/[table]/[parted column(s)]/. And a\n      typical partition directory would be similar to\n      wdb/database/2015.11.26/trade/MSFT_N. In the above example, the\n      data is parted by sym and source, and so a unique partition\n      directory MSFT_N is created in the wdb directory.  At the end of the day, data is upserted into the hdb without the\n  need for sorting. The number of rows that are joined at once is\n  limited by the mergenumrows and mergenumtab parameters.    The optional partbyattr method may provide a significant saving in time\nat the end of day, allowing the hdb to be accessed sooner. For large\ndata sets with a low cardinality (ie. small number of distinct elements)\nthe optional method may provide a significant time saving, upwards of\n50%. The optional method should also reduce the memory usage at the end\nof day event, as joining data is generally less memory intensive than\nsorting.", 
            "title": "Write Database (WDB)"
        }, 
        {
            "location": "/Processes/#tickerplant-log-replay", 
            "text": "The Tickerplant Log Replay script is for replaying tickerplant logs.\nThis is useful for:    handling end of day save down failures;    handling large volumes of data (larger than can fit into RAM).    The process takes as the main input either an individual log file to\nreplay, or a directory containing a set of log files. Amongst other\nfunctionality, the process can:    replay specific message ranges;    replay in manageable message chunks;    recover as many messages as possible from a log file rather than\n      just stopping at the first bad message;    ignore specific tables;    modify the tables before or after they are saved;    apply sorting and parting after all the data is written out.    The process must have some variables set (the tickerplant log file or\ndirectory, the schema file, and the on-disk database directory to write\nto) or it will fail on startup. These can either be set in the config\nfile, or overridden from the command line in the usual way. An example\nstart line would be:  q torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.tplogfile ../test/tplogs/marketdata2013.12.17 -.replay.schemafile ../test/marketdata.q -.replay.hdbdir ../test/hdb1  The tickerplant log replay script has extended usage information which\ncan be accessed with -.replay.usage.  q torq.q -debug -load code/processes/tickerlogreplay.q -p 9990 -.replay.usage", 
            "title": "Tickerplant Log Replay"
        }, 
        {
            "location": "/Processes/#housekeeping", 
            "text": "The housekeeping process is used to undertake periodic system\nhousekeeping and maintenance, such as compressing or removing files\nwhich are no longer required. The process will run the housekeeping jobs\nperiodically on a timer. Amongst other functionality the process:    Allows for removing and zipping of directory files;    Provides an inbuilt search utility and selectively searches using a\n      \u2018find\u2019 and \u2018exclude\u2019 string, and an \u2018older than\u2019 parameter;    Reads all tasks from a single CSV;    Runs on a user defined timer;    Can be run immediately from command line or within the process;    Can be easily extended to include new user defined housekeeping\n      tasks.    The process has two main parameters that should be set prior to use;\nruntimes and inputcsv.\u2018Runtimes\u2019 sets the timer to run housekeeping at\nthe set time(s), and \u2018Inputcsv\u2019 provides the location of the\nhousekeeping csv file. These can either be set in the config file, or\noverridden via the command line. If these are not set, then default\nparameters are used; 12.00 and \u2018KDBCONFIG/housekeeping.csv\u2019\nrespectively. The process is designed to run from a single csv file with\nfive headings:    Function details the action that you wish to be carried out on the\n    files, initially, this can be rm (remove) and zip (zipping);    Path specifies the directory that the files are in;    Match provides the search string to the find function, files\n      returned will have names that match this string;    Exclude provides a second string to the find function, and these\n      files are excluded from the match list;    Age is the \u2018older than\u2019 parameter, and the function will only be\n      carried out on files older than the age given (in days).    An example csv file would be:  function,path,match,exclude,age\nzip,./logs/,*.log,*tick*,2\nrm,./logs/,*.log*,*tick*,4\nzip,./logs/,*tick*,,1\nrm,./logs/,*tick*,,3\n\nfunction path      match    exclude  age\n----------------------------------------\nzip      \"./logs/\" \"*.log\"  \"*tick*\" 2\nrm       \"./logs/\" \"*.log*\" \"*tick*\" 4\nzip      \"./logs/\" \"*tick*\" \"\"       1\nrm       \"./logs/\" \"*tick*\" \"\"       3  The process reads in the csv file, and passes it line by line to a\n\u2018find\u2019 function; providing a dictionary of values that can be used to\nlocate the files required. The find function takes advantage of system\ncommands to search for the files according to the specifications in the\ndictionary. A search is performed for both the match string and the\nexclude string, and cross referenced to produce a list of files that\nmatch the parameters given. The files are then each passed to a further\nset of system commands to perform the task of either zipping or\nremoving. Note that an incomplete csv or non-existant path will throw an\nerror.  The remove and zipping functions form only basic implimentations of the\nhousekeeping process; it is designed to be exended to include more\nactions than those provided. Any user function defined in the\nhousekeeping code can be employed in the same fashion by providing the\nname of the function,search string and age of files to the csv.  As well as being scheduled on a timer, the process can also be run\nimmediately. Adding \u2018-hk.runnow 1\u2019 to the command line when starting the\nprocess will force immediate running of the actions in the housekeeping\ncsv. Likewise, setting runnow to 1b in the config file will immediately\nrun the cleaning process. Both methods will cause the process to exit\nupon completion. Calling hkrun[] from within the q process will also\nrun the csv instructions immediately. This will not affect any timer\nscheduling and the process will remain open upon completion.  Housekeeping works both on windows and unix based systems. Since the\nprocess utilizes inbuilt system commands to perform maintenances, a\nunix/windows switch detects the operating system of the host and applies\neither unix or widows functions appropriately. Extensions need only be\nmade in the namespace of the hosting operating system (i.e. if you are\nusing a unix system, and wish to add a new function, you do not need to\nadd the function to the windows namespace to). Usage information can be\naccessed using the \u2018-hkusage\u2019 flag:  q torq.q -load code/processes/housekeeping.q -p 9999 -proctype housekeeping -procname hk1 -debug -hkusage", 
            "title": "Housekeeping"
        }, 
        {
            "location": "/Processes/#file-alerter", 
            "text": "The file alerter process is a long-running process which periodically\nscans a set of directories for user-specified files. If a matching file\nis found it will then carry out a user-defined function on it. The files\nto search for and the functions to run are read in from a csv file.\nAdditionally, the file alerter process can:    run more than one function on the specified file.    optionally move the file to a new directory after running the\n      function.    store a table of files that have already been processed.    run the function only on new files or run it every time the file is\n      modified.    ignore any matching files already on the system when the process\n      starts and only run a function if a new file is added or a file is\n      modified.    The file alerter process has four parameters which should be set prior\nto use. These parameters can either be set in the config file or\noverridden on the command-line. If they are not set, the default\nparameters will be used. The parameters are as follows.  inputcsv  - The name and location of the csv file which defines the\nbehaviour of the process. The default is KDBCONFIG/filealerter.csv.  polltime  - How often the process will scan for matching files. The\ndefault is 0D:00:01, i.e., every minute.  alreadyprocessed  - The name and location of the already-processed\ntable. The default is KDBCONFIG/filealerterprocessed. This table will\nbe created automatically the first time the process is ran.  skipallonstart  - If this is set to 1, it will ignore all files\nalready on the system; if it is set to 0, it will not. The default\nvalue is 0.  The files to find and the functions to run are read in from a csv file\ncreated by the user. This file has five columns, which are detailed\nbelow.  path  - This is the path to the directory that will be scanned for\nthe file.  match  - This is a search string matching the name of the file to be\nfound. Wildcards can be used in this search, for example, \u201cfile*\u201d will\nfind all files starting with \u201cfil\u201d.  function  - This is the name of the function to be run on the file.\nThis function must be defined in the script\nKDBCODE/processes/filealerter.q. If the function is not defined or fails\nto run, the process will throw an error and ignore that file from then\non.  newonly  - This is a boolean value. If it is set to 1, it will\nonly run the function on the file if it has been newly created. If it is\nset to 0, then it will run the function every time the file is\nmodified.  movetodirectory  - This is the path of the directory you would like\nto move the file to after it has been processed. If this value is left\nblank, the file will not be moved.  It is possible to run two separate functions on the same file by adding\nthem as separate lines in the csv file. If the file is to be moved after\nit is processed, the file alerter will run both functions on the file\nand then attempt to move it. A typical csv file to configure the file\nalerter would look like:  path,match,function,newonly,movetodirectory\n/path/to/dirA,fileA.*,copy,0,/path/to/newDir\n/path/to/dirB,fileB.txt,email,1,\n/path/to/dirA,fileA.*,delete,0,/path/to/newDir\n\npath        match      function  newonly    movetodirectory\n---------------------------------------------------\n\"/path/to/dirA\" \"fileA.*\"   copy    0   \"/path/to/newDir\"\n\"/path/to/dirB\" \"fileB.txt\" email   1   \"\"\n\"/path/to/dirA\" \"fileA.*\"   delete  0   \"/path/to/newDir\"  The file alerter process reads in each line of the csv file and searches\nfiles matching the search string specified in that line. Note that there\nmay be more than one file found if a wildcard is used in the search\nstring. If it finds any files, it will check that they are not in the\nalready processed table. If newonly is set to 1, it only checks if\nthe filename is already in the table. If newonly is set to 0, it\nchecks against the filename, filesize and a md5 hash of the file. The\nmd5 hash and the filesize are used to determine if the file has been\nmodified since it was processed last. If the found files have not been\nprocessed already, it then attempts to run the specified function to\nthese files.  After the process has run through each line of the csv, it generates a\ntable of all files that were processed on that run. These files are\nappended to the already processed table which is then saved to disk. The\nfile alerter will attempt to move the files to the \u2018movetodirectory\u2019, if\nspecified. If the file has already been moved during the process (for\nexample, if the function to run on it was \u2018delete\u2019), the file alerter\nwill not attempt to move it.  The file alerter is designed to be extended by the user. Customised\nfunctions should be defined within the filealerter.q script. They should\nbe diadic functions, i.e., they take two parameters: the path and the\nfilename. As an example, a simple function to make a copy of a file in\nanother directory could be:  copy:{[path;file] system \"cp \", path,\"/\", file, \" /path/to/newDir\"}  Although the process is designed to run at regular intervals throughout\nthe day, it can be called manually by invoking the FArun[] command\nfrom within the q session. Similarly, if new lines are added to the csv\nfile, then it can be re-loaded by calling the loadcsv[] command\nfrom the q session.  Each stage of the process, along with any errors which may occur, are\nappropriately logged in the usual manner.  The file alerter process is designed to work on both Windows and Unix\nbased systems. Since many of the functions defined will use inbuilt\nsystem command they will be need to written to suit the operating system\nin use. It should also be noted that Windows does not have an inbuilt\nmd5 hashing function so the file alerter will only detect different\nversions of files if the filename or filesize changes.", 
            "title": "File Alerter"
        }, 
        {
            "location": "/Processes/#reporter", 
            "text": "", 
            "title": "Reporter"
        }, 
        {
            "location": "/Processes/#overview_1", 
            "text": "The reporter process is used to run periodic reports on specific\nprocesses. A report is the result of a query that is run on a process at\na specific time. The result of the query is then handled by one of the\ninbuilt result handlers, with the ability to add custom result handlers.   Features:    Easily create a report for information that you want;    Fully customizable scheduling such as start time, end time and days\n      of the week;    Run reports repeatedly with a custom period between them;    Asynchronous querying with custom timeout intervals;    Inbuilt result handlers allow reports to be written to file or\n      published;    Custom result handlers can be defined;    Logs each step of the report process;    Fully integrated with the TorQ gateway to allow reports to be run\n      across backend processes.    The reporter process has three parameters that are read in on\ninitialisation from the reporter.q file found in the\n$KDBCONFIG/settings directory. These settings are the string filepath\nof the input csv file, a boolean to output log messages and timestamp\nfor flushing the query log table.  To run the reporter process:  q torq.q -load code/processes/reporter.q -p 20004  Once the reporter process has been initiated, the reports will be\nscheduled and no further input is required from the user.", 
            "title": "Overview"
        }, 
        {
            "location": "/Processes/#report-configuration", 
            "text": "By default, the process takes its inputs from a file called reporter.csv\nwhich is found in the $KDBCONFIG directory. This allows the user\ncomplete control over the configuration of the reports. As the queries\nare evaluated on the target process, local variables can be referenced\nor foreign functions can be run. Table [table:reportertable] shows the\nmeaning of the csv schema.     Column Header  Description and Example      name  Report name e.g. Usage    query  Query to be evaluated on that process. It can be a string query or function    resulthandler  Result handlers are run on the returned result. Custom result handlers can be added. The result handler must be a monadic function with the result data being passed in e.g. writetofile[\u201c./output\u201d;\u201cusage\u201d]    gateway  If non null the reporter will query processes route the query to the proctype specified in this field. The values in the proctype field will be the process types on which the gateway runs the backend query. e.g. `gateway    joinfunction  Used to join the results when a gateway query is being used. The choice of joinfunction must take into account the result that will be received. The function must be monadic and the parameter will be the list of results returned from the backend processes e.g. raze    proctype  The type of process that the report will be run on. If the gateway field is not empty this may be a list of process types, otherwise the reporter will throw an error on startup. e.g. `rdb    procname  The name of a specific process to run the report on. If left null, the reporter process will select a random process with the specified proctype. If the gateway field is not null, this field specifies the specific gateway process name to run the query against e.g. `hdb1    start  Time on that day to start at e.g. 12:00    end  Time on that day that the report will stop at e.g. 23:00    period  The period between each report query e.g. 00:00:10    timeoutinterval  The amount of time the reporter waits before timing out a report e.g. 00:00:30    daysofweek  Numeric value required for the day of the week. Where 0 is Saturday and 2 is Monday     When running a report on a gateway, the gateway field must be set to the\nproctype of the gateway that will be queried. It will then run the\nreport on the processes which are listed in the proctype field and join\nthe results by using the function specified in the joinfunction field.\nIf there is no join function then the reporter process will not start.\nMultiple entries in the proctype field must be separated by a space and\nare only allowed when the gateway field is not empty. If gateway field\nis empty and there are multiple entries in the proctype field then the\nreporter process will not load.  Listing [code:csvschema] shows an example of the schema needed in the\ninput csv file.  name|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek\nusage|10#.usage.usage|writetofiletype[\"./output/\";\"usage\";\"csv\"]|||rdb||00:01|23:50|00:01|00:00:01|0 1 2 3 4 5 6\nmemory|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|||rdb|rdb1|00:05|18:00|00:01|00:00:08|0 1 2 3 4 5 6\nusage_gateway|10#.usage.usage||gateway|raze|rdb hdb||00:02|22:00|00:01|00:00:10|0 1 2 3 4 5 6", 
            "title": "Report Configuration"
        }, 
        {
            "location": "/Processes/#result-handlers", 
            "text": "There are several default result handlers which are listed below. Custom\nresult handlers can be defined as required. The result handler will be\ninvoked with a single parameter (the result of the query).  writetofiletype  - Accepts 3 parameters: path, filename, filetype and\ndata. When writing to file it uses a date time suffix so the resultant\nfilename will be  usage_rdb_2014_01_02_15_00_12.txt  e.g.  writetofiletype[\"./output/\";\"usage\";\"csv\"]  splaytable  - This accepts 3 parameters: path, file and data. This\nsplays the result to a directory. The result must be a table in order to\nuse this function e.g.  splaytable[\"./output/\";\"usage\"]  emailalert  - This accepts 3 parameters: period, recipient list and\ndata. The period dictates the throttle i.e. emails will be sent at most\nevery period. The result of the report must be a table with a single\ncolumn called messages which contains the character list of the email\nmessage. This is used with the monitoring checks to raise alerts, but\ncan be used with other functions.  emailalert[0D00:30;(\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\")]  emailreport  - This accepts 3 parameters: temporary path, recipient\nlist, file name, file type and data. The data is written out as the file\ntype (e.g. csv, xml, txt, xls, json) with the given file name to the\ntemporary path. It is then emailed to the recipient list, and the\ntemporary file removed.  emailreport[\"./tempdir/\"; (\"test@aquaq.co.uk\";\"test1@aquaq.co.uk\"); \"EndOfDayReport\"; \"csv\"]  publishresult  - Accepts 1 parameter and that is the data. This is\ndiscussed later in the subsection\u00a0subresults.\nCustom result handlers can be added to $KDBCODE/processes/reporter.q .\nIt is important to note that the result handler is referencing local\nfunctions as it is executed in the reporter process and not the target\nprocess. When the query has been successful the result handler will be\npassed a dictionary with the following keys: queryid, time, name,\nprocname, proctype and result.", 
            "title": "Result Handlers"
        }, 
        {
            "location": "/Processes/#report-process-tracking", 
            "text": "Each step of the query is logged by the reporter process. Each query is\ngiven a unique id and regular system messages are given the id 0. The\nstage column specifies what stage the query is in and these are shown in\ntable [table:stagetable]. An appropriate log message is also shown so\nany problems can easily be diagnosed. The in memory table is flushed\nevery interval depending on the value of the flushqueryloginterval\nvariable in the reporter.q file found in the $KDBCONFIG/settings\ndirectory.      Stage symbol  Explanation      R  The query is currently running    E  An error has occurred during the query    C  The query has been completed with no errors    T  The query has exceeded the timeout interval    S  System message e.g. \u201cReporter Process Initialised\u201d     time                         | queryid stage message\n-----------------------------| ------------------------------------------------------------------------\n2014.10.20D22:20:06.597035000| 37 R \"Received result\"\n2014.10.20D22:20:06.600692000| 37 R \"Running resulthandler\"\n2014.10.20D22:20:06.604455000| 37 C \"Finished report\"\n2014.10.20D22:30:00.984572000| 38 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\"\n2014.10.20D22:30:00.991862000| 38 R \"Received result\"\n2014.10.20D22:30:00.995527000| 38 R \"Running resulthandler\"\n2014.10.20D22:30:00.999236000| 38 C \"Finished report\"\n2014.10.20D22:30:06.784419000| 39 R \"Running report: rdbtablecount against proctype: rdb on handle: 7i\"\n2014.10.20D22:30:06.796431000| 39 R \"Received result\"", 
            "title": "Report Process Tracking"
        }, 
        {
            "location": "/Processes/#subscribing-for-results", 
            "text": "To publish the results of the report, the reporter process uses the pub\nsub functionality of TorQ. This is done by using the using the inbuilt\nresult handler called publishresult. In order to subscribe to this feed,\nconnect to the reporter process and send the function shown below over\nthe handle. To subscribe to all reports use a backtick as the second\nparameter and to subscribe to a specific reports results include the\nreporter name as a symbol.  /- define a upd function\nupd:insert\n\n/- handle to reporter process\nh: hopen 20004\n\n/- Subscribe to all results that use the publishresult handler\nh(`.ps.subscribe;`reporterprocessresults;`)\n\n/- Subscribe to a specific report called testreport\nh(`.ps.subscribe;`reporterprocessresults;`testreport)", 
            "title": "Subscribing for Results"
        }, 
        {
            "location": "/Processes/#example-reports", 
            "text": "The following are examples of reports that could be used in the reporter\nprocess. The rdbtablecount report will run hourly and return the count\nof all the tables in a rdb process. The memoryusage report will run\nevery 10 minutes against the gateway for multiple processes and will\nreturn the  .Q.w[]  information. Both of these reports run between\n9:30am to 4:00pm during the weekdays. The report onetimequery is an\nexample of a query that is run one time, in order to run a query once,\nthe period must be the same as the difference between the start and end\ntime.  name|query|resulthandler|gateway|joinfunction|proctype|procname|start|end|period|timeoutinterval|daysofweek\nrdbtablecount|ts!count each value each ts:tables[]|{show x`result}|||rdb|rdb1|09:30|16:00|01:00|00:00:10|2 3 4 5 6\nmemoryusage|.Q.w[]|writetofile[\"./output/\";\"memory.csv\"]|gateway1|{enlist raze x}|rdb hdb||09:30|16:00|00:10|00:00:10|2 3 4 5 6\nonetimequery|10#.usage.usage|writetofile[\"./output/\";\"onetime.csv\"]|||rdb||10:00|10:01|00:01|00:00:10|2 3 4 5 6", 
            "title": "Example reports"
        }, 
        {
            "location": "/Processes/#monitor", 
            "text": "The Monitor process is a simple process to monitor the health of the\nother processes in the system. It connects to each process that it finds\n(by default using the discovery service, though can use the static file\nas well) and subscribes to both heartbeats and log messages. It\nmaintains a keyed table of heartbeats, and a table of all log messages\nreceived.  Run it with:  aquaq $ q torq.q -load code/processes/monitor.q -p 20001  It is probably advisable to run the monitor process with the -trap flag,\nas there may be some start up errors if the processes it is connecting\nto do not have the necessary heartbeating or publish/subscribe code\nloaded.  aquaq $ q torq.q -load code/processes/monitor.q -p 20001 -trap  The current heartbeat statuses are tracked in .hb.hb, and the log\nmessages in logmsg  q)show .hb.hb                                                                                                                                                                                                                                                                 \nsym       procname    | time                          counter warning error\n----------------------| ---------------------------------------------------\ndiscovery discovery2  | 2014.01.07D13:24:31.848257000 893     0       0    \nhdb       hdb1        | 2014.01.07D13:24:31.866459000 955     0       0    \nrdb       rdb_europe_1| 2014.01.07D13:23:31.507203000 901     1       0    \nrdb       rdb1        | 2014.01.07D13:24:31.848259000 34      0       0\n\nq)show select from logmsg where loglevel=`ERR                                                                                                              \ntime                          sym  host  loglevel id      message                               \n-------------------------------------------------------------------------------------\n2014.01.07D12:25:17.457535000 hdb1 aquaq ERR      reload  \"failed to reload database\"           \n2014.01.07D13:29:28.784333000 rdb1 aquaq ERR      eodsave \"failed to save tables : trade, quote\"", 
            "title": "Monitor"
        }, 
        {
            "location": "/Processes/#html5-front-end", 
            "text": "A HTML5 front end has been built to display important process\ninformation that is sent from the monitor process. It uses HTML5,\nWebSockets and JavaScript on the front end and interacts with the\nmonitor process in the kdb+ side. The features of the front end include:    Heartbeat table with processes that have warnings highlighted in\n    orange and errors in red    Log message table displaying the last 30 errors    Log message error chart that is by default displayed in 5 minute\n      bins    Chart\u2019s bin value can be changed on the fly    Responsive design so works on all main devices i.e. phones, tablets\n      and desktop    It is accessible by going to the url  http://HOST:PORT/.non?monitorui", 
            "title": "HTML5 front end"
        }, 
        {
            "location": "/Processes/#compression", 
            "text": "The compression process is a thin wrapper around the compression utility\nlibrary. It allows periodic compression of whole or parts of databases\n(e.g. data is written out uncompressed and then compressed after a\ncertain period of time). It uses four variables defined in\nKDBCONFIG/settings/compression.q which specify    the compression configuration file to use    the database directory to compress    the maximum age of data to attempt to compress    whether the process should exit upon completion    The process is run like other TorQ processes:  q torq.q -load code/processes/compression.q -p 20005  Modify the settings file or override variables from the command line as\nappropriate.", 
            "title": "Compression"
        }, 
        {
            "location": "/Processes/#kill", 
            "text": "The kill process is used to connect to and terminate currently running\nprocesses. It kills the process by sending the exit command therefore\nthe kill process must have appropriate permissions to send the command,\nand it must be able to create a connection (i.e. it will not be able to\nkill a blocked process in the same way that the unix command kill -9\nwould). By default, the kill process will connect to the discovery\nservice(s), and kill the processes of the specified types. The kill\nprocess can be modified to not use the discovery service and instead use\nthe process.csv file via the configuration in the standard way.  If run without any command line parameters, kill.q will try to kill each\nprocess it finds with type defined by its .servers.CONNECTIONS variable.  q torq.q -load code/processes/kill.q -p 20000  .servers.CONNECTIONS can optionally be overridden from the command line\n(as can any other process variable):  q torq.q -load code/processes/kill.q -p 20000 -.servers.CONNECTIONS rdb tickerplant  The kill process can also be used to kill only specific named processes\nwithin the process types:  q torq.q -load code/processes/kill.q -p 20000 -killnames hdb1 hdb2", 
            "title": "Kill"
        }, 
        {
            "location": "/Processes/#chained-tickerplant", 
            "text": "In tick+ architecture the main tickerplant is the most important\ncomponent, as it is relied upon by all the real time subscribers. When\nthe tickerplant goes down data will be lost, compare this to an rdb\nwhich can be recovered after it fails. The chained tickerplant process\nis an additional tickerplant that is a real time subscriber to the main\ntickerplant but replicates its behaviour. It will have its own real time\nsubscribers and can be recovered when it fails. This is the recommended\napproach when users want to perform their own custom real time analysis.  The chained tickerplant can:    subscribe to specific tables and syms    batch publish at an interval or publish tick by tick    create a tickerplant log that its real time subscribers can replay    replay the source tickerplant log    To launch the chained tickerplant  q torq.q -load code/processes/chainedtp.q -p 12009  Chained tickerplant settings are found in  config/settings/chainedtp.q \nand are under the  .ctp  namespace.     Setting  Explanation  Default      tickerplantname  list of tickerplant names to try and make a connection to  `tickerplant1    pubinterval  publish batch updates at this interval. If the value is 0D00:00:00 then it will publish tick by tick  0D00:00:00    tpconnsleep  number of seconds between attempts to connect to the source tickerplant  10    createlogfile  create a log file  0b    logdir  directory containing chained tickerplant logs  `:hdb    subscribeto  subscribe to these tables only (null for all)  `    subscribesyms  subscribe to these syms only (null for all)  `    replay  replay the tickerplant log file  0b    schema  retrieve schema from tickerplant  1b    clearlogonsubscription  clear log on subscription, only called if createlogfile is also enabled  0b", 
            "title": "Chained Tickerplant"
        }, 
        {
            "location": "/visualisation/", 
            "text": "Visualisation\n\n\nkdb+ supports websockets and so HTML5 GUIs can be built. We have\nincorporated a set of server side and client side utilities to ease HTML\nGUI development.\n\n\n\n\nkdb+ Utilities\n\n\nThe server side utilities are contained in html.q. These utilise some\ncommunity code, specifically json.k and a modified version of u.q, both\nfrom Kx Systems. The supplied functionality includes:\n\n\n\n\n\n\njson.k provides two way conversion between kdb+ data structures and\n    JSON;\n\n\n\n\n\n\nu.q is the standard pub/sub functionality provided with kdb+tick,\n    and a modified version is incorporated to publish data structures\n    which can be easily interpreted in JavaScript;\n\n\n\n\n\n\nfunctions for reformatting temporal types to be JSON compliant;\n\n\n\n\n\n\npage serving to utilise the inbuilt kdb+ webserver to serve custom\n    web pages. An example would be instead of having to serve a page\n    with a hardcoded websocket connection host and port, the kdb+\n    process can serve a page connecting back to itself no matter which\n    host or port it is running on.\n\n\n\n\n\n\n\n\nJavaScript Utilities\n\n\nThe JavaScript utilities are contained in kdbconnect.js. The library\nallows you to:\n\n\n\n\n\n\ncreate a connection to the kdb+ process;\n\n\n\n\n\n\ndisplay the socket status;\n\n\n\n\n\n\nsending queries;\n\n\n\n\n\n\nbinding results returned from kdb+ to updates in the webpage.\n\n\n\n\n\n\n\n\nOutline\n\n\nAll communication between websockets and kdb+ is asynchronous. The\napproach we have adopted is to ensure that all data sent to the web\nbrowser is encoded as a JSON object containing a tag to enable the web\npage to decipher what the data relates to. The format we have chosen is\nfor kdb+ to send dictionaries of the form:\n\n\n`name`data!(\"dataID\";dataObject)\n\n\n\nAll the packing can be done by .html.dataformat. Please note that the\ntemporal types are converted to longs which can easily be converted to\nJavaScript Date types. This formatting can be modified in the formating\ndictionary .html.typemap.\n\n\nq)a:flip `minute`time`date`month`timestamp`timespan`datetime`float`sym!enlist each (09:00; 09:00:00.0;.z.d; `month$.z.d; .z.p; .z.n;.z.z;20f;`a)\nq).html.dataformat[\"start\";(enlist `tradegraph)!enlist a]\nname| \"start\"\ndata| (,`tradegraph)!,+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a)\nq)first (.html.dataformat[\"start\";(enlist `tradegraph)!enlist a])[`data;`tradegraph]                                                                                     \nminute   | 32400000\ntime     | 32400000\ndate     | 1396828800000\nmonth    | 1396310400000\ntimestamp| \"2014-04-07T13:23:01Z\"\ntimespan | 48181023\ndatetime | \"2014-04-07T13:23:01Z\"\nfloat    | 20f\nsym      | `a\n\n\n\nWe have also extended this structure to allow web pages to receive data\nin a way similar to the standard kdb+tick pub/sub format. In this case,\nthe data object looks like:\n\n\n`name`data!(\"upd\";`tablename`tabledata!(`trade;([]time:09:00 09:05 09:10; price:12 13 14)))\n\n\n\nThis can be packed with .html.updformat:\n\n\nq).html.updformat[\"upd\";`tablename`tabledata!(`trade;a)]                                                                                                                 \nname| \"upd\"\ndata| `tablename`tabledata!(`trade;+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a))\nq)first(.html.updformat[\"upd\";`tablename`tabledata!(`trade;a)])[`data;`tabledata]                                                                                        \nminute   | 32400000\ntime     | 32400000\ndate     | 1396828800000\nmonth    | 1396310400000\ntimestamp| \"2014-04-07T13:23:01Z\"\ntimespan | 48181023\ndatetime | \"2014-04-07T13:23:01Z\"\nfloat    | 20f\nsym      | `a\n\n\n\nTo utilise the pub/sub functionality, the web page must connect to the\nkdb+ process and subscribe for updates. Subscriptions are done using\n\n\n.html.wssub[`tablename]\n\n\n\nPublications from the kdb+ side are done with\n\n\n.html.pub[`tablename;tabledata]\n\n\n\nOn the JavaScript side the incoming messages (data events) must be bound\nto page updates. For example, there might be an initialisation event\ncalled \u201cstart\u201d which allows the web page to retrieve all the initial\ndata from the process. The code below redraws the areas of the page with\nthe received data.\n\n\n/* Bind data - Data type \"start\" will execute the callback function */\nKDBCONNECT.bind(\"data\",\"start\",function(data){\n  // Check that data is not empty\n  if(data.hbtable.length !== 0)\n   // Write HTML table to div element with id heartbeat-table\n   { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.hbtable));}\n  if(data.lmtable.length !== 0)\n   // Write HTML table to div element with id logmsg-table\n   { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.lmtable));}  \n  if(data.lmchart.length !== 0)\n   // Log message error chart\n   { MONITOR.barChart(data.lmchart,\"logmsg-chart\",\"Error Count\",\"myTab\"); }\n  });\n\n\n\nSimilarly the upd messages must be bound to page updates. In this case,\nthe structure is slightly different:\n\n\nKDBCONNECT.bind(\"data\",\"upd\",function(data){\n  if(data.tabledata.length===0) return;\n  if(data.tablename === \"heartbeat\")\n    { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.tabledata));}\n  if(data.tablename === \"logmsg\")\n    { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.tabledata));}\n  if(data.tablename === \"lmchart\")\n    { MONITOR.barChart(data.tabledata,\"logmsg-chart\",\"Error Count\",\"myTab\"); }\n });\n\n\n\nTo display the WebSocket connection status the event \u201cws_event\u201d must be\nbound and it will output one of these default messages: \u201cConnecting...\u201d,\n\u201cConnected\u201d and \u201cDisconnected\u201d depending on the connection state of the\nWebSocket. Alternatively the value of the readyState attribute will\ndetermine the WebSocket status.\n\n\n// Select html element using jQuery\nvar $statusMsg = $(\"#status-msg\");  \nKDBCONNECT.bind(\"ws_event\",function(data){\n  // Data is the default message string\n  $statusMsg.html(data);\n});\nKDBCONNECT.core.websocket.readyState // Returns 1 if connected.\n\n\n\nErrors can be displayed by binding the event called \u201cerror\u201d.\n\n\nKDBCONNECT.bind(\"error\",function(data){\n  $statusMsg.html(\"Error - \" + data);\n});\n\n\n\n\n\nExample\n\n\nA basic example is provided with the Monitor process. To get this to\nwork, u.q from kdb+tick should be placed in the code/common directory to\nallow all processes to publish updates. It should be noted that this is\nnot intended as a production monitoring visualisation screen, moreso a\ndemonstration of functionality. See section\u00a0monitorgui for more\ndetails.\n\n\n\n\nFurther Work\n\n\nFurther work planned includes:\n\n\n\n\n\n\nallow subscriptions on a key basis- currently all subscribers\n    receive all updates;\n\n\n\n\n\n\nadd JavaScript controls to allow in-place updates based on key\n    pairs, and scrolling window updates e.g. add N new rows to\n    top/bottom of the specified table;\n\n\n\n\n\n\nallow multiple websocket connections to be maintained at the same\n    time.", 
            "title": "Visualisation"
        }, 
        {
            "location": "/visualisation/#visualisation", 
            "text": "kdb+ supports websockets and so HTML5 GUIs can be built. We have\nincorporated a set of server side and client side utilities to ease HTML\nGUI development.", 
            "title": "Visualisation"
        }, 
        {
            "location": "/visualisation/#kdb-utilities", 
            "text": "The server side utilities are contained in html.q. These utilise some\ncommunity code, specifically json.k and a modified version of u.q, both\nfrom Kx Systems. The supplied functionality includes:    json.k provides two way conversion between kdb+ data structures and\n    JSON;    u.q is the standard pub/sub functionality provided with kdb+tick,\n    and a modified version is incorporated to publish data structures\n    which can be easily interpreted in JavaScript;    functions for reformatting temporal types to be JSON compliant;    page serving to utilise the inbuilt kdb+ webserver to serve custom\n    web pages. An example would be instead of having to serve a page\n    with a hardcoded websocket connection host and port, the kdb+\n    process can serve a page connecting back to itself no matter which\n    host or port it is running on.", 
            "title": "kdb+ Utilities"
        }, 
        {
            "location": "/visualisation/#javascript-utilities", 
            "text": "The JavaScript utilities are contained in kdbconnect.js. The library\nallows you to:    create a connection to the kdb+ process;    display the socket status;    sending queries;    binding results returned from kdb+ to updates in the webpage.", 
            "title": "JavaScript Utilities"
        }, 
        {
            "location": "/visualisation/#outline", 
            "text": "All communication between websockets and kdb+ is asynchronous. The\napproach we have adopted is to ensure that all data sent to the web\nbrowser is encoded as a JSON object containing a tag to enable the web\npage to decipher what the data relates to. The format we have chosen is\nfor kdb+ to send dictionaries of the form:  `name`data!(\"dataID\";dataObject)  All the packing can be done by .html.dataformat. Please note that the\ntemporal types are converted to longs which can easily be converted to\nJavaScript Date types. This formatting can be modified in the formating\ndictionary .html.typemap.  q)a:flip `minute`time`date`month`timestamp`timespan`datetime`float`sym!enlist each (09:00; 09:00:00.0;.z.d; `month$.z.d; .z.p; .z.n;.z.z;20f;`a)\nq).html.dataformat[\"start\";(enlist `tradegraph)!enlist a]\nname| \"start\"\ndata| (,`tradegraph)!,+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a)\nq)first (.html.dataformat[\"start\";(enlist `tradegraph)!enlist a])[`data;`tradegraph]                                                                                     \nminute   | 32400000\ntime     | 32400000\ndate     | 1396828800000\nmonth    | 1396310400000\ntimestamp| \"2014-04-07T13:23:01Z\"\ntimespan | 48181023\ndatetime | \"2014-04-07T13:23:01Z\"\nfloat    | 20f\nsym      | `a  We have also extended this structure to allow web pages to receive data\nin a way similar to the standard kdb+tick pub/sub format. In this case,\nthe data object looks like:  `name`data!(\"upd\";`tablename`tabledata!(`trade;([]time:09:00 09:05 09:10; price:12 13 14)))  This can be packed with .html.updformat:  q).html.updformat[\"upd\";`tablename`tabledata!(`trade;a)]                                                                                                                 \nname| \"upd\"\ndata| `tablename`tabledata!(`trade;+`minute`time`date`month`timestamp`timespan`datetime`float`sym!(,32400000;,32400000;,1396828800000;,1396310400000;,\"2014-04-07T13:23:01Z\";,48181023;,\"2014-04-07T13:23:01Z\";,20f;,`a))\nq)first(.html.updformat[\"upd\";`tablename`tabledata!(`trade;a)])[`data;`tabledata]                                                                                        \nminute   | 32400000\ntime     | 32400000\ndate     | 1396828800000\nmonth    | 1396310400000\ntimestamp| \"2014-04-07T13:23:01Z\"\ntimespan | 48181023\ndatetime | \"2014-04-07T13:23:01Z\"\nfloat    | 20f\nsym      | `a  To utilise the pub/sub functionality, the web page must connect to the\nkdb+ process and subscribe for updates. Subscriptions are done using  .html.wssub[`tablename]  Publications from the kdb+ side are done with  .html.pub[`tablename;tabledata]  On the JavaScript side the incoming messages (data events) must be bound\nto page updates. For example, there might be an initialisation event\ncalled \u201cstart\u201d which allows the web page to retrieve all the initial\ndata from the process. The code below redraws the areas of the page with\nthe received data.  /* Bind data - Data type \"start\" will execute the callback function */\nKDBCONNECT.bind(\"data\",\"start\",function(data){\n  // Check that data is not empty\n  if(data.hbtable.length !== 0)\n   // Write HTML table to div element with id heartbeat-table\n   { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.hbtable));}\n  if(data.lmtable.length !== 0)\n   // Write HTML table to div element with id logmsg-table\n   { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.lmtable));}  \n  if(data.lmchart.length !== 0)\n   // Log message error chart\n   { MONITOR.barChart(data.lmchart,\"logmsg-chart\",\"Error Count\",\"myTab\"); }\n  });  Similarly the upd messages must be bound to page updates. In this case,\nthe structure is slightly different:  KDBCONNECT.bind(\"data\",\"upd\",function(data){\n  if(data.tabledata.length===0) return;\n  if(data.tablename === \"heartbeat\")\n    { $(\"#heartbeat-table\").html(MONITOR.jsonTable(data.tabledata));}\n  if(data.tablename === \"logmsg\")\n    { $(\"#logmsg-table\").html(MONITOR.jsonTable(data.tabledata));}\n  if(data.tablename === \"lmchart\")\n    { MONITOR.barChart(data.tabledata,\"logmsg-chart\",\"Error Count\",\"myTab\"); }\n });  To display the WebSocket connection status the event \u201cws_event\u201d must be\nbound and it will output one of these default messages: \u201cConnecting...\u201d,\n\u201cConnected\u201d and \u201cDisconnected\u201d depending on the connection state of the\nWebSocket. Alternatively the value of the readyState attribute will\ndetermine the WebSocket status.  // Select html element using jQuery\nvar $statusMsg = $(\"#status-msg\");  \nKDBCONNECT.bind(\"ws_event\",function(data){\n  // Data is the default message string\n  $statusMsg.html(data);\n});\nKDBCONNECT.core.websocket.readyState // Returns 1 if connected.  Errors can be displayed by binding the event called \u201cerror\u201d.  KDBCONNECT.bind(\"error\",function(data){\n  $statusMsg.html(\"Error - \" + data);\n});", 
            "title": "Outline"
        }, 
        {
            "location": "/visualisation/#example", 
            "text": "A basic example is provided with the Monitor process. To get this to\nwork, u.q from kdb+tick should be placed in the code/common directory to\nallow all processes to publish updates. It should be noted that this is\nnot intended as a production monitoring visualisation screen, moreso a\ndemonstration of functionality. See section\u00a0monitorgui for more\ndetails.", 
            "title": "Example"
        }, 
        {
            "location": "/visualisation/#further-work", 
            "text": "Further work planned includes:    allow subscriptions on a key basis- currently all subscribers\n    receive all updates;    add JavaScript controls to allow in-place updates based on key\n    pairs, and scrolling window updates e.g. add N new rows to\n    top/bottom of the specified table;    allow multiple websocket connections to be maintained at the same\n    time.", 
            "title": "Further Work"
        }, 
        {
            "location": "/blog/", 
            "text": "Creating a Custom Application \n\n\nSomething that is not immediately clear is how a custom application built on TorQ should be structured and deployed. This blog outlines how a custom application should be structured.\n\n\n Avoiding End-of-Day Halts \n\n\nkdb+tick is great, but there\u2019s a problem- when the RDB (real time database) writes to disk on its daily schedule, users cannot access that day\u2019s data until the write out is complete. this blog post details how TorQ solves this problem.\n\n\n Fast, Flexible, Low Memory End-Of-Day Writes \n\n\nA discussion on which method you should use for an end-of-day sort in TorQ.\n\n\n End-of-Day Parallel Sorting \n\n\nDetails on how TorQ utilises sortslaves to vastly speed up the end-of-day sort.\n\n\n Broadcast Publish \n\n\nkdb+ v3.4 introduces a new broadcast feature that reduces the work done when publishing messages to multiple subscribers. This blog post explains how and why to use this feature.\n\n\n Recovering Corrupt Tickerplant Logs \n\n\nCorrupt tickerplant logs are a curse that no one deserves but that doesn\u2019t stop them from happening even to the best of us. However, all hope is not lost as it is possible to recover the good messages and discard the bad. In this post we will extend upon the standard rescuelog procedure to recover as much as possible from the log file.\n\n\n kdb+ Gateways \n\n\nThe advantages and methods of using a gateway in a kdb+ tick system.\n\n\n Faster Recovery With kdb+ tick \n\n\nHow to effectively recover a process to its previous state after a crash using the RDB instead of the tickerplant log.\n\n\n Parallel kdb+ Database Access with QPad \n\n\nWe\u2019ve been working with Oleg Zakharov, who created QPad, to implement asynchronous querying and allow users to run multiple concurrent queries more efficiently. This blog explains how it works.\n\n\n TorQ Permission Framework \n\n\nAn in depth and interactive post explaining TorQ permissioning.", 
            "title": "TorQ Blog Posts"
        }
    ]
}